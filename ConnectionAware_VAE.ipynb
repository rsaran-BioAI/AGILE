{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "V100",
      "authorship_tag": "ABX9TyOvb6PXJrXkz+CQOsCkCpIp",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rsaran-BioAI/AGILE/blob/main/ConnectionAware_VAE.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CzjuzD8qCCm0",
        "outputId": "394ceef8-755a-4293-acb7-cebfd1591f55"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# Connecting the drive with Colab\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W24TTrTZtvBl",
        "outputId": "b9a0ac63-a74c-4bfa-9849-9b0b29e8d9ae"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#%%bash\n",
        "#MINICONDA_INSTALLER_SCRIPT=Miniconda3-latest-Linux-x86_64.sh\n",
        "#MINICONDA_PREFIX=/usr/local\n",
        "#wget https://repo.continuum.io/miniconda/$MINICONDA_INSTALLER_SCRIPT\n",
        "#chmod +x $MINICONDA_INSTALLER_SCRIPT\n",
        "#./$MINICONDA_INSTALLER_SCRIPT -b -f -p $MINICONDA_PREFIX"
      ],
      "metadata": {
        "id": "Y3CBY-5YP3KP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install rdkit"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2X8_QseIOVe8",
        "outputId": "b9ed597b-b6e1-4947-d2e6-17f3a111d8fa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: rdkit in /usr/local/lib/python3.10/dist-packages (2023.9.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from rdkit) (1.23.5)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from rdkit) (9.4.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch_geometric"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ANVtQ-SHOjlD",
        "outputId": "f11016f7-5b81-41d0-f37e-c765ef27f0a9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch_geometric in /usr/local/lib/python3.10/dist-packages (2.4.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (4.66.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (1.23.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (1.11.4)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (3.1.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (2.31.0)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (3.1.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (1.2.2)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (5.9.5)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch_geometric) (2.1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric) (2023.11.17)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->torch_geometric) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->torch_geometric) (3.2.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uBNJUiG6w-rD",
        "outputId": "1d2f3839-a0c6-41eb-98bd-900f680ad6bd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.1.0+cu118)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install guacamol"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7alRIeqgOtzH",
        "outputId": "3375394e-0684-4ed3-9d79-4fcc97924189"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: guacamol in /usr/local/lib/python3.10/dist-packages (0.5.5)\n",
            "Requirement already satisfied: joblib>=0.12.5 in /usr/local/lib/python3.10/dist-packages (from guacamol) (1.3.2)\n",
            "Requirement already satisfied: numpy>=1.15.2 in /usr/local/lib/python3.10/dist-packages (from guacamol) (1.23.5)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from guacamol) (1.11.4)\n",
            "Requirement already satisfied: tqdm>=4.26.0 in /usr/local/lib/python3.10/dist-packages (from guacamol) (4.66.1)\n",
            "Requirement already satisfied: FCD>=1.1 in /usr/local/lib/python3.10/dist-packages (from guacamol) (1.2)\n",
            "Requirement already satisfied: rdkit-pypi>=2021.9.2.1 in /usr/local/lib/python3.10/dist-packages (from guacamol) (2022.9.5)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from FCD>=1.1->guacamol) (2.1.0+cu118)\n",
            "Requirement already satisfied: rdkit in /usr/local/lib/python3.10/dist-packages (from FCD>=1.1->guacamol) (2023.9.2)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from rdkit-pypi>=2021.9.2.1->guacamol) (9.4.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->FCD>=1.1->guacamol) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch->FCD>=1.1->guacamol) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->FCD>=1.1->guacamol) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->FCD>=1.1->guacamol) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->FCD>=1.1->guacamol) (3.1.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->FCD>=1.1->guacamol) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch->FCD>=1.1->guacamol) (2.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->FCD>=1.1->guacamol) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->FCD>=1.1->guacamol) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorboardX"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XjI7TwiOO3QD",
        "outputId": "bba333ba-d44c-4425-9f0d-9fe70d00135e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorboardX in /usr/local/lib/python3.10/dist-packages (2.6.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from tensorboardX) (1.23.5)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorboardX) (23.2)\n",
            "Requirement already satisfied: protobuf>=3.20 in /usr/local/lib/python3.10/dist-packages (from tensorboardX) (3.20.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install networkx"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c2ijtsjEd7gg",
        "outputId": "02f68f4b-1d54-49bf-f964-0661b605f133"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (3.2.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Merging Operation Learning"
      ],
      "metadata": {
        "id": "L0eyGyhiZjOn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/drive/MyDrive/AGILE2/AI4Sci-MiCaM/src/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AI8NPUWKNEoW",
        "outputId": "2b88764e-b233-4f56-bf9e-1aa918b12e20"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/AGILE2/AI4Sci-MiCaM/src\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import multiprocessing as mp\n",
        "import os\n",
        "from collections import defaultdict\n",
        "from dataclasses import dataclass\n",
        "from datetime import datetime\n",
        "from multiprocessing import Process, Queue\n",
        "from typing import Dict, List, Tuple\n",
        "\n",
        "import networkx as nx\n",
        "from rdkit import Chem"
      ],
      "metadata": {
        "id": "-E-e2qS2yYfI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import arguments\n",
        "import model.mydataclass\n",
        "from arguments import parse_arguments\n",
        "from model.mydataclass import Paths"
      ],
      "metadata": {
        "id": "kMAgxnLJW40o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@dataclass\n",
        "class MolGraph:\n",
        "    idx: int\n",
        "    mol_graph: Chem.rdchem.Mol\n",
        "    merging_graph: nx.Graph\n",
        "\n",
        "    def __init__(self, smiles: str, idx: int=0) -> \"MolGraph\":\n",
        "        self.idx = idx\n",
        "        self.mol_graph = Chem.MolFromSmiles(smiles)\n",
        "        self.merging_graph = nx.Graph(Chem.rdmolops.GetAdjacencyMatrix(self.mol_graph))\n",
        "        for atom in self.mol_graph.GetAtoms():\n",
        "            self.merging_graph.nodes[atom.GetIdx()][\"atom_indices\"] = set([atom.GetIdx()])\n",
        "\n",
        "    def apply_merging_operation(self, motif: str, stats: Dict[str, int], indices: Dict[str, Dict[int, int]]) -> None:\n",
        "        if self.merging_graph.number_of_nodes() == 1:\n",
        "            return\n",
        "        new_graph = self.merging_graph.copy()\n",
        "        for (node1, node2) in self.merging_graph.edges:\n",
        "            if not new_graph.has_edge(node1, node2):\n",
        "                continue\n",
        "            atom_indices = new_graph.nodes[node1][\"atom_indices\"].union(new_graph.nodes[node2][\"atom_indices\"])\n",
        "            motif_smiles = fragment2smiles(self, atom_indices)\n",
        "            if motif_smiles == motif:\n",
        "                graph_before_merge = new_graph.copy()\n",
        "                merge_nodes(new_graph, node1, node2)\n",
        "                update_stats(self, graph_before_merge, new_graph, node1, node2, stats, indices, self.idx)\n",
        "        self.merging_graph = new_graph\n",
        "        indices[motif][self.idx] = 0\n",
        "\n",
        "    def apply_merging_operation_producer(self, motif: str, q: Queue) -> None:\n",
        "        if self.merging_graph.number_of_nodes() == 1:\n",
        "            return\n",
        "        new_graph = self.merging_graph.copy()\n",
        "        for (node1, node2) in self.merging_graph.edges:\n",
        "            if not new_graph.has_edge(node1, node2):\n",
        "                continue\n",
        "            atom_indices = new_graph.nodes[node1][\"atom_indices\"].union(new_graph.nodes[node2][\"atom_indices\"])\n",
        "            motif_smiles = fragment2smiles(self, atom_indices)\n",
        "            if motif_smiles == motif:\n",
        "                graph_before_merge = new_graph.copy()\n",
        "                merge_nodes(new_graph, node1, node2)\n",
        "                update_stats_producer(self, graph_before_merge, new_graph, node1, node2, q, self.idx)\n",
        "        q.put((motif, self.idx, new_graph))\n",
        "\n",
        "def load_batch_mols(batch: List[Tuple[int, str]]) -> List[MolGraph]:\n",
        "    return [MolGraph(smi, idx) for (idx, smi) in batch]\n",
        "\n",
        "def load_mols(train_path: str, num_workers: int) -> List[MolGraph]:\n",
        "    print(f\"[{datetime.now()}] Loading molecules...\")\n",
        "    smiles_list = [smi.strip(\"\\n\") for smi in open(train_path)]\n",
        "    smiles_list = [(i, smi) for (i, smi) in enumerate(smiles_list)]\n",
        "\n",
        "    batch_size = (len(smiles_list) - 1) // num_workers + 1\n",
        "    batches = [smiles_list[i : i + batch_size] for i in range(0, len(smiles_list), batch_size)]\n",
        "    mols: List[MolGraph]= []\n",
        "    with mp.Pool(num_workers) as pool:\n",
        "        for mols_batch in pool.imap(load_batch_mols, batches):\n",
        "            mols.extend(mols_batch)\n",
        "\n",
        "    print(f\"[{datetime.now()}] Loading molecules finished. Total: {len(mols)} molecules.\\n\")\n",
        "    return mols\n",
        "\n",
        "def fragment2smiles(mol: MolGraph, indices: List[int]) -> str:\n",
        "    smiles = Chem.MolFragmentToSmiles(mol.mol_graph, tuple(indices))\n",
        "    return Chem.MolToSmiles(Chem.MolFromSmiles(smiles, sanitize=False))\n",
        "\n",
        "def merge_nodes(graph: nx.Graph, node1: int, node2: int) -> None:\n",
        "    neighbors = [n for n in graph.neighbors(node2)]\n",
        "    atom_indices = graph.nodes[node1][\"atom_indices\"].union(graph.nodes[node2][\"atom_indices\"])\n",
        "    for n in neighbors:\n",
        "        if node1 != n and not graph.has_edge(node1, n):\n",
        "            graph.add_edge(node1, n)\n",
        "        graph.remove_edge(node2, n)\n",
        "    graph.remove_node(node2)\n",
        "    graph.nodes[node1][\"atom_indices\"] = atom_indices\n",
        "\n",
        "def get_stats_producer(batch: List[MolGraph], q: Queue):\n",
        "    for mol in batch:\n",
        "        for (node1, node2) in mol.merging_graph.edges:\n",
        "            atom_indices = mol.merging_graph.nodes[node1][\"atom_indices\"].union(mol.merging_graph.nodes[node2][\"atom_indices\"])\n",
        "            motif_smiles = fragment2smiles(mol, atom_indices)\n",
        "            q.put((mol.idx, motif_smiles))\n",
        "    q.put(None)\n",
        "\n",
        "def get_stats_consumer(stats: Dict[str, int], indices: Dict[str, Dict[int, int]], q: Queue, num_workers: int):\n",
        "    num_tasks_done = 0\n",
        "    while True:\n",
        "        info = q.get()\n",
        "        if info == None:\n",
        "            num_tasks_done += 1\n",
        "            if num_tasks_done == num_workers:\n",
        "                break\n",
        "        else:\n",
        "            (idx, smi) = info\n",
        "            stats[smi] += 1\n",
        "            indices[smi][idx] += 1\n",
        "\n",
        "def get_stats(mols: List[MolGraph], num_workers: int) -> Tuple[Dict[str, int], Dict[int, int]]:\n",
        "    print(f\"[{datetime.now()}] Begin getting statistics.\")\n",
        "    stats = defaultdict(int)\n",
        "    indices = defaultdict(lambda: defaultdict(int))\n",
        "    if num_workers == 1:\n",
        "        for mol in mols:\n",
        "            for (node1, node2) in mol.merging_graph.edges:\n",
        "                atom_indices = mol.merging_graph.nodes[node1][\"atom_indices\"].union(mol.merging_graph.nodes[node2][\"atom_indices\"])\n",
        "                motif_smiles = fragment2smiles(mol, atom_indices)\n",
        "                stats[motif_smiles] += 1\n",
        "                indices[motif_smiles][mol.idx] += 1\n",
        "    else:\n",
        "        batch_size = (len(mols) - 1) // num_workers + 1\n",
        "        batches = [mols[i : i + batch_size] for i in range(0, len(mols), batch_size)]\n",
        "        q = Queue()\n",
        "        producers = [Process(target=get_stats_producer, args=(batches[i], q)) for i in range(num_workers)]\n",
        "        [p.start() for p in producers]\n",
        "        get_stats_consumer(stats, indices, q, num_workers)\n",
        "        [p.join() for p in producers]\n",
        "    return stats, indices\n",
        "\n",
        "def update_stats(mol: MolGraph, graph: nx.Graph, new_graph: nx.Graph, node1: int, node2: int, stats: Dict[str, int], indices: Dict[str, Dict[int, int]], i: int):\n",
        "    neighbors1 = [n for n in graph.neighbors(node1)]\n",
        "    for n in neighbors1:\n",
        "        if n != node2:\n",
        "            atom_indices = graph.nodes[node1][\"atom_indices\"].union(graph.nodes[n][\"atom_indices\"])\n",
        "            motif_smiles = fragment2smiles(mol, atom_indices)\n",
        "            stats[motif_smiles] -= 1\n",
        "            indices[motif_smiles][i] -= 1\n",
        "    neighbors2 = [n for n in graph.neighbors(node2)]\n",
        "    for n in neighbors2:\n",
        "        if n != node1:\n",
        "            atom_indices = graph.nodes[node2][\"atom_indices\"].union(graph.nodes[n][\"atom_indices\"])\n",
        "            motif_smiles = fragment2smiles(mol, atom_indices)\n",
        "            stats[motif_smiles] -= 1\n",
        "            indices[motif_smiles][i] -= 1\n",
        "    neighbors = [n for n in new_graph.neighbors(node1)]\n",
        "    for n in neighbors:\n",
        "        atom_indices = new_graph.nodes[node1][\"atom_indices\"].union(new_graph.nodes[n][\"atom_indices\"])\n",
        "        motif_smiles = fragment2smiles(mol, atom_indices)\n",
        "        stats[motif_smiles] += 1\n",
        "        indices[motif_smiles][i] += 1\n",
        "\n",
        "def update_stats_producer(mol: MolGraph, graph: nx.Graph, new_graph: nx.Graph, node1: int, node2: int, q: Queue, i: int):\n",
        "    neighbors1 = [n for n in graph.neighbors(node1)]\n",
        "    for n in neighbors1:\n",
        "        if n != node2:\n",
        "            atom_indices = graph.nodes[node1][\"atom_indices\"].union(graph.nodes[n][\"atom_indices\"])\n",
        "            motif_smiles = fragment2smiles(mol, atom_indices)\n",
        "            q.put((motif_smiles, i, -1))\n",
        "    neighbors2 = [n for n in graph.neighbors(node2)]\n",
        "    for n in neighbors2:\n",
        "        if n != node1:\n",
        "            atom_indices = graph.nodes[node2][\"atom_indices\"].union(graph.nodes[n][\"atom_indices\"])\n",
        "            motif_smiles = fragment2smiles(mol, atom_indices)\n",
        "            q.put((motif_smiles, i, -1))\n",
        "    neighbors = [n for n in new_graph.neighbors(node1)]\n",
        "    for n in neighbors:\n",
        "        atom_indices = new_graph.nodes[node1][\"atom_indices\"].union(new_graph.nodes[n][\"atom_indices\"])\n",
        "        motif_smiles = fragment2smiles(mol, atom_indices)\n",
        "        q.put((motif_smiles, i, 1))\n",
        "\n",
        "def apply_merging_operation_producer(motif: str, batch: List[MolGraph], q: Queue):\n",
        "    [mol.apply_merging_operation_producer(motif, q) for mol in batch]\n",
        "    q.put(None)\n",
        "\n",
        "def apply_merging_operation_consumer(mols: List[MolGraph], stats: Dict[str, int], indices: Dict[str, Dict[int, int]], q: Queue, num_workers: int):\n",
        "    num_tasks_done = 0\n",
        "    while True:\n",
        "        info = q.get()\n",
        "        if info == None:\n",
        "            num_tasks_done += 1\n",
        "            if num_tasks_done == num_workers:\n",
        "                break\n",
        "        else:\n",
        "            (motif, i, change) = info\n",
        "            if isinstance(change, int):\n",
        "                stats[motif] += change\n",
        "                indices[motif][i] += change\n",
        "            else:\n",
        "                assert isinstance(change, nx.Graph)\n",
        "                indices[motif][i] = 0\n",
        "                mols[i].merging_graph = change\n",
        "\n",
        "def apply_merging_operation(\n",
        "    motif: str,\n",
        "    mols: List[MolGraph],\n",
        "    stats: Dict[str, int],\n",
        "    indices: Dict[str, Dict[int, int]],\n",
        "    num_workers: int = 1,\n",
        "):\n",
        "    mols_to_process = [mols[i] for i, freq in indices[motif].items() if freq > 0]\n",
        "    if num_workers > 1:\n",
        "        batch_size = (len(mols_to_process) -1 ) // num_workers + 1\n",
        "        batches = [mols_to_process[i : i + batch_size] for i in range(0, len(mols_to_process), batch_size)]\n",
        "        q = Queue()\n",
        "        producers = [Process(target=apply_merging_operation_producer, args=(motif, batches[i], q)) for i in range(num_workers)]\n",
        "        [p.start() for p in producers]\n",
        "        apply_merging_operation_consumer(mols, stats, indices, q, num_workers)\n",
        "        [p.join() for p in producers]\n",
        "    else:\n",
        "        [mol.apply_merging_operation(motif, stats, indices) for mol in mols_to_process]\n",
        "    stats[motif] = 0\n",
        "\n",
        "def merging_operation_learning(\n",
        "    train_path: str,\n",
        "    operation_path: str,\n",
        "    num_iters: int,\n",
        "    min_frequency: int,\n",
        "    num_workers: int,\n",
        "    mp_threshold: int,\n",
        "):\n",
        "\n",
        "    print(f\"[{datetime.now()}] Learning merging operations from {train_path}.\")\n",
        "    print(f\"Number of workers: {num_workers}. Total number of CPUs: {mp.cpu_count()}.\\n\")\n",
        "\n",
        "    mols = load_mols(train_path, num_workers)\n",
        "    stats, indices = get_stats(mols, num_workers)\n",
        "\n",
        "    trace = []\n",
        "    dir = os.path.split(operation_path)[0]\n",
        "    os.makedirs(dir, exist_ok=True)\n",
        "    output = open(operation_path, \"w\")\n",
        "    for i in range(num_iters):\n",
        "        print(f\"[{datetime.now()}] Iteration {i}.\")\n",
        "        motif = max(stats, key=lambda x: (stats[x], x))\n",
        "        if stats[motif] < min_frequency:\n",
        "            print(f\"No motif has frequency >= {min_frequency}. Stopping.\\n\")\n",
        "            break\n",
        "        print(f\"[Iteration {i}] Most frequent motif: {motif}, frequency: {stats[motif]}.\\n\")\n",
        "        trace.append((motif, stats[motif]))\n",
        "\n",
        "        apply_merging_operation(\n",
        "            motif = motif,\n",
        "            mols = mols,\n",
        "            stats = stats,\n",
        "            indices = indices,\n",
        "            num_workers = num_workers if stats[motif] >= mp_threshold else 1,\n",
        "        )\n",
        "\n",
        "        output.write(f\"{motif}\\n\")\n",
        "\n",
        "    output.close()\n",
        "    print(f\"[{datetime.now()}] Merging operation learning finished.\")\n",
        "    print(f\"The merging operations are in {operation_path}.\\n\\n\")\n",
        "\n",
        "    return trace\n"
      ],
      "metadata": {
        "id": "KPnsIMXDYIFC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import argparse\n",
        "from arguments import parse_arguments"
      ],
      "metadata": {
        "id": "LUensBzIEMp5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from model.mydataclass import Paths"
      ],
      "metadata": {
        "id": "HEQdiSB4GCzW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import argparse # Just checking of the arguments.py file is imported and working\n",
        "from arguments import parse_arguments\n",
        "args = parse_arguments()\n",
        "print(args)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j50HFRIS9-DT",
        "outputId": "5625e842-ca80-467c-cdf9-dd8051228204"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Namespace(data_dir='/content/drive/MyDrive/AGILE2/AI4Sci-MiCaM/data/', preprocess_dir='/content/drive/MyDrive/AGILE2/AI4Sci-MiCaM/preprocess/', output_dir='/content/drive/MyDrive/AGILE2/AI4Sci-MiCaM/output/', tensorboard_dir='/content/drive/MyDrive/AGILE2/AI4Sci-MiCaM/tensorboard/', dataset='QM9/', job_name='train_micam', model_dir=None, generate_path='samples', num_workers=60, cuda=0, seed=2, num_operations=1000, num_iters=3000, min_frequency=0, mp_thd=100000.0, hidden_size=256, atom_embed_size=[192, 16, 16, 16, 16], edge_embed_size=256, motif_embed_size=[256, 256], latent_size=64, depth=15, motif_depth=6, dropout=0.3, virtual=False, pooling='add', steps=30000, batch_size=2000, lr=0.005, lr_anneal_iter=50, lr_anneal_rate=0.99, grad_clip_norm=1.0, beta_warmup=3000, beta_min=0.001, beta_max=0.3, beta_anneal_period=40000, prop_weight=0.2, num_sample=10000)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following hyperparameters were changed:\n",
        "\n",
        "parser.add_argument('--num_workers', type=int, default=60)  # Reduced number of workers\n",
        "\n",
        "parser.add_argument('--num_iters', type=int, default=1000)  # Fewer iterations (previously 3000)  \n",
        "\n",
        "parser.add_argument('--steps', type=int, default=10000)  # Fewer training steps (previously 50000)\n",
        "\n",
        "parser.add_argument('--batch_size', type=int, default=32)  # Smaller batch size (previously 128) - tried changing this but gave an error\n",
        "\n",
        "parser.add_argument('--dropout', type=float, default=0.2)  # Slightly lower dropout (previously 0.3)\n",
        "\n",
        "parser.add_argument('--lr', type=float, default=1e-3)  # Lower learning rate (previously 1e5)"
      ],
      "metadata": {
        "id": "gdWMzUlHOL4i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# if __name__ == \"__main__\":\n",
        "\n",
        "#     args = parse_arguments()\n",
        "#     paths = Paths(args)\n",
        "\n",
        "#     learning_trace = merging_operation_learning(\n",
        "#         train_path = paths.train_path,\n",
        "#         operation_path = paths.operation_path,\n",
        "#         num_iters = args.num_iters,\n",
        "#         min_frequency = args.min_frequency,\n",
        "#         num_workers = args.num_workers,\n",
        "#         mp_threshold = args.mp_thd,\n",
        "#     )"
      ],
      "metadata": {
        "id": "Hn91ZrcoYL28"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Motif Vocab Construction"
      ],
      "metadata": {
        "id": "srHAu1GuWFrM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pwd # just checking"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "Q4Z4wjWS0iJ1",
        "outputId": "9f60b4df-be6e-4285-c6c9-99dff5e0b2dc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/drive/MyDrive/AGILE2/AI4Sci-MiCaM/src'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from rdkit import Contrib\n",
        "from rdkit.Contrib import SA_Score\n",
        "from rdkit.Contrib.SA_Score import sascorer"
      ],
      "metadata": {
        "id": "0f0kWo3DWvXP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import multiprocessing as mp\n",
        "import os\n",
        "import os.path as path\n",
        "import pickle\n",
        "from collections import Counter\n",
        "from datetime import datetime\n",
        "from functools import partial\n",
        "from typing import List, Tuple\n",
        "\n",
        "from tqdm import tqdm\n",
        "\n",
        "from arguments import parse_arguments\n",
        "from model.mol_graph import MolGraph\n",
        "from model.mydataclass import Paths"
      ],
      "metadata": {
        "id": "642_FHD0WYKV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def apply_operations(batch: List[Tuple[int, str]], mols_pkl_dir: str) -> Counter:\n",
        "    vocab = Counter()\n",
        "    pos = mp.current_process()._identity[0]\n",
        "    with tqdm(total = len(batch), desc=f\"Processing {pos}\", position=pos-1, ncols=80, leave=False) as pbar:\n",
        "        for idx, smi in batch:\n",
        "            mol = MolGraph(smi, tokenizer=\"motif\")\n",
        "            with open(path.join(mols_pkl_dir, f\"{idx}.pkl\"), \"wb\") as f:\n",
        "                pickle.dump(mol, f)\n",
        "            vocab = vocab + Counter(mol.motifs)\n",
        "            pbar.update()\n",
        "    return vocab\n",
        "\n",
        "def motif_vocab_construction(\n",
        "    train_path: str,\n",
        "    vocab_path: str,\n",
        "    operation_path: str,\n",
        "    num_operations: int,\n",
        "    num_workers: int,\n",
        "    mols_pkl_dir: str,\n",
        "):\n",
        "\n",
        "    print(f\"[{datetime.now()}] Construcing motif vocabulary from {train_path}.\")\n",
        "    print(f\"Number of workers: {num_workers}. Total number of CPUs: {mp.cpu_count()}.\")\n",
        "\n",
        "    data_set = [(idx, smi.strip(\"\\n\")) for idx, smi in enumerate(open(train_path))]\n",
        "    batch_size = (len(data_set) - 1) // num_workers + 1\n",
        "    batches = [data_set[i : i + batch_size] for i in range(0, len(data_set), batch_size)]\n",
        "    print(f\"Total: {len(data_set)} molecules.\\n\")\n",
        "\n",
        "    print(f\"Processing...\")\n",
        "    vocab = Counter()\n",
        "    os.makedirs(mols_pkl_dir, exist_ok=True)\n",
        "    MolGraph.load_operations(operation_path, num_operations)\n",
        "    func = partial(apply_operations, mols_pkl_dir=mols_pkl_dir)\n",
        "    with mp.Pool(num_workers, initializer=tqdm.set_lock, initargs=(mp.RLock(),)) as pool:\n",
        "        for batch_vocab in pool.imap(func, batches):\n",
        "            vocab = vocab + batch_vocab\n",
        "\n",
        "    atom_list = [x for (x, _) in vocab.keys() if x not in MolGraph.OPERATIONS]\n",
        "    atom_list.sort()\n",
        "    new_vocab = []\n",
        "    full_list = atom_list + MolGraph.OPERATIONS\n",
        "    for (x, y), value in vocab.items():\n",
        "        assert x in full_list\n",
        "        new_vocab.append((x, y, value))\n",
        "\n",
        "    index_dict = dict(zip(full_list, range(len(full_list))))\n",
        "    sorted_vocab = sorted(new_vocab, key=lambda x: index_dict[x[0]])\n",
        "    with open(vocab_path, \"w\") as f:\n",
        "        for (x, y, _) in sorted_vocab:\n",
        "            f.write(f\"{x} {y}\\n\")\n",
        "\n",
        "    print(f\"\\r[{datetime.now()}] Motif vocabulary construction finished.\")\n",
        "    print(f\"The motif vocabulary is in {vocab_path}.\\n\\n\")\n"
      ],
      "metadata": {
        "id": "sgE0HruOY_OC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# if __name__ == \"__main__\":\n",
        "\n",
        "#     args = parse_arguments()\n",
        "#     paths = Paths(args)\n",
        "#     os.makedirs(paths.preprocess_dir, exist_ok=True)\n",
        "\n",
        "#     motif_vocab_construction(\n",
        "#         train_path = paths.train_path,\n",
        "#         vocab_path = paths.vocab_path,\n",
        "#         operation_path = paths.operation_path,\n",
        "#         num_operations = args.num_operations,\n",
        "#         mols_pkl_dir = paths.mols_pkl_dir,\n",
        "#         num_workers = args.num_workers,\n",
        "#     )"
      ],
      "metadata": {
        "id": "JC5ndmzsZBtz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Make Training Data"
      ],
      "metadata": {
        "id": "KR37I8Ybng9J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import multiprocessing as mp\n",
        "import os\n",
        "import os.path as path\n",
        "import pickle\n",
        "from datetime import datetime\n",
        "from functools import partial\n",
        "from typing import List, Tuple\n",
        "\n",
        "import torch\n",
        "from tqdm import tqdm\n",
        "\n",
        "from arguments import parse_arguments\n",
        "from model.mol_graph import MolGraph\n",
        "from model.mydataclass import Paths"
      ],
      "metadata": {
        "id": "9qufoKTrn5Ay"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "S4155HjZkHob"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def process_train_batch(batch: List[str], raw_dir: str, save_dir: str):\n",
        "    pos = mp.current_process()._identity[0]\n",
        "    with tqdm(total = len(batch), desc=f\"Processing {pos}\", position=pos-1, ncols=80, leave=False) as pbar:\n",
        "        for file in batch:\n",
        "            with open(path.join(raw_dir, file), \"rb\") as f:\n",
        "                mol: MolGraph = pickle.load(f)\n",
        "            data = mol.get_data()\n",
        "            torch.save(data, path.join(save_dir, file.split()[0]+\".pth\"))\n",
        "            pbar.update()\n",
        "\n",
        "def process_valid_batch(batch: List[Tuple[int, str]], save_dir: str):\n",
        "    pos = mp.current_process()._identity[0]\n",
        "    with tqdm(total = len(batch), desc=f\"Processing {pos}\", position=pos-1, ncols=80, leave=False) as pbar:\n",
        "        for idx, smi in batch:\n",
        "            mol = MolGraph(smi, tokenizer=\"motif\")\n",
        "            data = mol.get_data()\n",
        "            torch.save(data, path.join(save_dir, f\"{idx}.pth\"))\n",
        "            pbar.update()\n",
        "\n",
        "def make_trainig_data(\n",
        "    mols_pkl_dir: str,\n",
        "    valid_path: str,\n",
        "    vocab_path: str,\n",
        "    train_processed_dir: str,\n",
        "    valid_processed_dir: str,\n",
        "    vocab_processed_path: str,\n",
        "    num_workers: int,\n",
        "):\n",
        "\n",
        "    print(f\"[{datetime.now()}] Preprocessing traing data.\")\n",
        "    print(f\"Number of workers: {num_workers}. Total number of CPUs: {mp.cpu_count()}.\\n\")\n",
        "\n",
        "\n",
        "    print(f\"[{datetime.now()}] Loading training set from {mols_pkl_dir}.\\n\")\n",
        "    os.makedirs(train_processed_dir, exist_ok=True)\n",
        "    data_set = os.listdir(mols_pkl_dir)\n",
        "    batch_size = (len(data_set) - 1) // num_workers + 1\n",
        "    batches = [data_set[i : i + batch_size] for i in range(0, len(data_set), batch_size)]\n",
        "    func = partial(process_train_batch, raw_dir=mols_pkl_dir, save_dir=train_processed_dir)\n",
        "    with mp.Pool(num_workers, initializer=tqdm.set_lock, initargs=(mp.RLock(),)) as pool:\n",
        "        pool.map(func, batches)\n",
        "\n",
        "\n",
        "    print(f\"[{datetime.now()}] Preprocessing valid set from {valid_path}.\\n\")\n",
        "    os.makedirs(valid_processed_dir, exist_ok=True)\n",
        "    data_set = [(idx, smi.strip(\"\\n\")) for idx, smi in enumerate(open(valid_path))]\n",
        "    batch_size = (len(data_set) - 1) // num_workers + 1\n",
        "    batches = [data_set[i : i + batch_size] for i in range(0, len(data_set), batch_size)]\n",
        "    func = partial(process_valid_batch, save_dir=valid_processed_dir)\n",
        "    with mp.Pool(num_workers, initializer=tqdm.set_lock, initargs=(mp.RLock(),)) as pool:\n",
        "        pool.map(func, batches)\n",
        "\n",
        "    print(f\"[{datetime.now()}] Preprocessing motif vocabulary from {vocab_path}.\\n\")\n",
        "    vocab_data = MolGraph.preprocess_vocab()\n",
        "    with open(vocab_processed_path, \"wb\") as f:\n",
        "        torch.save(vocab_data, f)\n",
        "\n",
        "    print(f\"[{datetime.now()}] Preprocessing finished.\\n\\n\")"
      ],
      "metadata": {
        "id": "uaZCXi-Ho6va"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from rdkit import Contrib\n",
        "from rdkit.Contrib import SA_Score\n",
        "from rdkit.Contrib.SA_Score import sascorer"
      ],
      "metadata": {
        "id": "KY-qYX5xoVaH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# if __name__ == \"__main__\":\n",
        "\n",
        "#     args = parse_arguments()\n",
        "#     paths  = Paths(args)\n",
        "\n",
        "#     MolGraph.load_operations(paths.operation_path, args.num_operations)\n",
        "#     MolGraph.load_vocab(paths.vocab_path)\n",
        "\n",
        "#     make_trainig_data(\n",
        "#         mols_pkl_dir = paths.mols_pkl_dir,\n",
        "#         valid_path = paths.valid_path,\n",
        "#         vocab_path = paths.vocab_path,\n",
        "#         train_processed_dir = paths.train_processed_dir,\n",
        "#         valid_processed_dir = paths.valid_processed_dir,\n",
        "#         vocab_processed_path = paths.vocab_processed_path,\n",
        "#         num_workers = args.num_workers,\n",
        "#     )\n"
      ],
      "metadata": {
        "id": "Ju82NxetkWK0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training MiCaM"
      ],
      "metadata": {
        "id": "OPrhFefsuw5E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import argparse\n",
        "import logging\n",
        "import os\n",
        "import os.path as path\n",
        "import random\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.optim.lr_scheduler as lr_scheduler\n",
        "from tensorboardX import SummaryWriter\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "from arguments import parse_arguments\n",
        "from model.dataset import MolsDataset, batch_collate\n",
        "from model.MiCaM_VAE import MiCaM, VAE_Output\n",
        "from model.mol_graph import MolGraph\n",
        "from model.mydataclass import ModelParams, Paths, TrainingParams\n",
        "from model.scheduler import beta_annealing_schedule\n",
        "\n",
        "from model.vocab import MotifVocab, SubMotifVocab, Vocab"
      ],
      "metadata": {
        "id": "HhGPVev0u0Jg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(args: argparse.Namespace):\n",
        "\n",
        "    torch.manual_seed(args.seed)\n",
        "    random.seed(args.seed)\n",
        "    paths = Paths(args)\n",
        "    tb = SummaryWriter(log_dir=paths.tensorboard_dir)\n",
        "\n",
        "    model_params = ModelParams(args)\n",
        "    training_params = TrainingParams(args)\n",
        "\n",
        "    MolGraph.load_operations(paths.operation_path)\n",
        "    MolGraph.load_vocab(paths.vocab_path)\n",
        "\n",
        "    os.makedirs(paths.output_dir)\n",
        "    log_file = path.join(paths.output_dir, \"train.log\")\n",
        "    print(f\"See {log_file} for log.\" )\n",
        "    logging.basicConfig(\n",
        "        filename = log_file,\n",
        "        filemode = \"w\",\n",
        "        format = \"[%(asctime)s]: %(message)s\",\n",
        "        level = logging.INFO\n",
        "    )\n",
        "\n",
        "    model = MiCaM(model_params)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=training_params.lr)\n",
        "\n",
        "    total_step, beta = 0, training_params.beta_min\n",
        "\n",
        "    logging.info(\"HyperParameters:\")\n",
        "    logging.info(model_params)\n",
        "    logging.info(training_params)\n",
        "\n",
        "    scheduler = lr_scheduler.ExponentialLR(optimizer, training_params.lr_anneal_rate)\n",
        "    beta_scheduler = beta_annealing_schedule(params=training_params, init_beta=beta, init_step=total_step)\n",
        "    train_dataset = MolsDataset(paths.train_processed_dir)\n",
        "\n",
        "    logging.info(f\"Begin training...\")\n",
        "    os.makedirs(paths.model_save_dir)\n",
        "    stop_train = False\n",
        "    i = 0\n",
        "    while True:\n",
        "        for input in DataLoader(dataset=train_dataset, batch_size=training_params.batch_size, shuffle=True, collate_fn=batch_collate):\n",
        "            print(i)\n",
        "\n",
        "            total_step += 1\n",
        "            model.zero_grad()\n",
        "\n",
        "            input = input\n",
        "            output: VAE_Output = model(input, beta=beta, prop_weight=training_params.prop_weight)\n",
        "\n",
        "            output.total_loss.backward()\n",
        "            nn.utils.clip_grad_norm_(model.parameters(), training_params.grad_clip_norm)\n",
        "\n",
        "            optimizer.step()\n",
        "            output.log_tb_results(total_step, tb, beta, scheduler.get_last_lr()[0])\n",
        "\n",
        "            if total_step % 50 == 0:\n",
        "                output.print_results(total_step, lr=scheduler.get_last_lr()[0], beta=beta)\n",
        "\n",
        "            if total_step % training_params.lr_anneal_iter == 0:\n",
        "                scheduler.step()\n",
        "\n",
        "            beta = beta_scheduler.step()\n",
        "\n",
        "            if total_step == training_params.steps:\n",
        "                stop_train = True\n",
        "                break\n",
        "\n",
        "        if stop_train: break\n",
        "\n",
        "    model.eval()\n",
        "    model.zero_grad()\n",
        "    torch.cuda.empty_cache()\n",
        "    model_path = path.join(paths.model_save_dir,\"model.ckpt\")\n",
        "    motifs_embed_path = path.join(paths.model_save_dir,\"motifs_embed.ckpt\" )\n",
        "    with torch.no_grad():\n",
        "        ckpt = (model.state_dict(), optimizer.state_dict(), total_step, beta)\n",
        "        torch.save(ckpt, model_path)\n",
        "        model.save_motifs_embed(motifs_embed_path)\n",
        "    tb.close()"
      ],
      "metadata": {
        "id": "KWDFtbCUu4tY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "\n",
        "    args = parse_arguments()\n",
        "\n",
        "    train(args)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "id": "25AWioy5u-nx",
        "outputId": "907e6de8-68c8-48b1-f46b-07fd3ca012ad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "See /content/drive/MyDrive/AGILE2/AI4Sci-MiCaM/output/12-13/18:03:37-train_micam/train.log for log.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-29-745bcc5e6eb2>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparse_arguments\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-28-7fd1fea0e310>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0mi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0minput\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining_params\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollate_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_collate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    628\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    629\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 630\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    631\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    632\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    672\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    673\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 674\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    675\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    676\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/content/drive/MyDrive/AGILE2/AI4Sci-MiCaM/src/model/dataset.py\u001b[0m in \u001b[0;36mbatch_collate\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m     67\u001b[0m                 \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconn_offset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m                 \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmotif_vocab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_conn_label\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmotif_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconn_idx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m             \u001b[0mbatch_train_graphs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/drive/MyDrive/AGILE2/AI4Sci-MiCaM/src/model/vocab.py\u001b[0m in \u001b[0;36mget_conn_label\u001b[0;34m(self, motif_idx, order_idx)\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_conn_label\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmotif_idx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder_idx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab_conn_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmotif_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0morder_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     98\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_conns_idx\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 3"
          ]
        }
      ]
    }
  ]
}