{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "18Opm689pwq1RBwlve0lAf7rrlEakuUC7",
      "authorship_tag": "ABX9TyPOv1EwF4MABCewI1lVtJCp",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rsaran-BioAI/AGILE/blob/main/FashionMNIST_VAE.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The key differences between the VAE and a standard autoencoder are:\n",
        "\n",
        "The encode function outputs two vectors (for mu and logvar). These represent the parameters of the Gaussian distribution from which we'll sample to obtain the latent vector.\n",
        "The reparameterize function takes mu and logvar and performs the \"reparameterization trick\" to allow for gradient descent to work.\n",
        "The loss_function is different. It combines reconstruction loss (BCE) with the KL divergence (KLD) between the learned latent distribution and the prior distribution, enforcing the latent space to assume a normal distribution which helps in generating new samples.\n",
        "The rest of the training and testing loop remains the same, but now includes the loss due to the KL divergence."
      ],
      "metadata": {
        "id": "ey0vUZxCaWWt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets\n",
        "from torchvision.transforms import ToTensor\n",
        "import torch.nn.functional as F"
      ],
      "metadata": {
        "id": "iq6ii2oIafGO"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare the data\n",
        "training_data = datasets.FashionMNIST(\n",
        "    root=\"data\",\n",
        "    train=True,\n",
        "    download=True,\n",
        "    transform=ToTensor()\n",
        ")\n",
        "\n",
        "test_data = datasets.FashionMNIST(\n",
        "    root=\"data\",\n",
        "    train=False,\n",
        "    download=True,\n",
        "    transform=ToTensor()\n",
        ")\n",
        "\n",
        "train_dataloader = DataLoader(training_data, batch_size=64)\n",
        "test_dataloader = DataLoader(test_data, batch_size=64)\n"
      ],
      "metadata": {
        "id": "q56kMa0Jayjn",
        "outputId": "fc753a05-9023-43a3-bb35-314245571567",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to data/FashionMNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 26421880/26421880 [00:01<00:00, 14250588.46it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/FashionMNIST/raw/train-images-idx3-ubyte.gz to data/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to data/FashionMNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 29515/29515 [00:00<00:00, 228039.06it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/FashionMNIST/raw/train-labels-idx1-ubyte.gz to data/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4422102/4422102 [00:01<00:00, 4210232.44it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz to data/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 5148/5148 [00:00<00:00, 18662296.45it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz to data/FashionMNIST/raw\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Variational Autoencoder Definition\n",
        "class VAE(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(VAE, self).__init__()\n",
        "\n",
        "        self.fc1 = nn.Linear(28*28, 400)\n",
        "        self.fc21 = nn.Linear(400, 20)  # Mean vector\n",
        "        self.fc22 = nn.Linear(400, 20)  # Variance vector (log_var)\n",
        "        self.fc3 = nn.Linear(20, 400)\n",
        "        self.fc4 = nn.Linear(400, 28*28)\n",
        "\n",
        "    def encode(self, x):\n",
        "        h1 = F.relu(self.fc1(x))\n",
        "        return self.fc21(h1), self.fc22(h1)\n",
        "\n",
        "    def reparameterize(self, mu, logvar):\n",
        "        std = torch.exp(0.5*logvar)\n",
        "        eps = torch.randn_like(std)\n",
        "        return mu + eps*std\n",
        "\n",
        "    def decode(self, z):\n",
        "        h3 = F.relu(self.fc3(z))\n",
        "        return torch.sigmoid(self.fc4(h3))\n",
        "\n",
        "    def forward(self, x):\n",
        "        mu, logvar = self.encode(x.view(-1, 28*28))\n",
        "        z = self.reparameterize(mu, logvar)\n",
        "        return self.decode(z), mu, logvar"
      ],
      "metadata": {
        "id": "w6sZV2cba49Q"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Instantiate the model\n",
        "model = VAE()\n",
        "\n",
        "# Loss function\n",
        "def loss_function(recon_x, x, mu, logvar):\n",
        "    BCE = F.binary_cross_entropy(recon_x, x.view(-1, 28*28), reduction='sum')\n",
        "    # KL divergence regularization term.\n",
        "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
        "    return BCE + KLD\n",
        "\n",
        "# Optimizer\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)"
      ],
      "metadata": {
        "id": "VkolONBJbD2X"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training Loop\n",
        "def train_loop(dataloader, model, loss_fn, optimizer):\n",
        "    model.train()\n",
        "    train_loss = 0\n",
        "    for batch_idx, (data, _) in enumerate(dataloader):\n",
        "        optimizer.zero_grad()\n",
        "        recon_batch, mu, logvar = model(data)\n",
        "        loss = loss_fn(recon_batch, data, mu, logvar)\n",
        "        loss.backward()\n",
        "        train_loss += loss.item()\n",
        "        optimizer.step()\n",
        "\n",
        "        if batch_idx % 100 == 0:\n",
        "            print(f\"Train Epoch: {batch_idx} Loss: {loss.item() / len(data):.6f}\")\n",
        "\n",
        "    print(f\"====> Epoch: Train loss: {train_loss / len(dataloader.dataset):.6f}\")"
      ],
      "metadata": {
        "id": "NQyHYvi3bHhP"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the VAE\n",
        "epochs = 10\n",
        "for epoch in range(1, epochs + 1):\n",
        "    train_loop(train_dataloader, model, loss_function, optimizer)"
      ],
      "metadata": {
        "id": "8K37B9MFbQuZ",
        "outputId": "006c3c9c-1329-47b2-8641-8775605a997e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Epoch: 0 Loss: 550.158569\n",
            "Train Epoch: 100 Loss: 292.511078\n",
            "Train Epoch: 200 Loss: 276.104584\n",
            "Train Epoch: 300 Loss: 278.024261\n",
            "Train Epoch: 400 Loss: 260.506073\n",
            "Train Epoch: 500 Loss: 273.031555\n",
            "Train Epoch: 600 Loss: 259.550385\n",
            "Train Epoch: 700 Loss: 255.688828\n",
            "Train Epoch: 800 Loss: 256.657593\n",
            "Train Epoch: 900 Loss: 261.208160\n",
            "====> Epoch: Train loss: 274.672749\n",
            "Train Epoch: 0 Loss: 252.666916\n",
            "Train Epoch: 100 Loss: 252.590408\n",
            "Train Epoch: 200 Loss: 254.644073\n",
            "Train Epoch: 300 Loss: 259.366730\n",
            "Train Epoch: 400 Loss: 244.908524\n",
            "Train Epoch: 500 Loss: 260.511841\n",
            "Train Epoch: 600 Loss: 249.384155\n",
            "Train Epoch: 700 Loss: 245.076050\n",
            "Train Epoch: 800 Loss: 247.370789\n",
            "Train Epoch: 900 Loss: 253.362823\n",
            "====> Epoch: Train loss: 251.649328\n",
            "Train Epoch: 0 Loss: 246.994171\n",
            "Train Epoch: 100 Loss: 246.609955\n",
            "Train Epoch: 200 Loss: 247.089233\n",
            "Train Epoch: 300 Loss: 254.366776\n",
            "Train Epoch: 400 Loss: 241.017242\n",
            "Train Epoch: 500 Loss: 256.787476\n",
            "Train Epoch: 600 Loss: 245.493652\n",
            "Train Epoch: 700 Loss: 242.064377\n",
            "Train Epoch: 800 Loss: 245.864944\n",
            "Train Epoch: 900 Loss: 252.128143\n",
            "====> Epoch: Train loss: 247.290488\n",
            "Train Epoch: 0 Loss: 243.124313\n",
            "Train Epoch: 100 Loss: 245.168732\n",
            "Train Epoch: 200 Loss: 246.257736\n",
            "Train Epoch: 300 Loss: 251.858429\n",
            "Train Epoch: 400 Loss: 238.379395\n",
            "Train Epoch: 500 Loss: 254.073578\n",
            "Train Epoch: 600 Loss: 243.611267\n",
            "Train Epoch: 700 Loss: 240.041534\n",
            "Train Epoch: 800 Loss: 243.631073\n",
            "Train Epoch: 900 Loss: 250.233139\n",
            "====> Epoch: Train loss: 245.289470\n",
            "Train Epoch: 0 Loss: 241.262238\n",
            "Train Epoch: 100 Loss: 243.649506\n",
            "Train Epoch: 200 Loss: 244.454987\n",
            "Train Epoch: 300 Loss: 251.258118\n",
            "Train Epoch: 400 Loss: 236.867096\n",
            "Train Epoch: 500 Loss: 252.715958\n",
            "Train Epoch: 600 Loss: 242.843918\n",
            "Train Epoch: 700 Loss: 239.964630\n",
            "Train Epoch: 800 Loss: 242.866043\n",
            "Train Epoch: 900 Loss: 249.702499\n",
            "====> Epoch: Train loss: 244.168421\n",
            "Train Epoch: 0 Loss: 239.737228\n",
            "Train Epoch: 100 Loss: 243.256012\n",
            "Train Epoch: 200 Loss: 243.673218\n",
            "Train Epoch: 300 Loss: 250.845001\n",
            "Train Epoch: 400 Loss: 236.468933\n",
            "Train Epoch: 500 Loss: 252.553024\n",
            "Train Epoch: 600 Loss: 242.046265\n",
            "Train Epoch: 700 Loss: 239.115005\n",
            "Train Epoch: 800 Loss: 242.140152\n",
            "Train Epoch: 900 Loss: 249.206055\n",
            "====> Epoch: Train loss: 243.398105\n",
            "Train Epoch: 0 Loss: 239.244232\n",
            "Train Epoch: 100 Loss: 241.836441\n",
            "Train Epoch: 200 Loss: 243.218338\n",
            "Train Epoch: 300 Loss: 250.007660\n",
            "Train Epoch: 400 Loss: 236.581467\n",
            "Train Epoch: 500 Loss: 252.486389\n",
            "Train Epoch: 600 Loss: 241.167007\n",
            "Train Epoch: 700 Loss: 238.587341\n",
            "Train Epoch: 800 Loss: 241.476929\n",
            "Train Epoch: 900 Loss: 248.463379\n",
            "====> Epoch: Train loss: 242.778247\n",
            "Train Epoch: 0 Loss: 238.483719\n",
            "Train Epoch: 100 Loss: 241.490707\n",
            "Train Epoch: 200 Loss: 243.009018\n",
            "Train Epoch: 300 Loss: 249.566605\n",
            "Train Epoch: 400 Loss: 235.020462\n",
            "Train Epoch: 500 Loss: 251.031952\n",
            "Train Epoch: 600 Loss: 241.375412\n",
            "Train Epoch: 700 Loss: 238.293930\n",
            "Train Epoch: 800 Loss: 241.372925\n",
            "Train Epoch: 900 Loss: 248.275208\n",
            "====> Epoch: Train loss: 242.295089\n",
            "Train Epoch: 0 Loss: 238.216782\n",
            "Train Epoch: 100 Loss: 241.550339\n",
            "Train Epoch: 200 Loss: 242.270233\n",
            "Train Epoch: 300 Loss: 248.972565\n",
            "Train Epoch: 400 Loss: 235.111969\n",
            "Train Epoch: 500 Loss: 251.431427\n",
            "Train Epoch: 600 Loss: 240.438904\n",
            "Train Epoch: 700 Loss: 237.889954\n",
            "Train Epoch: 800 Loss: 240.342346\n",
            "Train Epoch: 900 Loss: 248.129669\n",
            "====> Epoch: Train loss: 241.924531\n",
            "Train Epoch: 0 Loss: 236.665833\n",
            "Train Epoch: 100 Loss: 240.939865\n",
            "Train Epoch: 200 Loss: 240.883392\n",
            "Train Epoch: 300 Loss: 249.691681\n",
            "Train Epoch: 400 Loss: 233.957932\n",
            "Train Epoch: 500 Loss: 250.259613\n",
            "Train Epoch: 600 Loss: 241.622253\n",
            "Train Epoch: 700 Loss: 237.329544\n",
            "Train Epoch: 800 Loss: 240.726273\n",
            "Train Epoch: 900 Loss: 246.959946\n",
            "====> Epoch: Train loss: 241.534200\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Testing Loop\n",
        "def test_loop(dataloader, model, loss_fn):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for data, _ in dataloader:\n",
        "            recon_batch, mu, logvar = model(data)\n",
        "            test_loss += loss_fn(recon_batch, data, mu, logvar).item()\n",
        "\n",
        "    test_loss /= len(dataloader.dataset)\n",
        "    print(f\"====> Test set loss: {test_loss:.6f}\")"
      ],
      "metadata": {
        "id": "QbziJ60zaGTM"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_loop(test_dataloader, model, loss_function)"
      ],
      "metadata": {
        "id": "vreual7ObUgJ",
        "outputId": "7e7d82fa-4ea3-40cb-db51-01450a65427a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "====> Test set loss: 243.320911\n"
          ]
        }
      ]
    }
  ]
}