{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP4hH8ObIqvJOvCXvWoeZw3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rsaran-BioAI/AGILE/blob/main/SMILES_GraphRep_AGILE_VAE.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B-tx-9S6hkKz"
      },
      "outputs": [],
      "source": [
        "# Connecting the drive with Colab\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## This is the code from AGILE for preparing data (dataset.py)"
      ],
      "metadata": {
        "id": "o27A7eUWyw-L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import codecs\n",
        "import os\n",
        "import csv\n",
        "import math\n",
        "import time\n",
        "import random\n",
        "import networkx as nx\n",
        "import numpy as np\n",
        "from copy import deepcopy\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# from torch.utils.data import Dataset, DataLoader\n",
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "import torchvision.transforms as transforms"
      ],
      "metadata": {
        "id": "Jko8ldglh7ah"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch_scatter"
      ],
      "metadata": {
        "id": "hxdpgJdph8EP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch_geometric"
      ],
      "metadata": {
        "id": "hARtEDxGh97i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch_scatter import scatter\n",
        "from torch_geometric.data import Data, Dataset, DataLoader\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "import rdkit\n",
        "from rdkit import Chem\n",
        "from rdkit.Chem.rdchem import HybridizationType\n",
        "from rdkit.Chem.rdchem import BondType as BT\n",
        "from rdkit.Chem import AllChem"
      ],
      "metadata": {
        "id": "1Ox4fJroh_rx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ATOM_LIST = list(range(1, 119))\n",
        "CHIRALITY_LIST = [\n",
        "    Chem.rdchem.ChiralType.CHI_UNSPECIFIED,\n",
        "    Chem.rdchem.ChiralType.CHI_TETRAHEDRAL_CW,\n",
        "    Chem.rdchem.ChiralType.CHI_TETRAHEDRAL_CCW,\n",
        "    Chem.rdchem.ChiralType.CHI_OTHER,\n",
        "]\n",
        "BOND_LIST = [BT.SINGLE, BT.DOUBLE, BT.TRIPLE, BT.AROMATIC]\n",
        "BONDDIR_LIST = [\n",
        "    Chem.rdchem.BondDir.NONE,\n",
        "    Chem.rdchem.BondDir.ENDUPRIGHT,\n",
        "    Chem.rdchem.BondDir.ENDDOWNRIGHT,\n",
        "]"
      ],
      "metadata": {
        "id": "79b6_y_0iB-h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def read_smiles(data_path, smiles_col=\"smiles\"): # defines a function named read_smiles with two parameters:\n",
        "                       # data_path (a string representing the path to a data file) and smiles_col (a string representing the column name in the data file that contains SMILES strings).\n",
        "                       # The default value of smiles_col is set to \"smiles\".\n",
        "    smiles_data = [] # An empty list smiles_data is initialized. This list will be used to store the SMILES strings extracted from the file.\n",
        "    with open(data_path, \"r\") as f: # This line opens the file located at the path data_path in read mode (\"r\"). The file is referred to as f within the with block.\n",
        "        reader = csv.reader(f) # A CSV reader object is created using the file object f. This reader will be used to iterate over the rows in the CSV file.\n",
        "        headers = next(reader) # The first line of the CSV file, typically containing the column headers, is read. This line is stored in the variable headers.\n",
        "        smiles_idx = headers.index(smiles_col) # The index of the column containing SMILES strings is determined by finding the position of smiles_col in the headers list.\n",
        "        for row in reader: # This loop iterates over each row in the CSV file, starting from the second line (since the first line containing headers has already been read).\n",
        "            smiles = row[smiles_idx].strip() # The SMILES string is extracted from the row using the column index smiles_idx. Any leading or trailing whitespace is removed using strip().\n",
        "            smiles = codecs.decode(smiles, \"unicode_escape\") # The extracted SMILES string is decoded.\n",
        "                                                   # This is necessary because SMILES strings might contain escaped unicode characters, and this step ensures they are correctly interpreted.\n",
        "            smiles_data.append(smiles) # The decoded SMILES string is appended to the smiles_data list.\n",
        "    return smiles_data # After all rows in the file have been processed, the function returns the list of SMILES strings."
      ],
      "metadata": {
        "id": "6bGVernt7DLR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MoleculeDataset(Dataset):\n",
        "    def __init__(self, data_path):\n",
        "        super(Dataset, self).__init__()\n",
        "        self.smiles_data = read_smiles(data_path)\n",
        "        self.smiles_data = self.validate_smiles()\n",
        "        print(f\"Number of valid smiles: {len(self.smiles_data)}\")\n",
        "        print(f\"Valid example smiles: '{self.smiles_data[0]}'\")\n",
        "\n",
        "    def validate_smiles(self):\n",
        "        valid_smiles = []\n",
        "        for i, smiles in enumerate(self.smiles_data):\n",
        "            mol = Chem.MolFromSmiles(smiles)\n",
        "            if mol is not None:\n",
        "                valid_smiles.append(smiles)\n",
        "            else:\n",
        "                print(f\"Invalid smiles: at index {i} '{smiles}'\")\n",
        "        return valid_smiles\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        mol = Chem.MolFromSmiles(self.smiles_data[index])\n",
        "        # mol = Chem.AddHs(mol)\n",
        "\n",
        "        try:\n",
        "            N = mol.GetNumAtoms()\n",
        "            M = mol.GetNumBonds()\n",
        "        except:\n",
        "            print(f\"Invalid SMILES: at index {index}\")\n",
        "            raise ValueError\n",
        "\n",
        "        type_idx = []\n",
        "        chirality_idx = []\n",
        "        atomic_number = []\n",
        "        # aromatic = []\n",
        "        # sp, sp2, sp3, sp3d = [], [], [], []\n",
        "        # num_hs = []\n",
        "        for atom in mol.GetAtoms():\n",
        "            type_idx.append(ATOM_LIST.index(atom.GetAtomicNum()))\n",
        "            chirality_idx.append(CHIRALITY_LIST.index(atom.GetChiralTag()))\n",
        "            atomic_number.append(atom.GetAtomicNum())\n",
        "            # aromatic.append(1 if atom.GetIsAromatic() else 0)\n",
        "            # hybridization = atom.GetHybridization()\n",
        "            # sp.append(1 if hybridization == HybridizationType.SP else 0)\n",
        "            # sp2.append(1 if hybridization == HybridizationType.SP2 else 0)\n",
        "            # sp3.append(1 if hybridization == HybridizationType.SP3 else 0)\n",
        "            # sp3d.append(1 if hybridization == HybridizationType.SP3D else 0)\n",
        "\n",
        "        # z = torch.tensor(atomic_number, dtype=torch.long)\n",
        "        x1 = torch.tensor(type_idx, dtype=torch.long).view(-1, 1)\n",
        "        x2 = torch.tensor(chirality_idx, dtype=torch.long).view(-1, 1)\n",
        "        x = torch.cat([x1, x2], dim=-1)\n",
        "        # x2 = torch.tensor([atomic_number, aromatic, sp, sp2, sp3, sp3d, num_hs],\n",
        "        #                     dtype=torch.float).t().contiguous()\n",
        "        # x = torch.cat([x1.to(torch.float), x2], dim=-1)\n",
        "\n",
        "        row, col, edge_feat = [], [], []\n",
        "        for bond in mol.GetBonds():\n",
        "            start, end = bond.GetBeginAtomIdx(), bond.GetEndAtomIdx()\n",
        "            row += [start, end]\n",
        "            col += [end, start]\n",
        "            # edge_type += 2 * [MOL_BONDS[bond.GetBondType()]]\n",
        "            edge_feat.append(\n",
        "                [\n",
        "                    BOND_LIST.index(bond.GetBondType()),\n",
        "                    BONDDIR_LIST.index(bond.GetBondDir()),\n",
        "                ]\n",
        "            )\n",
        "            edge_feat.append(\n",
        "                [\n",
        "                    BOND_LIST.index(bond.GetBondType()),\n",
        "                    BONDDIR_LIST.index(bond.GetBondDir()),\n",
        "                ]\n",
        "            )\n",
        "\n",
        "        edge_index = torch.tensor([row, col], dtype=torch.long)\n",
        "        edge_attr = torch.tensor(np.array(edge_feat), dtype=torch.long)\n",
        "\n",
        "        # random mask a subgraph of the molecule\n",
        "        num_mask_nodes = max([1, math.floor(0.25 * N)])\n",
        "        num_mask_edges = max([0, math.floor(0.25 * M)])\n",
        "        mask_nodes_i = random.sample(list(range(N)), num_mask_nodes)\n",
        "        mask_nodes_j = random.sample(list(range(N)), num_mask_nodes)\n",
        "        mask_edges_i_single = random.sample(list(range(M)), num_mask_edges)\n",
        "        mask_edges_j_single = random.sample(list(range(M)), num_mask_edges)\n",
        "        mask_edges_i = [2 * i for i in mask_edges_i_single] + [\n",
        "            2 * i + 1 for i in mask_edges_i_single\n",
        "        ]\n",
        "        mask_edges_j = [2 * i for i in mask_edges_j_single] + [\n",
        "            2 * i + 1 for i in mask_edges_j_single\n",
        "        ]\n",
        "\n",
        "        x_i = deepcopy(x)\n",
        "        for atom_idx in mask_nodes_i:\n",
        "            x_i[atom_idx, :] = torch.tensor([len(ATOM_LIST), 0])\n",
        "        edge_index_i = torch.zeros((2, 2 * (M - num_mask_edges)), dtype=torch.long)\n",
        "        edge_attr_i = torch.zeros((2 * (M - num_mask_edges), 2), dtype=torch.long)\n",
        "        count = 0\n",
        "        for bond_idx in range(2 * M):\n",
        "            if bond_idx not in mask_edges_i:\n",
        "                edge_index_i[:, count] = edge_index[:, bond_idx]\n",
        "                edge_attr_i[count, :] = edge_attr[bond_idx, :]\n",
        "                count += 1\n",
        "        data_i = Data(x=x_i, edge_index=edge_index_i, edge_attr=edge_attr_i)\n",
        "\n",
        "        x_j = deepcopy(x)\n",
        "        for atom_idx in mask_nodes_j:\n",
        "            x_j[atom_idx, :] = torch.tensor([len(ATOM_LIST), 0])\n",
        "        edge_index_j = torch.zeros((2, 2 * (M - num_mask_edges)), dtype=torch.long)\n",
        "        edge_attr_j = torch.zeros((2 * (M - num_mask_edges), 2), dtype=torch.long)\n",
        "        count = 0\n",
        "        for bond_idx in range(2 * M):\n",
        "            if bond_idx not in mask_edges_j:\n",
        "                edge_index_j[:, count] = edge_index[:, bond_idx]\n",
        "                edge_attr_j[count, :] = edge_attr[bond_idx, :]\n",
        "                count += 1\n",
        "        data_j = Data(x=x_j, edge_index=edge_index_j, edge_attr=edge_attr_j)\n",
        "\n",
        "        return data_i, data_j\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.smiles_data)\n",
        "class MoleculeDatasetWrapper(object):\n",
        "    def __init__(self, batch_size, num_workers, valid_size, data_path):\n",
        "        super(object, self).__init__()\n",
        "        self.data_path = data_path\n",
        "        self.batch_size = batch_size\n",
        "        self.num_workers = num_workers\n",
        "        self.valid_size = valid_size\n",
        "\n",
        "    def get_data_loaders(self):\n",
        "        train_dataset = MoleculeDataset(data_path=self.data_path)\n",
        "        train_loader, valid_loader = self.get_train_validation_data_loaders(\n",
        "            train_dataset\n",
        "        )\n",
        "        return train_loader, valid_loader\n",
        "\n",
        "    def get_train_validation_data_loaders(self, train_dataset):\n",
        "        # obtain training indices that will be used for validation\n",
        "        num_train = len(train_dataset)\n",
        "        indices = list(range(num_train))\n",
        "        np.random.shuffle(indices)\n",
        "\n",
        "        split = int(np.floor(self.valid_size * num_train))\n",
        "        train_idx, valid_idx = indices[split:], indices[:split]\n",
        "\n",
        "        # define samplers for obtaining training and validation batches\n",
        "        train_sampler = SubsetRandomSampler(train_idx)\n",
        "        valid_sampler = SubsetRandomSampler(valid_idx)\n",
        "\n",
        "        train_loader = DataLoader(\n",
        "            train_dataset,\n",
        "            batch_size=self.batch_size,\n",
        "            sampler=train_sampler,\n",
        "            num_workers=self.num_workers,\n",
        "            drop_last=True,\n",
        "        )\n",
        "\n",
        "        valid_loader = DataLoader(\n",
        "            train_dataset,\n",
        "            batch_size=self.batch_size,\n",
        "            sampler=valid_sampler,\n",
        "            num_workers=self.num_workers,\n",
        "            drop_last=True,\n",
        "        )\n",
        "\n",
        "        return train_loader, valid_loader"
      ],
      "metadata": {
        "id": "VaLUY_Vvq0Bl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_path = '/content/drive/MyDrive/AGILE2/SMILES_features.csv'"
      ],
      "metadata": {
        "id": "5ovCjh9hiWMT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 32  # Example size, adjust as needed\n",
        "num_workers = 2  # Adjust based on your machine's capabilities\n",
        "valid_size = 0.2  # 20% of the data for validation, adjust as needed"
      ],
      "metadata": {
        "id": "mJJaDx9BiX2a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_wrapper = MoleculeDatasetWrapper(batch_size, num_workers, valid_size, data_path)"
      ],
      "metadata": {
        "id": "rek69UJAiYQZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader, valid_loader = dataset_wrapper.get_data_loaders()"
      ],
      "metadata": {
        "id": "GKSlOGnLicLa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "DDsXEdQMiezK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# This code is from AGILE for bilding the VAE"
      ],
      "metadata": {
        "id": "AseAbxPacNkM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch_geometric.nn import GCNConv\n",
        "from torch_geometric.nn import MessagePassing\n",
        "from torch_geometric.utils import add_self_loops\n",
        "from torch_geometric.nn import global_add_pool, global_mean_pool, global_max_pool"
      ],
      "metadata": {
        "id": "-Dwd4zys9_kl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_atom_type = 119  # including the extra mask tokens\n",
        "num_chirality_tag = 3\n",
        "\n",
        "num_bond_type = 5  # including aromatic and self-loop edge\n",
        "num_bond_direction = 3\n",
        "\n",
        "\n",
        "class GINEConv(MessagePassing):\n",
        "    def __init__(self, emb_dim):\n",
        "        super(GINEConv, self).__init__()\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(emb_dim, 2 * emb_dim), nn.ReLU(), nn.Linear(2 * emb_dim, emb_dim)\n",
        "        )\n",
        "        self.edge_embedding1 = nn.Embedding(num_bond_type, emb_dim)\n",
        "        self.edge_embedding2 = nn.Embedding(num_bond_direction, emb_dim)\n",
        "        nn.init.xavier_uniform_(self.edge_embedding1.weight.data)\n",
        "        nn.init.xavier_uniform_(self.edge_embedding2.weight.data)\n",
        "\n",
        "    def forward(self, x, edge_index, edge_attr):\n",
        "        # add self loops in the edge space\n",
        "        edge_index = add_self_loops(edge_index, num_nodes=x.size(0))[0]\n",
        "\n",
        "        # add features corresponding to self-loop edges.\n",
        "        self_loop_attr = torch.zeros(x.size(0), 2)\n",
        "        self_loop_attr[:, 0] = 4  # bond type for self-loop edge\n",
        "        self_loop_attr = self_loop_attr.to(edge_attr.device).to(edge_attr.dtype)\n",
        "        edge_attr = torch.cat((edge_attr, self_loop_attr), dim=0)\n",
        "\n",
        "        edge_embeddings = self.edge_embedding1(edge_attr[:, 0]) + self.edge_embedding2(\n",
        "            edge_attr[:, 1]\n",
        "        )\n",
        "\n",
        "        return self.propagate(edge_index, x=x, edge_attr=edge_embeddings)\n",
        "\n",
        "    def message(self, x_j, edge_attr):\n",
        "        return x_j + edge_attr\n",
        "\n",
        "    def update(self, aggr_out):\n",
        "        return self.mlp(aggr_out)\n"
      ],
      "metadata": {
        "id": "9_T_rKS5Mfc_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class AGILE_Encoder(nn.Module):\n",
        "    \"\"\"\n",
        "    Modified AGILE class to serve as an encoder in a VAE.\n",
        "\n",
        "    Args:\n",
        "        num_layer (int): Number of GNN layers.\n",
        "        emb_dim (int): Dimensionality of embeddings.\n",
        "        feat_dim (int): Dimensionality of feature vector before producing mu and logvar.\n",
        "        latent_dim (int): Dimensionality of the latent space (output of the encoder).\n",
        "        drop_ratio (float): Dropout rate.\n",
        "    Output:\n",
        "        mu (torch.Tensor): Mean of the latent space distribution.\n",
        "        logvar (torch.Tensor): Log variance of the latent space distribution.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, num_layer=5, emb_dim=300, feat_dim=256, latent_dim=128, drop_ratio=0):\n",
        "        super(AGILE_Encoder, self).__init__()\n",
        "        self.num_layer = num_layer\n",
        "        self.emb_dim = emb_dim\n",
        "        self.feat_dim = feat_dim\n",
        "        self.latent_dim = latent_dim\n",
        "        self.drop_ratio = drop_ratio\n",
        "\n",
        "        self.x_embedding1 = nn.Embedding(num_atom_type, emb_dim)\n",
        "        self.x_embedding2 = nn.Embedding(num_chirality_tag, emb_dim)\n",
        "        nn.init.xavier_uniform_(self.x_embedding1.weight.data)\n",
        "        nn.init.xavier_uniform_(self.x_embedding2.weight.data)\n",
        "\n",
        "        # List of MLPs\n",
        "        self.gnns = nn.ModuleList()\n",
        "        for layer in range(num_layer):\n",
        "            self.gnns.append(GINEConv(emb_dim))\n",
        "\n",
        "        # List of batchnorms\n",
        "        self.batch_norms = nn.ModuleList()\n",
        "        for layer in range(num_layer):\n",
        "            self.batch_norms.append(nn.BatchNorm1d(emb_dim))\n",
        "\n",
        "        if pool == \"mean\":\n",
        "            self.pool = global_mean_pool\n",
        "        elif pool == \"max\":\n",
        "            self.pool = global_max_pool\n",
        "        elif pool == \"add\":\n",
        "            self.pool = global_add_pool\n",
        "\n",
        "        # Replacing the output layer with two separate layers for mu and logvar\n",
        "        self.feat_lin = nn.Linear(self.emb_dim, self.feat_dim)\n",
        "        self.mu_lin = nn.Linear(self.feat_dim, self.latent_dim)\n",
        "        self.logvar_lin = nn.Linear(self.feat_dim, self.latent_dim)"
      ],
      "metadata": {
        "id": "YLaFcIwpKGfJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GNNVAE(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(GNNVAE, self).__init__()\n",
        "\n",
        "        # Encoder: Using AGILE_Encoder\n",
        "        self.encoder = AGILE_Encoder(num_layer=num_layer, emb_dim=emb_dim,\n",
        "                                     feat_dim=feat_dim, latent_dim=latent_dim,\n",
        "                                     drop_ratio=drop_ratio, pool=pool) # This should return two values mu and logvar\n",
        "\n",
        "        # Decoder\n",
        "        self.decoder = torch.nn.Linear(latent_dim, num_classes) # Here the data should be regenerated in same dimension as encoded\n",
        "\n",
        "    def reparameterize(self, mu, logvar):\n",
        "        std = torch.exp(0.5 * logvar)\n",
        "        eps = torch.randn_like(std)\n",
        "        return eps.mul(std).add_(mu)\n",
        "\n",
        "    def decode(self, z):\n",
        "        return torch.sigmoid(self.decoder(z))\n",
        "\n",
        "    def forward(self, data):\n",
        "        mu, logvar = self.encoder(data) # Here data should be the dataset prepared\n",
        "        z = self.reparameterize(mu, logvar)\n",
        "        return self.decode(z), mu, logvar"
      ],
      "metadata": {
        "id": "ty6XorMfI7Qi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "class GNNVAE(torch.nn.Module):\n",
        "    def __init__(self, num_features, num_classes, latent_dim):\n",
        "        super(GNNVAE, self).__init__()\n",
        "        # Encoder\n",
        "        self.conv1 = GCNConv(num_features, 2 * latent_dim)\n",
        "        self.conv2 = GCNConv(2 * latent_dim, latent_dim)\n",
        "        \n",
        "        # Latent space\n",
        "        self.fc_mu = torch.nn.Linear(latent_dim, latent_dim)\n",
        "        self.fc_logvar = torch.nn.Linear(latent_dim, latent_dim)\n",
        "\n",
        "        # Decoder\n",
        "        self.decoder = torch.nn.Linear(latent_dim, num_classes)\n",
        "\n",
        "    def encode(self, x, edge_index):\n",
        "        x = F.relu(self.conv1(x, edge_index))\n",
        "        x = self.conv2(x, edge_index)\n",
        "        return self.fc_mu(x), self.fc_logvar(x)\n",
        "\n",
        "    def reparameterize(self, mu, logvar):\n",
        "        std = torch.exp(0.5*logvar)\n",
        "        eps = torch.randn_like(std)\n",
        "        return eps.mul(std).add_(mu)\n",
        "\n",
        "    def decode(self, z):\n",
        "        return torch.sigmoid(self.decoder(z))\n",
        "\n",
        "    def forward(self, data):\n",
        "        x, edge_index = data.x, data.edge_index\n",
        "        mu, logvar = self.encode(x, edge_index)\n",
        "        z = self.reparameterize(mu, logvar)\n",
        "        return self.decode(z), mu, logvar"
      ],
      "metadata": {
        "id": "KaILbt3AKzhR"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "y0EcRm_Miv3U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def loss_function(recon_x, x, mu, logvar):\n",
        "    BCE = F.binary_cross_entropy(recon_x, x, reduction='sum')\n",
        "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
        "    return BCE + KLD"
      ],
      "metadata": {
        "id": "SVrQK-gHAfA9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = GNNVAE()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Define a Training Loop here\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for data in train_loader:  # Assuming train_loader is your DataLoader\n",
        "        optimizer.zero_grad()\n",
        "        recon_batch, mu, logvar = model(data)\n",
        "        loss = loss_function(recon_batch, data.x, mu, logvar)\n",
        "        loss.backward()\n",
        "        total_loss += loss.item()\n",
        "        optimizer.step()\n",
        "\n",
        "    print('Epoch: {}, Loss: {:.4f}'.format(epoch, total_loss / len(train_loader.dataset)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 366
        },
        "id": "eernMRSQDHX4",
        "outputId": "5582ee16-9065-430e-aa23-f8bd7c616b95"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-104-99e163536c0d>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGNNVAE\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.001\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Define a Training Loop here\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-95-e3f256e15ae8>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0;31m# Encoder: Using AGILE_Encoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m         self.encoder = AGILE_Encoder(num_layer=num_layer, emb_dim=emb_dim, \n\u001b[0m\u001b[1;32m      7\u001b[0m                                      \u001b[0mfeat_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeat_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlatent_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlatent_dim\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m                                      drop_ratio=drop_ratio, pool=pool) # This should return two values mu and logvar\n",
            "\u001b[0;31mNameError\u001b[0m: name 'num_layer' is not defined"
          ]
        }
      ]
    }
  ]
}