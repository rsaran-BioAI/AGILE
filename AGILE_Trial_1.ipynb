{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled",
      "provenance": [],
      "mount_file_id": "https://github.com/rsaran-BioAI/AGILE/blob/main/AGILE_Trial_1.ipynb",
      "authorship_tag": "ABX9TyMn/INf8LPwO6PodGhQeBzZ"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Installation of requirements"
      ],
      "metadata": {
        "id": "azsK8X73UuoI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# install requirements\n",
        "\n",
        "!pip install torch==1.12.1+cu113 torchvision==0.13.1+cu113  --extra-index-url https://download.pytorch.org/whl/cu113\n"
      ],
      "metadata": {
        "id": "drb8TyfFFr_Q",
        "outputId": "4ac38d6a-782b-4b02-e56c-50c910b3fb67",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://download.pytorch.org/whl/cu113\n",
            "Requirement already satisfied: torch==1.12.1+cu113 in /usr/local/lib/python3.10/dist-packages (1.12.1+cu113)\n",
            "Requirement already satisfied: torchvision==0.13.1+cu113 in /usr/local/lib/python3.10/dist-packages (0.13.1+cu113)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch==1.12.1+cu113) (4.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision==0.13.1+cu113) (1.21.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchvision==0.13.1+cu113) (2.31.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision==0.13.1+cu113) (9.4.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision==0.13.1+cu113) (3.3.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision==0.13.1+cu113) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision==0.13.1+cu113) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision==0.13.1+cu113) (2023.7.22)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# install requirements\n",
        "\n",
        "!pip install torch-geometric==2.2.0 torch-sparse==0.6.16 torch-scatter==2.1.0 -f https://data.pyg.org/whl/torch-1.12.0+cu113.html"
      ],
      "metadata": {
        "id": "rnZo7oadGgby",
        "outputId": "d913c439-0b1d-4062-8442-d04718dd94b1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in links: https://data.pyg.org/whl/torch-1.12.0+cu113.html\n",
            "Requirement already satisfied: torch-geometric==2.2.0 in /usr/local/lib/python3.10/dist-packages (2.2.0)\n",
            "Requirement already satisfied: torch-sparse==0.6.16 in /usr/local/lib/python3.10/dist-packages (0.6.16+pt112cu113)\n",
            "Requirement already satisfied: torch-scatter==2.1.0 in /usr/local/lib/python3.10/dist-packages (2.1.0+pt112cu113)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torch-geometric==2.2.0) (4.66.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torch-geometric==2.2.0) (1.21.6)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from torch-geometric==2.2.0) (1.7.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch-geometric==2.2.0) (3.1.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torch-geometric==2.2.0) (2.31.0)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.10/dist-packages (from torch-geometric==2.2.0) (3.1.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from torch-geometric==2.2.0) (1.0.2)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.10/dist-packages (from torch-geometric==2.2.0) (5.9.5)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch-geometric==2.2.0) (2.1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric==2.2.0) (3.3.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric==2.2.0) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric==2.2.0) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric==2.2.0) (2023.7.22)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->torch-geometric==2.2.0) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->torch-geometric==2.2.0) (3.2.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Connecting the drive with Colab\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "a9wAv5mAGskJ",
        "outputId": "f328caae-16b1-43ab-c80e-9c1c16f2776c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Making sure the working directory is the one\n",
        "\n",
        "%cd /content/drive/MyDrive/AGILE"
      ],
      "metadata": {
        "id": "hDnkmT7IG2dU",
        "outputId": "03888af8-af60-4984-ce93-b9eceff5e517",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/AGILE\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# install requirements\n",
        "\n",
        "!pip install -r requirements.txt"
      ],
      "metadata": {
        "id": "u0SLdbBeIEKo",
        "outputId": "655daae9-0fb3-4086-9ca2-3bad08e93e6e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: exmol==3.0.2 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 1)) (3.0.2)\n",
            "Requirement already satisfied: matplotlib==3.5.3 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 2)) (3.5.3)\n",
            "Requirement already satisfied: networkx==2.6.3 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 3)) (2.6.3)\n",
            "Requirement already satisfied: numpy==1.21.6 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 4)) (1.21.6)\n",
            "Requirement already satisfied: pandas==1.3.5 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 5)) (1.3.5)\n",
            "Requirement already satisfied: PyYAML==6.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 6)) (6.0)\n",
            "Requirement already satisfied: rdkit==2023.3.1 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 7)) (2023.3.1)\n",
            "Requirement already satisfied: scikit-learn==1.0.2 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 8)) (1.0.2)\n",
            "Requirement already satisfied: scipy==1.7.3 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 9)) (1.7.3)\n",
            "Requirement already satisfied: seaborn==0.12.2 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 10)) (0.12.2)\n",
            "Requirement already satisfied: skunk==1.2.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 11)) (1.2.0)\n",
            "Requirement already satisfied: typing_extensions==4.4.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 12)) (4.4.0)\n",
            "Requirement already satisfied: umap-learn==0.5.3 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 13)) (0.5.3)\n",
            "Requirement already satisfied: tensorboard==2.13.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 14)) (2.13.0)\n",
            "Requirement already satisfied: selfies>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from exmol==3.0.2->-r requirements.txt (line 1)) (2.1.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from exmol==3.0.2->-r requirements.txt (line 1)) (2.31.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from exmol==3.0.2->-r requirements.txt (line 1)) (4.66.1)\n",
            "Requirement already satisfied: ratelimit in /usr/local/lib/python3.10/dist-packages (from exmol==3.0.2->-r requirements.txt (line 1)) (2.2.1)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.10/dist-packages (from exmol==3.0.2->-r requirements.txt (line 1)) (6.1.0)\n",
            "Requirement already satisfied: synspace in /usr/local/lib/python3.10/dist-packages (from exmol==3.0.2->-r requirements.txt (line 1)) (0.3.0)\n",
            "Requirement already satisfied: langchain in /usr/local/lib/python3.10/dist-packages (from exmol==3.0.2->-r requirements.txt (line 1)) (0.0.324)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib==3.5.3->-r requirements.txt (line 2)) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib==3.5.3->-r requirements.txt (line 2)) (4.43.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib==3.5.3->-r requirements.txt (line 2)) (1.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib==3.5.3->-r requirements.txt (line 2)) (23.2)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib==3.5.3->-r requirements.txt (line 2)) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.2.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib==3.5.3->-r requirements.txt (line 2)) (3.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib==3.5.3->-r requirements.txt (line 2)) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.10/dist-packages (from pandas==1.3.5->-r requirements.txt (line 5)) (2023.3.post1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.10/dist-packages (from scikit-learn==1.0.2->-r requirements.txt (line 8)) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn==1.0.2->-r requirements.txt (line 8)) (3.2.0)\n",
            "Requirement already satisfied: numba>=0.49 in /usr/local/lib/python3.10/dist-packages (from umap-learn==0.5.3->-r requirements.txt (line 13)) (0.56.4)\n",
            "Requirement already satisfied: pynndescent>=0.5 in /usr/local/lib/python3.10/dist-packages (from umap-learn==0.5.3->-r requirements.txt (line 13)) (0.5.10)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.10/dist-packages (from tensorboard==2.13.0->-r requirements.txt (line 14)) (1.4.0)\n",
            "Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.10/dist-packages (from tensorboard==2.13.0->-r requirements.txt (line 14)) (1.59.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard==2.13.0->-r requirements.txt (line 14)) (2.17.3)\n",
            "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard==2.13.0->-r requirements.txt (line 14)) (1.0.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard==2.13.0->-r requirements.txt (line 14)) (3.5)\n",
            "Requirement already satisfied: protobuf>=3.19.6 in /usr/local/lib/python3.10/dist-packages (from tensorboard==2.13.0->-r requirements.txt (line 14)) (3.20.3)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard==2.13.0->-r requirements.txt (line 14)) (67.7.2)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard==2.13.0->-r requirements.txt (line 14)) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard==2.13.0->-r requirements.txt (line 14)) (3.0.1)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.10/dist-packages (from tensorboard==2.13.0->-r requirements.txt (line 14)) (0.41.2)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard==2.13.0->-r requirements.txt (line 14)) (5.3.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard==2.13.0->-r requirements.txt (line 14)) (0.3.0)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard==2.13.0->-r requirements.txt (line 14)) (1.16.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard==2.13.0->-r requirements.txt (line 14)) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard==2.13.0->-r requirements.txt (line 14)) (1.3.1)\n",
            "Requirement already satisfied: llvmlite<0.40,>=0.39.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba>=0.49->umap-learn==0.5.3->-r requirements.txt (line 13)) (0.39.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->exmol==3.0.2->-r requirements.txt (line 1)) (3.3.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->exmol==3.0.2->-r requirements.txt (line 1)) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->exmol==3.0.2->-r requirements.txt (line 1)) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->exmol==3.0.2->-r requirements.txt (line 1)) (2023.7.22)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard==2.13.0->-r requirements.txt (line 14)) (2.1.3)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain->exmol==3.0.2->-r requirements.txt (line 1)) (2.0.22)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain->exmol==3.0.2->-r requirements.txt (line 1)) (3.8.6)\n",
            "Requirement already satisfied: anyio<4.0 in /usr/local/lib/python3.10/dist-packages (from langchain->exmol==3.0.2->-r requirements.txt (line 1)) (3.7.1)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain->exmol==3.0.2->-r requirements.txt (line 1)) (4.0.3)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.10/dist-packages (from langchain->exmol==3.0.2->-r requirements.txt (line 1)) (0.6.1)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain->exmol==3.0.2->-r requirements.txt (line 1)) (1.33)\n",
            "Requirement already satisfied: langsmith<0.1.0,>=0.0.52 in /usr/local/lib/python3.10/dist-packages (from langchain->exmol==3.0.2->-r requirements.txt (line 1)) (0.0.52)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain->exmol==3.0.2->-r requirements.txt (line 1)) (1.10.13)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain->exmol==3.0.2->-r requirements.txt (line 1)) (8.2.3)\n",
            "Requirement already satisfied: pystow in /usr/local/lib/python3.10/dist-packages (from synspace->exmol==3.0.2->-r requirements.txt (line 1)) (0.5.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain->exmol==3.0.2->-r requirements.txt (line 1)) (23.1.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain->exmol==3.0.2->-r requirements.txt (line 1)) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain->exmol==3.0.2->-r requirements.txt (line 1)) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain->exmol==3.0.2->-r requirements.txt (line 1)) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain->exmol==3.0.2->-r requirements.txt (line 1)) (1.3.1)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<4.0->langchain->exmol==3.0.2->-r requirements.txt (line 1)) (1.3.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<4.0->langchain->exmol==3.0.2->-r requirements.txt (line 1)) (1.1.3)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain->exmol==3.0.2->-r requirements.txt (line 1)) (3.20.1)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain->exmol==3.0.2->-r requirements.txt (line 1)) (0.9.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain->exmol==3.0.2->-r requirements.txt (line 1)) (2.4)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard==2.13.0->-r requirements.txt (line 14)) (0.5.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard==2.13.0->-r requirements.txt (line 14)) (3.2.2)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain->exmol==3.0.2->-r requirements.txt (line 1)) (3.0.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from pystow->synspace->exmol==3.0.2->-r requirements.txt (line 1)) (8.1.7)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain->exmol==3.0.2->-r requirements.txt (line 1)) (1.0.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# The following is the content of the file 'config_pretrain.yaml'\n",
        "\n",
        "batch_size: 512 # batch size\n",
        "warm_up: 10 # warm-up epochs\n",
        "epochs: 2 # total number of epochs\n",
        "\n",
        "load_model: pretrained_gin # resume training\n",
        "eval_every_n_epochs: 1 # validation frequency\n",
        "save_every_n_epochs: 5 # automatic model saving frequecy\n",
        "log_every_n_steps: 50 # print training log frequency\n",
        "\n",
        "fp16_precision: False # float precision 16 (i.e. True/False)\n",
        "init_lr: 0.0005 # initial learning rate for Adam\n",
        "weight_decay: 1e-5 # weight decay for Adam\n",
        "gpu: cuda:0 # training GPU\n",
        "\n",
        "model:\n",
        "  num_layer: 5 # number of graph conv layers\n",
        "  emb_dim: 300 # embedding dimension in graph conv layers\n",
        "  feat_dim: 512 # output feature dimention\n",
        "  drop_ratio: 0 # dropout ratio\n",
        "  pool: mean # readout pooling (i.e., mean/max/add)\n",
        "\n",
        "aug: node # molecule graph augmentation strategy (i.e., node/subgraph/mix)\n",
        "dataset:\n",
        "  num_workers: 12 # dataloader number of workers\n",
        "  valid_size: 0.05 # ratio of validation data\n",
        "  data_path: data/{yourowndata}.csv # path of pre-training data\n",
        "\n",
        "loss:\n",
        "  temperature: 0.1 # temperature of NT-Xent loss\n",
        "  use_cosine_similarity: True # whether to use cosine similarity in NT-Xent loss (i.e. True/False)\n"
      ],
      "metadata": {
        "id": "6w6EeesTQimc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Further ahead from here, I will try to pre-train the MolCLR model with the file 'pretrain.py'"
      ],
      "metadata": {
        "id": "0QU4PQmcTdB1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "import sys\n",
        "import torch\n",
        "import yaml\n",
        "import numpy as np\n",
        "from datetime import datetime"
      ],
      "metadata": {
        "id": "E0MpRX0uNXq5"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# imports the torch.nn.functional module\n",
        "# The torch.nn.functional module contains various functions that are commonly used in neural network operations, such as activation functions, loss functions, and other operations commonly applied to tensors.\n",
        "\n",
        "import torch.nn.functional as F"
      ],
      "metadata": {
        "id": "7Xvqiq7jXgV6"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This line imports the SummaryWriter class from the torch.utils.tensorboard module.\n",
        "# SummaryWriter is a PyTorch utility that enables you to write TensorBoard-compatible logs.\n",
        "# TensorBoard is a visualization tool provided with TensorFlow, but PyTorch provides integration to use it with PyTorch models as well.\n",
        "\n",
        "from torch.utils.tensorboard import SummaryWriter"
      ],
      "metadata": {
        "id": "4bVUpCKKX77L"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# imports the CosineAnnealingLR class from the torch.optim.lr_scheduler module in PyTorch.\n",
        "# The learning rate is a hyperparameter that controls how much we are adjusting the weights of our network during training.\n",
        "# CosineAnnealingLR is a learning rate scheduler in PyTorch that reduces the learning rate following the cosine annealing schedule.\n",
        "# The learning rate starts at the initial value and is decreased following a cosine function until it reaches a minimum value and then starts increasing again.\n",
        "# This kind of schedule is often used to improve the convergence and generalization of neural networks during training.\n",
        "\n",
        "from torch.optim.lr_scheduler import CosineAnnealingLR"
      ],
      "metadata": {
        "id": "mRXCXaxpoxZ6"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Changing the working directory to 'utils'\n",
        "\n",
        "%cd /content/drive/MyDrive/AGILE"
      ],
      "metadata": {
        "id": "sfuEgK1r7rqa",
        "outputId": "dbf555c4-fea1-4ec1-e04f-35490c2fec7a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/AGILE\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#  imports the NTXentLoss class from a custom module named nt_xent inside a package or directory called utils.\n",
        "# This class likely contains the implementation of the NT-Xent loss function, which is commonly used in contrastive learning tasks.\n",
        "\n",
        "from utils.nt_xent import NTXentLoss"
      ],
      "metadata": {
        "id": "kDmsoDdg1muZ"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This code snippet checks if the Apex library is installed and imports it for mixed-precision training if it is available.\n",
        "# If Apex is not installed, it prints a message indicating that Apex needs to be installed from a specific GitHub repository.\n",
        "\n",
        "apex_support = False # Initializes the apex_support variable to False.\n",
        "try: #The code inside the try block attempts to import the amp module from the Apex library.\n",
        "    sys.path.append(\"./apex\") #  Adds the \"./apex\" directory to the Python system path, allowing Python to find the Apex module in that directory.\n",
        "    from apex import amp # Tries to import the amp module from the Apex library.\n",
        "\n",
        "    apex_support = True # If the import is successful, sets apex_support to True indicating that Apex is available.\n",
        "except: # If there is an ImportError (i.e., Apex is not installed), the code inside the except block is executed.\n",
        "    print(\n",
        "        \"Please install apex for mixed precision training from: https://github.com/NVIDIA/apex\" # Prints a message indicating that Apex needs to be installed and provides the GitHub repository URL for installation.\n",
        "    )\n",
        "    apex_support = False"
      ],
      "metadata": {
        "id": "YxwY5g4G8PMs"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Installing Nvidia apex\n",
        "\n",
        "import os, sys, shutil\n",
        "import time\n",
        "import gc\n",
        "from contextlib import contextmanager\n",
        "from pathlib import Path\n",
        "import random\n",
        "import numpy as np, pandas as pd\n",
        "from tqdm import tqdm, tqdm_notebook\n",
        "\n",
        "@contextmanager\n",
        "def timer(name):\n",
        "    t0 = time.time()\n",
        "    yield\n",
        "    print(f'[{name}] done in {time.time() - t0:.0f} s')\n",
        "\n",
        "USE_APEX = True\n",
        "\n",
        "if USE_APEX:\n",
        "            with timer('install Nvidia apex'):\n",
        "                # Installing Nvidia Apex\n",
        "                os.system('git clone https://github.com/NVIDIA/apex; cd apex; pip install -v --no-cache-dir' +\n",
        "                          ' --global-option=\"--cpp_ext\" --global-option=\"--cuda_ext\" ./')\n",
        "                os.system('rm -rf apex/.git') # too many files, Kaggle fails\n"
      ],
      "metadata": {
        "id": "0RBiIXQF_gUX",
        "outputId": "0bd4e838-d40c-4df5-c8de-5b176c8bee89",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[install Nvidia apex] done in 2 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from apex import amp\n"
      ],
      "metadata": {
        "id": "hMtgroCiCiFh"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This function appears to be responsible for saving a configuration file (config.yaml)\n",
        "# to a specified directory (model_checkpoints_folder).\n",
        "def _save_config_file(model_checkpoints_folder): # checks if the model_checkpoints_folder directory exists\n",
        "    if not os.path.exists(model_checkpoints_folder): # If the directory does not exist, it creates it\n",
        "        os.makedirs(model_checkpoints_folder) # copies a configuration file specified by args.config to this directory\n",
        "        shutil.copy(args.config, os.path.join(model_checkpoints_folder, \"config.yaml\")) #  copied file is named \"config.yaml\"\n",
        "\n",
        "\n",
        "class PreTrain(object): # initializer for a training process\n",
        "    def __init__(self, dataset, config): # constructor for the PreTrain class. It takes two arguments: dataset and config.\n",
        "        self.config = config # assigns the config argument to an instance variable self.config.\n",
        "        self.device = self._get_device() # It calls the _get_device() method to determine and store the...\n",
        "                                         # ...computing device (CPU or GPU) to be used for training\n",
        "\n",
        "        dir_name = datetime.now().strftime(\"%b%d_%H-%M-%S\") # generates a timestamp string in format \"MonthDay_Hour-Minute-Second\"\n",
        "        log_dir = os.path.join(\"ckpt\", dir_name)\n",
        "        # It creates a directory path by joining \"ckpt\" (presumably a checkpoint directory)...\n",
        "        # ...and the timestamp generated in the previous step. This path is where logs related to the training process will be saved.\n",
        "        self.writer = SummaryWriter(log_dir=log_dir) # It initializes a SummaryWriter object...\n",
        "        # ...typically used for logging and visualization during training. The logs will be saved in the directory specified by log_dir.\n",
        "\n",
        "        self.dataset = dataset # It assigns the dataset argument to an instance variable 'self.dataset'...\n",
        "        # ...This presumably represents the dataset used for training.\n",
        "        self.nt_xent_criterion = NTXentLoss(\n",
        "            self.device, config[\"batch_size\"], **config[\"loss\"]\n",
        "        ) #  It initializes an instance of NTXentLoss and assigns it to the self.nt_xent_criterion instance variable.\n",
        "        # This is likely a loss function used for training,\n",
        "        # it is configured based on the config dictionary's \"batch_size\" and \"loss\" settings.\n",
        "\n",
        "\n",
        "    # this method is responsible for dynamically selecting the appropriate computing device for training based on system capabilities...\n",
        "    # ...and the configuration settings.\n",
        "    # It sets the device to GPU if available and allowed, or to CPU if not.\n",
        "    def _get_device(self):\n",
        "        if torch.cuda.is_available() and self.config[\"gpu\"] != \"cpu\": # checks if CUDA (GPU support for PyTorch) is available on the system.\n",
        "            # checks if the \"gpu\" setting in the configuration (self.config) is not explicitly set to \"cpu\".\n",
        "            device = self.config[\"gpu\"] # assigns the GPU device specified in the configuration to the device variable.\n",
        "            torch.cuda.set_device(device) # sets the current CUDA device to the one specified in device.\n",
        "                                          # This ensures that the specified GPU is used for training\n",
        "        else:\n",
        "            device = \"cpu\" # assigns the string \"cpu\" to the device variable, indicating CPU usage.\n",
        "        print(\"Running on:\", device) # prints a message indicating whether the code is running on the CPU or GPU\n",
        "\n",
        "        return device # method returns the selected device (device) as a string (\"cpu\" or the GPU identifier).\n",
        "\n",
        "\n",
        "    # The _step method appears to define a single training step within a training loop for a contrastive learning task.\n",
        "    def _step(self, model, xis, xjs, n_iter):\n",
        "        # get the representations and the projections\n",
        "        ris, zis = model(xis)  # [N,C]\n",
        "        # This line passes the input data 'xis' through the neural network model. It computes two sets of values:\n",
        "        # ris: These are the intermediate representations (features) obtained from the model for the input data xis.\n",
        "        # zis: These are the projection vectors or embeddings corresponding to the representations ris.\n",
        "\n",
        "        # get the representations and the projections\n",
        "        rjs, zjs = model(xjs)  # [N,C]\n",
        "        # this line passes the input data xjs through the same model to obtain representations and projections for xjs.\n",
        "        #These are stored in rjs and zjs, respectively.\n",
        "\n",
        "        # normalize projection feature vectors\n",
        "        zis = F.normalize(zis, dim=1)\n",
        "        zjs = F.normalize(zjs, dim=1)\n",
        "        # These lines normalize the projection feature vectors zis and zjs along dimension 1 (usually the channel dimension for image data).\n",
        "        # Normalization is typically done to ensure that the embeddings have a consistent scale, which can be important for contrastive learning.\n",
        "\n",
        "        loss = self.nt_xent_criterion(zis, zjs)\n",
        "        # This line computes the loss between the normalized projection vectors zis and zjs.\n",
        "        # It uses the nt_xent_criterion, which is likely a custom loss function specifically designed for contrastive learning tasks.\n",
        "        # Contrastive loss functions aim to minimize the similarity (e.g., cosine similarity) between positive pairs (pairs of similar items)...\n",
        "        # ...and maximize the similarity between negative pairs (pairs of dissimilar items).\n",
        "        return loss\n",
        "        # Finally, the computed loss is returned as the result of this _step method.\n",
        "\n",
        "\n",
        "# This portion of the train method sets up the training process for the neural network model.\n",
        "    def train(self):\n",
        "        train_loader, valid_loader = self.dataset.get_data_loaders()\n",
        "        # This line obtains training and validation data loaders from the dataset object.\n",
        "        # It suggests that the dataset object has a method named 'get_data_loaders' that returns data loaders for training and validation data.\n",
        "\n",
        "        from models.agile_pretrain import AGILE # It imports the AGILE model from a module named 'agile_pretrain' within the 'models' package.\n",
        "        model = AGILE(**self.config[\"model\"]).to(self.device) # It instantiates the AGILE model using the model configuration specified in self.config[\"model\"].\n",
        "        # The ** syntax is used to unpack the dictionary and pass its contents as keyword arguments to the AGILE constructor.\n",
        "        # The resulting model is then moved to the specified device (self.device), which can be either CPU or GPU.\n",
        "        model = self._load_pre_trained_weights(model) #  It loads pre-trained weights for the model if they exist (as explained earlier in your code).\n",
        "        #If pre-trained weights are not found, it initializes the model from scratch.\n",
        "        print(model)\n",
        "\n",
        "        optimizer = torch.optim.Adam(\n",
        "            model.parameters(),\n",
        "            self.config[\"init_lr\"],\n",
        "            weight_decay=eval(self.config[\"weight_decay\"]),\n",
        "        )\n",
        "        scheduler = CosineAnnealingLR(\n",
        "            optimizer,\n",
        "            T_max=self.config[\"epochs\"] - self.config[\"warm_up\"],\n",
        "            eta_min=0,\n",
        "            last_epoch=-1,\n",
        "        )\n",
        "\n",
        "        if apex_support and self.config[\"fp16_precision\"]:\n",
        "            model, optimizer = amp.initialize(\n",
        "                model, optimizer, opt_level=\"O2\", keep_batchnorm_fp32=True\n",
        "            )\n",
        "\n",
        "        model_checkpoints_folder = os.path.join(self.writer.log_dir, \"checkpoints\")\n",
        "\n",
        "        # save config file\n",
        "        _save_config_file(model_checkpoints_folder)\n",
        "\n",
        "        n_iter = 0\n",
        "        valid_n_iter = 0\n",
        "        best_valid_loss = np.inf\n",
        "\n",
        "        for epoch_counter in range(self.config[\"epochs\"]):\n",
        "            for bn, (xis, xjs) in enumerate(train_loader):\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                xis = xis.to(self.device)\n",
        "                xjs = xjs.to(self.device)\n",
        "\n",
        "                loss = self._step(model, xis, xjs, n_iter)\n",
        "\n",
        "                if n_iter % self.config[\"log_every_n_steps\"] == 0:\n",
        "                    self.writer.add_scalar(\"train_loss\", loss, global_step=n_iter)\n",
        "                    self.writer.add_scalar(\n",
        "                        \"cosine_lr_decay\",\n",
        "                        scheduler.get_last_lr()[0],\n",
        "                        global_step=n_iter,\n",
        "                    )\n",
        "                    print(\"Epoch:\", epoch_counter, \"Iteration:\", bn, \"Train loss:\",loss.item())\n",
        "\n",
        "                if apex_support and self.config[\"fp16_precision\"]:\n",
        "                    with amp.scale_loss(loss, optimizer) as scaled_loss:\n",
        "                        scaled_loss.backward()\n",
        "                else:\n",
        "                    loss.backward()\n",
        "\n",
        "                optimizer.step()\n",
        "                n_iter += 1\n",
        "\n",
        "            # validate the model if requested\n",
        "            if epoch_counter % self.config[\"eval_every_n_epochs\"] == 0:\n",
        "                valid_loss = self._validate(model, valid_loader)\n",
        "                print(\"Epoch:\", epoch_counter, \"Iteration:\", bn, \"Valid loss:\", valid_loss)\n",
        "                if valid_loss < best_valid_loss:\n",
        "                    # save the model weights\n",
        "                    best_valid_loss = valid_loss\n",
        "                    torch.save(\n",
        "                        model.state_dict(),\n",
        "                        os.path.join(model_checkpoints_folder, \"model.pth\"),\n",
        "                    )\n",
        "\n",
        "                self.writer.add_scalar(\n",
        "                    \"validation_loss\", valid_loss, global_step=valid_n_iter\n",
        "                )\n",
        "                valid_n_iter += 1\n",
        "\n",
        "            if (epoch_counter + 1) % self.config[\"save_every_n_epochs\"] == 0:\n",
        "                torch.save(\n",
        "                    model.state_dict(),\n",
        "                    os.path.join(\n",
        "                        model_checkpoints_folder,\n",
        "                        \"model_{}.pth\".format(str(epoch_counter)),\n",
        "                    ),\n",
        "                )\n",
        "\n",
        "            # warmup for the first few epochs\n",
        "            if epoch_counter >= self.config[\"warm_up\"]:\n",
        "                scheduler.step()\n",
        "\n",
        "    def _load_pre_trained_weights(self, model):\n",
        "        try:\n",
        "            checkpoints_folder = os.path.join(\n",
        "                \"./ckpt\", self.config[\"load_model\"], \"checkpoints\"\n",
        "            )\n",
        "            state_dict = torch.load(\n",
        "                os.path.join(checkpoints_folder, \"model.pth\"),\n",
        "                map_location=self.device,\n",
        "            )\n",
        "            model.load_state_dict(state_dict)\n",
        "            print(\"Loaded pre-trained model with success.\")\n",
        "        except FileNotFoundError:\n",
        "            print(\"Pre-trained weights not found. Training from scratch.\")\n",
        "\n",
        "        return model\n",
        "\n",
        "    def _validate(self, model, valid_loader):\n",
        "        # validation steps\n",
        "        with torch.no_grad():\n",
        "            model.eval()\n",
        "\n",
        "            valid_loss = 0.0\n",
        "            counter = 0\n",
        "            for (xis, xjs) in valid_loader:\n",
        "                xis = xis.to(self.device)\n",
        "                xjs = xjs.to(self.device)\n",
        "\n",
        "                loss = self._step(model, xis, xjs, counter)\n",
        "                valid_loss += loss.item()\n",
        "                counter += 1\n",
        "            valid_loss /= counter\n",
        "\n",
        "        model.train()\n",
        "        return valid_loss\n",
        "\n"
      ],
      "metadata": {
        "id": "mBoxDWXdvCtV"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main(config):\n",
        "    if config[\"aug\"] == \"node\":\n",
        "        from dataset.dataset import MoleculeDatasetWrapper\n",
        "    elif config[\"aug\"] == \"subgraph\":\n",
        "        from dataset.dataset_subgraph import MoleculeDatasetWrapper\n",
        "    elif config[\"aug\"] == \"mix\":\n",
        "        from dataset.dataset_mix import MoleculeDatasetWrapper\n",
        "    else:\n",
        "        raise ValueError(\"Not defined molecule augmentation!\")\n",
        "\n",
        "    dataset = MoleculeDatasetWrapper(config[\"batch_size\"], **config[\"dataset\"])\n",
        "    agile_pretrain = PreTrain(dataset, config)\n",
        "    agile_pretrain.train()\n",
        "    print(f\"Training finished. Checkpoints saved in {agile_pretrain.writer.log_dir}.\")"
      ],
      "metadata": {
        "id": "cGIOSxyGvlUX"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "if __name__ == \"__main__\":\n",
        "    import argparse\n",
        "\n",
        "    parser = argparse.ArgumentParser()\n",
        "    print(parser)\n",
        "    parser.add_argument(\"config\", type=str, help=\"config_pretrain.yaml\")\n",
        "    args = parser.parse_args()\n",
        "    print(args)\n",
        "    config = yaml.load(open(args.config, \"r\"), Loader=yaml.FullLoader)\n",
        "    print(config)\n",
        "    main(config)"
      ],
      "metadata": {
        "id": "mwLajHHPBcUU",
        "outputId": "a406c768-adff-4199-d6dd-7fd304f4f889",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 246
        }
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-50-2553edcff979>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;32mimport\u001b[0m \u001b[0margparse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0margparse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mArgumentParser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig_pretrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparser\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_argument\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"config\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhelp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"config_pretrain.yaml\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'config_pretrain' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "qOLDeTitc19A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Further ahead from here, I will try to finetune the pre-trained MolCLR model with the file 'finetune.py'"
      ],
      "metadata": {
        "id": "P-NWi-i_c3U0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "import sys\n",
        "import torch\n",
        "import yaml\n",
        "import numpy as np\n",
        "from datetime import datetime"
      ],
      "metadata": {
        "id": "RS5bDNTnc9pH"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from torch.optim.lr_scheduler import CosineAnnealingLR"
      ],
      "metadata": {
        "id": "VMoGsaAEdKkQ"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from utils.nt_xent import NTXentLoss"
      ],
      "metadata": {
        "id": "Cw81GmAOdNfA"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "apex_support = False\n",
        "try:\n",
        "    sys.path.append(\"./apex\")\n",
        "    from apex import amp\n",
        "\n",
        "    apex_support = True\n",
        "except:\n",
        "    print(\n",
        "        \"Please install apex for mixed precision training from: https://github.com/NVIDIA/apex\"\n",
        "    )\n",
        "    apex_support = False"
      ],
      "metadata": {
        "id": "fo_sX9CidQor"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def _save_config_file(model_checkpoints_folder):\n",
        "    if not os.path.exists(model_checkpoints_folder):\n",
        "        os.makedirs(model_checkpoints_folder)\n",
        "        shutil.copy(args.config, os.path.join(model_checkpoints_folder, \"config.yaml\"))"
      ],
      "metadata": {
        "id": "bERThEj6dUTa"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PreTrain(object):\n",
        "    def __init__(self, dataset, config):\n",
        "        self.config = config\n",
        "        self.device = self._get_device()\n",
        "\n",
        "        dir_name = datetime.now().strftime(\"%b%d_%H-%M-%S\")\n",
        "        log_dir = os.path.join(\"ckpt\", dir_name)\n",
        "        self.writer = SummaryWriter(log_dir=log_dir)\n",
        "\n",
        "        self.dataset = dataset\n",
        "        self.nt_xent_criterion = NTXentLoss(\n",
        "            self.device, config[\"batch_size\"], **config[\"loss\"]\n",
        "        )\n",
        "\n",
        "    def _get_device(self):\n",
        "        if torch.cuda.is_available() and self.config[\"gpu\"] != \"cpu\":\n",
        "            device = self.config[\"gpu\"]\n",
        "            torch.cuda.set_device(device)\n",
        "        else:\n",
        "            device = \"cpu\"\n",
        "        print(\"Running on:\", device)\n",
        "\n",
        "        return device\n",
        "\n",
        "    def _step(self, model, xis, xjs, n_iter):\n",
        "        # get the representations and the projections\n",
        "        ris, zis = model(xis)  # [N,C]\n",
        "\n",
        "        # get the representations and the projections\n",
        "        rjs, zjs = model(xjs)  # [N,C]\n",
        "\n",
        "        # normalize projection feature vectors\n",
        "        zis = F.normalize(zis, dim=1)\n",
        "        zjs = F.normalize(zjs, dim=1)\n",
        "\n",
        "        loss = self.nt_xent_criterion(zis, zjs)\n",
        "        return loss\n",
        "\n",
        "    def train(self):\n",
        "        train_loader, valid_loader = self.dataset.get_data_loaders()\n",
        "\n",
        "        from models.agile_pretrain import AGILE\n",
        "        model = AGILE(**self.config[\"model\"]).to(self.device)\n",
        "        model = self._load_pre_trained_weights(model)\n",
        "        print(model)\n",
        "\n",
        "        optimizer = torch.optim.Adam(\n",
        "            model.parameters(),\n",
        "            self.config[\"init_lr\"],\n",
        "            weight_decay=eval(self.config[\"weight_decay\"]),\n",
        "        )\n",
        "        scheduler = CosineAnnealingLR(\n",
        "            optimizer,\n",
        "            T_max=self.config[\"epochs\"] - self.config[\"warm_up\"],\n",
        "            eta_min=0,\n",
        "            last_epoch=-1,\n",
        "        )\n",
        "\n",
        "        if apex_support and self.config[\"fp16_precision\"]:\n",
        "            model, optimizer = amp.initialize(\n",
        "                model, optimizer, opt_level=\"O2\", keep_batchnorm_fp32=True\n",
        "            )\n",
        "\n",
        "        model_checkpoints_folder = os.path.join(self.writer.log_dir, \"checkpoints\")\n",
        "\n",
        "        # save config file\n",
        "        _save_config_file(model_checkpoints_folder)\n",
        "\n",
        "        n_iter = 0\n",
        "        valid_n_iter = 0\n",
        "        best_valid_loss = np.inf\n",
        "\n",
        "        for epoch_counter in range(self.config[\"epochs\"]):\n",
        "            for bn, (xis, xjs) in enumerate(train_loader):\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                xis = xis.to(self.device)\n",
        "                xjs = xjs.to(self.device)\n",
        "\n",
        "                loss = self._step(model, xis, xjs, n_iter)\n",
        "\n",
        "                if n_iter % self.config[\"log_every_n_steps\"] == 0:\n",
        "                    self.writer.add_scalar(\"train_loss\", loss, global_step=n_iter)\n",
        "                    self.writer.add_scalar(\n",
        "                        \"cosine_lr_decay\",\n",
        "                        scheduler.get_last_lr()[0],\n",
        "                        global_step=n_iter,\n",
        "                    )\n",
        "                    print(\"Epoch:\", epoch_counter, \"Iteration:\", bn, \"Train loss:\",loss.item())\n",
        "\n",
        "                if apex_support and self.config[\"fp16_precision\"]:\n",
        "                    with amp.scale_loss(loss, optimizer) as scaled_loss:\n",
        "                        scaled_loss.backward()\n",
        "                else:\n",
        "                    loss.backward()\n",
        "\n",
        "                optimizer.step()\n",
        "                n_iter += 1\n",
        "\n",
        "            # validate the model if requested\n",
        "            if epoch_counter % self.config[\"eval_every_n_epochs\"] == 0:\n",
        "                valid_loss = self._validate(model, valid_loader)\n",
        "                print(\"Epoch:\", epoch_counter, \"Iteration:\", bn, \"Valid loss:\", valid_loss)\n",
        "                if valid_loss < best_valid_loss:\n",
        "                    # save the model weights\n",
        "                    best_valid_loss = valid_loss\n",
        "                    torch.save(\n",
        "                        model.state_dict(),\n",
        "                        os.path.join(model_checkpoints_folder, \"model.pth\"),\n",
        "                    )\n",
        "\n",
        "                self.writer.add_scalar(\n",
        "                    \"validation_loss\", valid_loss, global_step=valid_n_iter\n",
        "                )\n",
        "                valid_n_iter += 1\n",
        "\n",
        "            if (epoch_counter + 1) % self.config[\"save_every_n_epochs\"] == 0:\n",
        "                torch.save(\n",
        "                    model.state_dict(),\n",
        "                    os.path.join(\n",
        "                        model_checkpoints_folder,\n",
        "                        \"model_{}.pth\".format(str(epoch_counter)),\n",
        "                    ),\n",
        "                )\n",
        "\n",
        "            # warmup for the first few epochs\n",
        "            if epoch_counter >= self.config[\"warm_up\"]:\n",
        "                scheduler.step()\n",
        "\n",
        "    def _load_pre_trained_weights(self, model):\n",
        "        try:\n",
        "            checkpoints_folder = os.path.join(\n",
        "                \"./ckpt\", self.config[\"load_model\"], \"checkpoints\"\n",
        "            )\n",
        "            state_dict = torch.load(\n",
        "                os.path.join(checkpoints_folder, \"model.pth\"),\n",
        "                map_location=self.device,\n",
        "            )\n",
        "            model.load_state_dict(state_dict)\n",
        "            print(\"Loaded pre-trained model with success.\")\n",
        "        except FileNotFoundError:\n",
        "            print(\"Pre-trained weights not found. Training from scratch.\")\n",
        "\n",
        "        return model\n",
        "\n",
        "    def _validate(self, model, valid_loader):\n",
        "        # validation steps\n",
        "        with torch.no_grad():\n",
        "            model.eval()\n",
        "\n",
        "            valid_loss = 0.0\n",
        "            counter = 0\n",
        "            for (xis, xjs) in valid_loader:\n",
        "                xis = xis.to(self.device)\n",
        "                xjs = xjs.to(self.device)\n",
        "\n",
        "                loss = self._step(model, xis, xjs, counter)\n",
        "                valid_loss += loss.item()\n",
        "                counter += 1\n",
        "            valid_loss /= counter\n",
        "\n",
        "        model.train()\n",
        "        return valid_loss"
      ],
      "metadata": {
        "id": "wGpGPrtjdgz4"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main(config):\n",
        "    if config[\"aug\"] == \"node\":\n",
        "        from dataset.dataset import MoleculeDatasetWrapper\n",
        "    elif config[\"aug\"] == \"subgraph\":\n",
        "        from dataset.dataset_subgraph import MoleculeDatasetWrapper\n",
        "    elif config[\"aug\"] == \"mix\":\n",
        "        from dataset.dataset_mix import MoleculeDatasetWrapper\n",
        "    else:\n",
        "        raise ValueError(\"Not defined molecule augmentation!\")\n",
        "\n",
        "    dataset = MoleculeDatasetWrapper(config[\"batch_size\"], **config[\"dataset\"])\n",
        "    agile_pretrain = PreTrain(dataset, config)\n",
        "    agile_pretrain.train()\n",
        "    print(f\"Training finished. Checkpoints saved in {agile_pretrain.writer.log_dir}.\")"
      ],
      "metadata": {
        "id": "u_bT7Xjzdjvq"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    import argparse\n",
        "\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument(\"config\", type=str, help=\"config_finetune.yaml\")\n",
        "    args = parser.parse_args()\n",
        "    config = yaml.load(open(args.config, \"r\"), Loader=yaml.FullLoader)\n",
        "    print(config)\n",
        "    main(config)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 159
        },
        "id": "hKTQZls-do3I",
        "outputId": "d1df7b64-2f92-485d-cbb1-8ea24637efd3"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "usage: colab_kernel_launcher.py [-h] config\n",
            "colab_kernel_launcher.py: error: unrecognized arguments: -f\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "SystemExit",
          "evalue": "ignored",
          "traceback": [
            "An exception has occurred, use %tb to see the full traceback.\n",
            "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
          ]
        }
      ]
    }
  ]
}