{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled",
      "provenance": [],
      "mount_file_id": "https://github.com/rsaran-BioAI/AGILE/blob/main/AGILE_Trial_1.ipynb",
      "authorship_tag": "ABX9TyP4pu/55qQCfvhIepThK8N/"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Installation of requirements"
      ],
      "metadata": {
        "id": "azsK8X73UuoI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# install requirements\n",
        "\n",
        "!pip install torch==1.12.1+cu113 torchvision==0.13.1+cu113  --extra-index-url https://download.pytorch.org/whl/cu113\n"
      ],
      "metadata": {
        "id": "drb8TyfFFr_Q",
        "outputId": "017846b0-b4ae-4c61-8405-2362e382e894",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://download.pytorch.org/whl/cu113\n",
            "Collecting torch==1.12.1+cu113\n",
            "  Downloading https://download.pytorch.org/whl/cu113/torch-1.12.1%2Bcu113-cp310-cp310-linux_x86_64.whl (1837.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 GB\u001b[0m \u001b[31m576.0 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torchvision==0.13.1+cu113\n",
            "  Downloading https://download.pytorch.org/whl/cu113/torchvision-0.13.1%2Bcu113-cp310-cp310-linux_x86_64.whl (23.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.4/23.4 MB\u001b[0m \u001b[31m40.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch==1.12.1+cu113) (4.5.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision==0.13.1+cu113) (1.23.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchvision==0.13.1+cu113) (2.31.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision==0.13.1+cu113) (9.4.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision==0.13.1+cu113) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision==0.13.1+cu113) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision==0.13.1+cu113) (2.0.5)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision==0.13.1+cu113) (2023.7.22)\n",
            "Installing collected packages: torch, torchvision\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.0.1+cu118\n",
            "    Uninstalling torch-2.0.1+cu118:\n",
            "      Successfully uninstalled torch-2.0.1+cu118\n",
            "  Attempting uninstall: torchvision\n",
            "    Found existing installation: torchvision 0.15.2+cu118\n",
            "    Uninstalling torchvision-0.15.2+cu118:\n",
            "      Successfully uninstalled torchvision-0.15.2+cu118\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchaudio 2.0.2+cu118 requires torch==2.0.1, but you have torch 1.12.1+cu113 which is incompatible.\n",
            "torchdata 0.6.1 requires torch==2.0.1, but you have torch 1.12.1+cu113 which is incompatible.\n",
            "torchtext 0.15.2 requires torch==2.0.1, but you have torch 1.12.1+cu113 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed torch-1.12.1+cu113 torchvision-0.13.1+cu113\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# install requirements\n",
        "\n",
        "!pip install torch-geometric==2.2.0 torch-sparse==0.6.16 torch-scatter==2.1.0 -f https://data.pyg.org/whl/torch-1.12.0+cu113.html"
      ],
      "metadata": {
        "id": "rnZo7oadGgby",
        "outputId": "0185afcf-c99a-42dc-f5e2-c058520af387",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in links: https://data.pyg.org/whl/torch-1.12.0+cu113.html\n",
            "Collecting torch-geometric==2.2.0\n",
            "  Downloading torch_geometric-2.2.0.tar.gz (564 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m565.0/565.0 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting torch-sparse==0.6.16\n",
            "  Downloading https://data.pyg.org/whl/torch-1.12.0%2Bcu113/torch_sparse-0.6.16%2Bpt112cu113-cp310-cp310-linux_x86_64.whl (3.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.5/3.5 MB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torch-scatter==2.1.0\n",
            "  Downloading https://data.pyg.org/whl/torch-1.12.0%2Bcu113/torch_scatter-2.1.0%2Bpt112cu113-cp310-cp310-linux_x86_64.whl (8.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.9/8.9 MB\u001b[0m \u001b[31m26.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torch-geometric==2.2.0) (4.66.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torch-geometric==2.2.0) (1.23.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from torch-geometric==2.2.0) (1.11.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch-geometric==2.2.0) (3.1.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torch-geometric==2.2.0) (2.31.0)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.10/dist-packages (from torch-geometric==2.2.0) (3.1.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from torch-geometric==2.2.0) (1.2.2)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.10/dist-packages (from torch-geometric==2.2.0) (5.9.5)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch-geometric==2.2.0) (2.1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric==2.2.0) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric==2.2.0) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric==2.2.0) (2.0.5)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric==2.2.0) (2023.7.22)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->torch-geometric==2.2.0) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->torch-geometric==2.2.0) (3.2.0)\n",
            "Building wheels for collected packages: torch-geometric\n",
            "  Building wheel for torch-geometric (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for torch-geometric: filename=torch_geometric-2.2.0-py3-none-any.whl size=773276 sha256=69e6261da745a1011396d056f730a4c511f86fce10b197bcf1455e8139adda9e\n",
            "  Stored in directory: /root/.cache/pip/wheels/c8/e4/83/5e964867e23f8a61cb8c5d5b9477617b710e96e6ebf1844562\n",
            "Successfully built torch-geometric\n",
            "Installing collected packages: torch-scatter, torch-sparse, torch-geometric\n",
            "Successfully installed torch-geometric-2.2.0 torch-scatter-2.1.0+pt112cu113 torch-sparse-0.6.16+pt112cu113\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Connecting the drive with Colab\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "a9wAv5mAGskJ",
        "outputId": "eedf4d80-7868-41df-cf30-fdd2e539d844",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Making sure the working directory is the one\n",
        "\n",
        "%cd /content/drive/MyDrive/AGILE"
      ],
      "metadata": {
        "id": "hDnkmT7IG2dU",
        "outputId": "6b2ebc4d-fd84-4076-b818-1dfb49c8afbd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/AGILE\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# install requirements\n",
        "\n",
        "!pip install -r requirements.txt"
      ],
      "metadata": {
        "id": "u0SLdbBeIEKo",
        "outputId": "7d9d53a5-7534-43b1-b546-4ee9ec9459a8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting exmol==3.0.2 (from -r requirements.txt (line 1))\n",
            "  Downloading exmol-3.0.2-py3-none-any.whl (4.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.7/4.7 MB\u001b[0m \u001b[31m19.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting matplotlib==3.5.3 (from -r requirements.txt (line 2))\n",
            "  Downloading matplotlib-3.5.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.9/11.9 MB\u001b[0m \u001b[31m61.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting networkx==2.6.3 (from -r requirements.txt (line 3))\n",
            "  Downloading networkx-2.6.3-py3-none-any.whl (1.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m61.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting numpy==1.21.6 (from -r requirements.txt (line 4))\n",
            "  Downloading numpy-1.21.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (15.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.9/15.9 MB\u001b[0m \u001b[31m77.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pandas==1.3.5 (from -r requirements.txt (line 5))\n",
            "  Downloading pandas-1.3.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.5/11.5 MB\u001b[0m \u001b[31m90.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting PyYAML==6.0 (from -r requirements.txt (line 6))\n",
            "  Downloading PyYAML-6.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (682 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m682.2/682.2 kB\u001b[0m \u001b[31m44.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting rdkit==2023.3.1 (from -r requirements.txt (line 7))\n",
            "  Downloading rdkit-2023.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (29.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m29.7/29.7 MB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting scikit-learn==1.0.2 (from -r requirements.txt (line 8))\n",
            "  Downloading scikit_learn-1.0.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (26.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m26.5/26.5 MB\u001b[0m \u001b[31m55.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting scipy==1.7.3 (from -r requirements.txt (line 9))\n",
            "  Downloading scipy-1.7.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (39.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m39.9/39.9 MB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: seaborn==0.12.2 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 10)) (0.12.2)\n",
            "Collecting skunk==1.2.0 (from -r requirements.txt (line 11))\n",
            "  Downloading skunk-1.2.0-py3-none-any.whl (6.4 kB)\n",
            "Collecting typing_extensions==4.4.0 (from -r requirements.txt (line 12))\n",
            "  Downloading typing_extensions-4.4.0-py3-none-any.whl (26 kB)\n",
            "Collecting umap-learn==0.5.3 (from -r requirements.txt (line 13))\n",
            "  Downloading umap-learn-0.5.3.tar.gz (88 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m88.2/88.2 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: tensorboard==2.13.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 14)) (2.13.0)\n",
            "Collecting selfies>=2.0.0 (from exmol==3.0.2->-r requirements.txt (line 1))\n",
            "  Downloading selfies-2.1.1-py3-none-any.whl (35 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from exmol==3.0.2->-r requirements.txt (line 1)) (2.31.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from exmol==3.0.2->-r requirements.txt (line 1)) (4.66.1)\n",
            "Collecting ratelimit (from exmol==3.0.2->-r requirements.txt (line 1))\n",
            "  Downloading ratelimit-2.2.1.tar.gz (5.3 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.10/dist-packages (from exmol==3.0.2->-r requirements.txt (line 1)) (6.1.0)\n",
            "Collecting synspace (from exmol==3.0.2->-r requirements.txt (line 1))\n",
            "  Downloading synspace-0.3.0-py3-none-any.whl (2.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.6/2.6 MB\u001b[0m \u001b[31m42.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting langchain (from exmol==3.0.2->-r requirements.txt (line 1))\n",
            "  Downloading langchain-0.0.307-py3-none-any.whl (1.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m63.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib==3.5.3->-r requirements.txt (line 2)) (0.12.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib==3.5.3->-r requirements.txt (line 2)) (4.43.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib==3.5.3->-r requirements.txt (line 2)) (1.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib==3.5.3->-r requirements.txt (line 2)) (23.1)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib==3.5.3->-r requirements.txt (line 2)) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.2.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib==3.5.3->-r requirements.txt (line 2)) (3.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib==3.5.3->-r requirements.txt (line 2)) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.10/dist-packages (from pandas==1.3.5->-r requirements.txt (line 5)) (2023.3.post1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.10/dist-packages (from scikit-learn==1.0.2->-r requirements.txt (line 8)) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn==1.0.2->-r requirements.txt (line 8)) (3.2.0)\n",
            "Requirement already satisfied: numba>=0.49 in /usr/local/lib/python3.10/dist-packages (from umap-learn==0.5.3->-r requirements.txt (line 13)) (0.56.4)\n",
            "Collecting pynndescent>=0.5 (from umap-learn==0.5.3->-r requirements.txt (line 13))\n",
            "  Downloading pynndescent-0.5.10.tar.gz (1.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m57.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.10/dist-packages (from tensorboard==2.13.0->-r requirements.txt (line 14)) (1.4.0)\n",
            "Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.10/dist-packages (from tensorboard==2.13.0->-r requirements.txt (line 14)) (1.58.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard==2.13.0->-r requirements.txt (line 14)) (2.17.3)\n",
            "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard==2.13.0->-r requirements.txt (line 14)) (1.0.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard==2.13.0->-r requirements.txt (line 14)) (3.4.4)\n",
            "Requirement already satisfied: protobuf>=3.19.6 in /usr/local/lib/python3.10/dist-packages (from tensorboard==2.13.0->-r requirements.txt (line 14)) (3.20.3)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard==2.13.0->-r requirements.txt (line 14)) (67.7.2)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard==2.13.0->-r requirements.txt (line 14)) (0.7.1)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard==2.13.0->-r requirements.txt (line 14)) (2.3.7)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.10/dist-packages (from tensorboard==2.13.0->-r requirements.txt (line 14)) (0.41.2)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard==2.13.0->-r requirements.txt (line 14)) (5.3.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard==2.13.0->-r requirements.txt (line 14)) (0.3.0)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard==2.13.0->-r requirements.txt (line 14)) (1.16.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard==2.13.0->-r requirements.txt (line 14)) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard==2.13.0->-r requirements.txt (line 14)) (1.3.1)\n",
            "Requirement already satisfied: llvmlite<0.40,>=0.39.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba>=0.49->umap-learn==0.5.3->-r requirements.txt (line 13)) (0.39.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->exmol==3.0.2->-r requirements.txt (line 1)) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->exmol==3.0.2->-r requirements.txt (line 1)) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->exmol==3.0.2->-r requirements.txt (line 1)) (2.0.5)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->exmol==3.0.2->-r requirements.txt (line 1)) (2023.7.22)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard==2.13.0->-r requirements.txt (line 14)) (2.1.3)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain->exmol==3.0.2->-r requirements.txt (line 1)) (2.0.21)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain->exmol==3.0.2->-r requirements.txt (line 1)) (3.8.5)\n",
            "Requirement already satisfied: anyio<4.0 in /usr/local/lib/python3.10/dist-packages (from langchain->exmol==3.0.2->-r requirements.txt (line 1)) (3.7.1)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain->exmol==3.0.2->-r requirements.txt (line 1)) (4.0.3)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain->exmol==3.0.2->-r requirements.txt (line 1))\n",
            "  Downloading dataclasses_json-0.6.1-py3-none-any.whl (27 kB)\n",
            "Collecting jsonpatch<2.0,>=1.33 (from langchain->exmol==3.0.2->-r requirements.txt (line 1))\n",
            "  Downloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
            "Collecting langsmith<0.1.0,>=0.0.40 (from langchain->exmol==3.0.2->-r requirements.txt (line 1))\n",
            "  Downloading langsmith-0.0.41-py3-none-any.whl (39 kB)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain->exmol==3.0.2->-r requirements.txt (line 1)) (1.10.13)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain->exmol==3.0.2->-r requirements.txt (line 1)) (8.2.3)\n",
            "Collecting pystow (from synspace->exmol==3.0.2->-r requirements.txt (line 1))\n",
            "  Downloading pystow-0.5.0-py3-none-any.whl (37 kB)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain->exmol==3.0.2->-r requirements.txt (line 1)) (23.1.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain->exmol==3.0.2->-r requirements.txt (line 1)) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain->exmol==3.0.2->-r requirements.txt (line 1)) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain->exmol==3.0.2->-r requirements.txt (line 1)) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain->exmol==3.0.2->-r requirements.txt (line 1)) (1.3.1)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<4.0->langchain->exmol==3.0.2->-r requirements.txt (line 1)) (1.3.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<4.0->langchain->exmol==3.0.2->-r requirements.txt (line 1)) (1.1.3)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain->exmol==3.0.2->-r requirements.txt (line 1))\n",
            "  Downloading marshmallow-3.20.1-py3-none-any.whl (49 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain->exmol==3.0.2->-r requirements.txt (line 1))\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Collecting jsonpointer>=1.9 (from jsonpatch<2.0,>=1.33->langchain->exmol==3.0.2->-r requirements.txt (line 1))\n",
            "  Downloading jsonpointer-2.4-py2.py3-none-any.whl (7.8 kB)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard==2.13.0->-r requirements.txt (line 14)) (0.5.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard==2.13.0->-r requirements.txt (line 14)) (3.2.2)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain->exmol==3.0.2->-r requirements.txt (line 1)) (2.0.2)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from pystow->synspace->exmol==3.0.2->-r requirements.txt (line 1)) (8.1.7)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain->exmol==3.0.2->-r requirements.txt (line 1))\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Building wheels for collected packages: umap-learn, pynndescent, ratelimit\n",
            "  Building wheel for umap-learn (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for umap-learn: filename=umap_learn-0.5.3-py3-none-any.whl size=82807 sha256=a1102824063f71aa911441fc13fdaa526e90fd9755aa624692d110f805ceabe3\n",
            "  Stored in directory: /root/.cache/pip/wheels/a0/e8/c6/a37ea663620bd5200ea1ba0907ab3c217042c1d035ef606acc\n",
            "  Building wheel for pynndescent (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pynndescent: filename=pynndescent-0.5.10-py3-none-any.whl size=55615 sha256=1029f420635de3bc432284996b325a9ed10c323bbf945f88db858578882092bb\n",
            "  Stored in directory: /root/.cache/pip/wheels/4a/38/5d/f60a40a66a9512b7e5e83517ebc2d1b42d857be97d135f1096\n",
            "  Building wheel for ratelimit (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ratelimit: filename=ratelimit-2.2.1-py3-none-any.whl size=5894 sha256=dda6e7135bf1eb6d418e7869d525baa2beb9c1657a9c65f0d319b8fb9c7e9d2b\n",
            "  Stored in directory: /root/.cache/pip/wheels/27/5f/ba/e972a56dcbf5de9f2b7d2b2a710113970bd173c4dcd3d2c902\n",
            "Successfully built umap-learn pynndescent ratelimit\n",
            "Installing collected packages: ratelimit, typing_extensions, selfies, PyYAML, numpy, networkx, mypy-extensions, marshmallow, jsonpointer, typing-inspect, scipy, rdkit, pystow, pandas, matplotlib, jsonpatch, skunk, scikit-learn, langsmith, dataclasses-json, synspace, pynndescent, langchain, umap-learn, exmol\n",
            "  Attempting uninstall: typing_extensions\n",
            "    Found existing installation: typing_extensions 4.5.0\n",
            "    Uninstalling typing_extensions-4.5.0:\n",
            "      Successfully uninstalled typing_extensions-4.5.0\n",
            "  Attempting uninstall: PyYAML\n",
            "    Found existing installation: PyYAML 6.0.1\n",
            "    Uninstalling PyYAML-6.0.1:\n",
            "      Successfully uninstalled PyYAML-6.0.1\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.23.5\n",
            "    Uninstalling numpy-1.23.5:\n",
            "      Successfully uninstalled numpy-1.23.5\n",
            "  Attempting uninstall: networkx\n",
            "    Found existing installation: networkx 3.1\n",
            "    Uninstalling networkx-3.1:\n",
            "      Successfully uninstalled networkx-3.1\n",
            "  Attempting uninstall: scipy\n",
            "    Found existing installation: scipy 1.11.3\n",
            "    Uninstalling scipy-1.11.3:\n",
            "      Successfully uninstalled scipy-1.11.3\n",
            "  Attempting uninstall: pandas\n",
            "    Found existing installation: pandas 1.5.3\n",
            "    Uninstalling pandas-1.5.3:\n",
            "      Successfully uninstalled pandas-1.5.3\n",
            "  Attempting uninstall: matplotlib\n",
            "    Found existing installation: matplotlib 3.7.1\n",
            "    Uninstalling matplotlib-3.7.1:\n",
            "      Successfully uninstalled matplotlib-3.7.1\n",
            "  Attempting uninstall: scikit-learn\n",
            "    Found existing installation: scikit-learn 1.2.2\n",
            "    Uninstalling scikit-learn-1.2.2:\n",
            "      Successfully uninstalled scikit-learn-1.2.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "arviz 0.15.1 requires scipy>=1.8.0, but you have scipy 1.7.3 which is incompatible.\n",
            "google-colab 1.0.0 requires pandas==1.5.3, but you have pandas 1.3.5 which is incompatible.\n",
            "jax 0.4.14 requires numpy>=1.22, but you have numpy 1.21.6 which is incompatible.\n",
            "jaxlib 0.4.14+cuda11.cudnn86 requires numpy>=1.22, but you have numpy 1.21.6 which is incompatible.\n",
            "plotnine 0.12.3 requires matplotlib>=3.6.0, but you have matplotlib 3.5.3 which is incompatible.\n",
            "plotnine 0.12.3 requires numpy>=1.23.0, but you have numpy 1.21.6 which is incompatible.\n",
            "plotnine 0.12.3 requires pandas>=1.5.0, but you have pandas 1.3.5 which is incompatible.\n",
            "tensorflow 2.13.0 requires numpy<=1.24.3,>=1.22, but you have numpy 1.21.6 which is incompatible.\n",
            "torchaudio 2.0.2+cu118 requires torch==2.0.1, but you have torch 1.12.1+cu113 which is incompatible.\n",
            "torchdata 0.6.1 requires torch==2.0.1, but you have torch 1.12.1+cu113 which is incompatible.\n",
            "torchtext 0.15.2 requires torch==2.0.1, but you have torch 1.12.1+cu113 which is incompatible.\n",
            "xarray 2023.7.0 requires pandas>=1.4, but you have pandas 1.3.5 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed PyYAML-6.0 dataclasses-json-0.6.1 exmol-3.0.2 jsonpatch-1.33 jsonpointer-2.4 langchain-0.0.307 langsmith-0.0.41 marshmallow-3.20.1 matplotlib-3.5.3 mypy-extensions-1.0.0 networkx-2.6.3 numpy-1.21.6 pandas-1.3.5 pynndescent-0.5.10 pystow-0.5.0 ratelimit-2.2.1 rdkit-2023.3.1 scikit-learn-1.0.2 scipy-1.7.3 selfies-2.1.1 skunk-1.2.0 synspace-0.3.0 typing-inspect-0.9.0 typing_extensions-4.4.0 umap-learn-0.5.3\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "matplotlib",
                  "mpl_toolkits",
                  "numpy"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# The following is the content of the file 'config_pretrain.yaml'\n",
        "\n",
        "batch_size: 512 # batch size\n",
        "warm_up: 10 # warm-up epochs\n",
        "epochs: 2 # total number of epochs\n",
        "\n",
        "load_model: pretrained_gin # resume training\n",
        "eval_every_n_epochs: 1 # validation frequency\n",
        "save_every_n_epochs: 5 # automatic model saving frequecy\n",
        "log_every_n_steps: 50 # print training log frequency\n",
        "\n",
        "fp16_precision: False # float precision 16 (i.e. True/False)\n",
        "init_lr: 0.0005 # initial learning rate for Adam\n",
        "weight_decay: 1e-5 # weight decay for Adam\n",
        "gpu: cuda:0 # training GPU\n",
        "\n",
        "model:\n",
        "  num_layer: 5 # number of graph conv layers\n",
        "  emb_dim: 300 # embedding dimension in graph conv layers\n",
        "  feat_dim: 512 # output feature dimention\n",
        "  drop_ratio: 0 # dropout ratio\n",
        "  pool: mean # readout pooling (i.e., mean/max/add)\n",
        "\n",
        "aug: node # molecule graph augmentation strategy (i.e., node/subgraph/mix)\n",
        "dataset:\n",
        "  num_workers: 12 # dataloader number of workers\n",
        "  valid_size: 0.05 # ratio of validation data\n",
        "  data_path: data/{yourowndata}.csv # path of pre-training data\n",
        "\n",
        "loss:\n",
        "  temperature: 0.1 # temperature of NT-Xent loss\n",
        "  use_cosine_similarity: True # whether to use cosine similarity in NT-Xent loss (i.e. True/False)\n"
      ],
      "metadata": {
        "id": "6w6EeesTQimc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Further ahead from here, I will try to pre-train the MolCLR model with the file 'pretrain.py'"
      ],
      "metadata": {
        "id": "0QU4PQmcTdB1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "import sys\n",
        "import torch\n",
        "import yaml\n",
        "import numpy as np\n",
        "from datetime import datetime"
      ],
      "metadata": {
        "id": "E0MpRX0uNXq5"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# imports the torch.nn.functional module\n",
        "# The torch.nn.functional module contains various functions that are commonly used in neural network operations, such as activation functions, loss functions, and other operations commonly applied to tensors.\n",
        "\n",
        "import torch.nn.functional as F"
      ],
      "metadata": {
        "id": "7Xvqiq7jXgV6"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This line imports the SummaryWriter class from the torch.utils.tensorboard module.\n",
        "# SummaryWriter is a PyTorch utility that enables you to write TensorBoard-compatible logs.\n",
        "# TensorBoard is a visualization tool provided with TensorFlow, but PyTorch provides integration to use it with PyTorch models as well.\n",
        "\n",
        "from torch.utils.tensorboard import SummaryWriter"
      ],
      "metadata": {
        "id": "4bVUpCKKX77L"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# imports the CosineAnnealingLR class from the torch.optim.lr_scheduler module in PyTorch.\n",
        "# The learning rate is a hyperparameter that controls how much we are adjusting the weights of our network during training.\n",
        "# CosineAnnealingLR is a learning rate scheduler in PyTorch that reduces the learning rate following the cosine annealing schedule.\n",
        "# The learning rate starts at the initial value and is decreased following a cosine function until it reaches a minimum value and then starts increasing again.\n",
        "# This kind of schedule is often used to improve the convergence and generalization of neural networks during training.\n",
        "\n",
        "from torch.optim.lr_scheduler import CosineAnnealingLR"
      ],
      "metadata": {
        "id": "mRXCXaxpoxZ6"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Changing the working directory to 'utils'\n",
        "\n",
        "%cd /content/drive/MyDrive/AGILE"
      ],
      "metadata": {
        "id": "sfuEgK1r7rqa",
        "outputId": "72df8fb2-432a-4c94-cec1-fbf80e8bc96e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/AGILE\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#  imports the NTXentLoss class from a custom module named nt_xent inside a package or directory called utils.\n",
        "# This class likely contains the implementation of the NT-Xent loss function, which is commonly used in contrastive learning tasks.\n",
        "\n",
        "from utils.nt_xent import NTXentLoss"
      ],
      "metadata": {
        "id": "kDmsoDdg1muZ"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This code snippet checks if the Apex library is installed and imports it for mixed-precision training if it is available.\n",
        "# If Apex is not installed, it prints a message indicating that Apex needs to be installed from a specific GitHub repository.\n",
        "\n",
        "apex_support = False # Initializes the apex_support variable to False.\n",
        "try: #The code inside the try block attempts to import the amp module from the Apex library.\n",
        "    sys.path.append(\"./apex\") #  Adds the \"./apex\" directory to the Python system path, allowing Python to find the Apex module in that directory.\n",
        "    from apex import amp # Tries to import the amp module from the Apex library.\n",
        "\n",
        "    apex_support = True # If the import is successful, sets apex_support to True indicating that Apex is available.\n",
        "except: # If there is an ImportError (i.e., Apex is not installed), the code inside the except block is executed.\n",
        "    print(\n",
        "        \"Please install apex for mixed precision training from: https://github.com/NVIDIA/apex\" # Prints a message indicating that Apex needs to be installed and provides the GitHub repository URL for installation.\n",
        "    )\n",
        "    apex_support = False"
      ],
      "metadata": {
        "id": "YxwY5g4G8PMs"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Installing Nvidia apex\n",
        "\n",
        "import os, sys, shutil\n",
        "import time\n",
        "import gc\n",
        "from contextlib import contextmanager\n",
        "from pathlib import Path\n",
        "import random\n",
        "import numpy as np, pandas as pd\n",
        "from tqdm import tqdm, tqdm_notebook\n",
        "\n",
        "@contextmanager\n",
        "def timer(name):\n",
        "    t0 = time.time()\n",
        "    yield\n",
        "    print(f'[{name}] done in {time.time() - t0:.0f} s')\n",
        "\n",
        "USE_APEX = True\n",
        "\n",
        "if USE_APEX:\n",
        "            with timer('install Nvidia apex'):\n",
        "                # Installing Nvidia Apex\n",
        "                os.system('git clone https://github.com/NVIDIA/apex; cd apex; pip install -v --no-cache-dir' +\n",
        "                          ' --global-option=\"--cpp_ext\" --global-option=\"--cuda_ext\" ./')\n",
        "                os.system('rm -rf apex/.git') # too many files, Kaggle fails\n"
      ],
      "metadata": {
        "id": "0RBiIXQF_gUX",
        "outputId": "716bc714-64fb-4f4d-85ad-85f47fb2d106",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[install Nvidia apex] done in 1 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from apex import amp\n"
      ],
      "metadata": {
        "id": "hMtgroCiCiFh"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def _save_config_file(model_checkpoints_folder):\n",
        "    if not os.path.exists(model_checkpoints_folder):\n",
        "        os.makedirs(model_checkpoints_folder)\n",
        "        shutil.copy(args.config, os.path.join(model_checkpoints_folder, \"config.yaml\"))\n",
        "\n",
        "\n",
        "class PreTrain(object):\n",
        "    def __init__(self, dataset, config):\n",
        "        self.config = config\n",
        "        self.device = self._get_device()\n",
        "\n",
        "        dir_name = datetime.now().strftime(\"%b%d_%H-%M-%S\")\n",
        "        log_dir = os.path.join(\"ckpt\", dir_name)\n",
        "        self.writer = SummaryWriter(log_dir=log_dir)\n",
        "\n",
        "        self.dataset = dataset\n",
        "        self.nt_xent_criterion = NTXentLoss(\n",
        "            self.device, config[\"batch_size\"], **config[\"loss\"]\n",
        "        )\n",
        "\n",
        "    def _get_device(self):\n",
        "        if torch.cuda.is_available() and self.config[\"gpu\"] != \"cpu\":\n",
        "            device = self.config[\"gpu\"]\n",
        "            torch.cuda.set_device(device)\n",
        "        else:\n",
        "            device = \"cpu\"\n",
        "        print(\"Running on:\", device)\n",
        "\n",
        "        return device\n",
        "\n",
        "    def _step(self, model, xis, xjs, n_iter):\n",
        "        # get the representations and the projections\n",
        "        ris, zis = model(xis)  # [N,C]\n",
        "\n",
        "        # get the representations and the projections\n",
        "        rjs, zjs = model(xjs)  # [N,C]\n",
        "\n",
        "        # normalize projection feature vectors\n",
        "        zis = F.normalize(zis, dim=1)\n",
        "        zjs = F.normalize(zjs, dim=1)\n",
        "\n",
        "        loss = self.nt_xent_criterion(zis, zjs)\n",
        "        return loss\n",
        "\n",
        "    def train(self):\n",
        "        train_loader, valid_loader = self.dataset.get_data_loaders()\n",
        "\n",
        "        from models.agile_pretrain import AGILE\n",
        "        model = AGILE(**self.config[\"model\"]).to(self.device)\n",
        "        model = self._load_pre_trained_weights(model)\n",
        "        print(model)\n",
        "\n",
        "        optimizer = torch.optim.Adam(\n",
        "            model.parameters(),\n",
        "            self.config[\"init_lr\"],\n",
        "            weight_decay=eval(self.config[\"weight_decay\"]),\n",
        "        )\n",
        "        scheduler = CosineAnnealingLR(\n",
        "            optimizer,\n",
        "            T_max=self.config[\"epochs\"] - self.config[\"warm_up\"],\n",
        "            eta_min=0,\n",
        "            last_epoch=-1,\n",
        "        )\n",
        "\n",
        "        if apex_support and self.config[\"fp16_precision\"]:\n",
        "            model, optimizer = amp.initialize(\n",
        "                model, optimizer, opt_level=\"O2\", keep_batchnorm_fp32=True\n",
        "            )\n",
        "\n",
        "        model_checkpoints_folder = os.path.join(self.writer.log_dir, \"checkpoints\")\n",
        "\n",
        "        # save config file\n",
        "        _save_config_file(model_checkpoints_folder)\n",
        "\n",
        "        n_iter = 0\n",
        "        valid_n_iter = 0\n",
        "        best_valid_loss = np.inf\n",
        "\n",
        "        for epoch_counter in range(self.config[\"epochs\"]):\n",
        "            for bn, (xis, xjs) in enumerate(train_loader):\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                xis = xis.to(self.device)\n",
        "                xjs = xjs.to(self.device)\n",
        "\n",
        "                loss = self._step(model, xis, xjs, n_iter)\n",
        "\n",
        "                if n_iter % self.config[\"log_every_n_steps\"] == 0:\n",
        "                    self.writer.add_scalar(\"train_loss\", loss, global_step=n_iter)\n",
        "                    self.writer.add_scalar(\n",
        "                        \"cosine_lr_decay\",\n",
        "                        scheduler.get_last_lr()[0],\n",
        "                        global_step=n_iter,\n",
        "                    )\n",
        "                    print(\"Epoch:\", epoch_counter, \"Iteration:\", bn, \"Train loss:\",loss.item())\n",
        "\n",
        "                if apex_support and self.config[\"fp16_precision\"]:\n",
        "                    with amp.scale_loss(loss, optimizer) as scaled_loss:\n",
        "                        scaled_loss.backward()\n",
        "                else:\n",
        "                    loss.backward()\n",
        "\n",
        "                optimizer.step()\n",
        "                n_iter += 1\n",
        "\n",
        "            # validate the model if requested\n",
        "            if epoch_counter % self.config[\"eval_every_n_epochs\"] == 0:\n",
        "                valid_loss = self._validate(model, valid_loader)\n",
        "                print(\"Epoch:\", epoch_counter, \"Iteration:\", bn, \"Valid loss:\", valid_loss)\n",
        "                if valid_loss < best_valid_loss:\n",
        "                    # save the model weights\n",
        "                    best_valid_loss = valid_loss\n",
        "                    torch.save(\n",
        "                        model.state_dict(),\n",
        "                        os.path.join(model_checkpoints_folder, \"model.pth\"),\n",
        "                    )\n",
        "\n",
        "                self.writer.add_scalar(\n",
        "                    \"validation_loss\", valid_loss, global_step=valid_n_iter\n",
        "                )\n",
        "                valid_n_iter += 1\n",
        "\n",
        "            if (epoch_counter + 1) % self.config[\"save_every_n_epochs\"] == 0:\n",
        "                torch.save(\n",
        "                    model.state_dict(),\n",
        "                    os.path.join(\n",
        "                        model_checkpoints_folder,\n",
        "                        \"model_{}.pth\".format(str(epoch_counter)),\n",
        "                    ),\n",
        "                )\n",
        "\n",
        "            # warmup for the first few epochs\n",
        "            if epoch_counter >= self.config[\"warm_up\"]:\n",
        "                scheduler.step()\n",
        "\n",
        "    def _load_pre_trained_weights(self, model):\n",
        "        try:\n",
        "            checkpoints_folder = os.path.join(\n",
        "                \"./ckpt\", self.config[\"load_model\"], \"checkpoints\"\n",
        "            )\n",
        "            state_dict = torch.load(\n",
        "                os.path.join(checkpoints_folder, \"model.pth\"),\n",
        "                map_location=self.device,\n",
        "            )\n",
        "            model.load_state_dict(state_dict)\n",
        "            print(\"Loaded pre-trained model with success.\")\n",
        "        except FileNotFoundError:\n",
        "            print(\"Pre-trained weights not found. Training from scratch.\")\n",
        "\n",
        "        return model\n",
        "\n",
        "    def _validate(self, model, valid_loader):\n",
        "        # validation steps\n",
        "        with torch.no_grad():\n",
        "            model.eval()\n",
        "\n",
        "            valid_loss = 0.0\n",
        "            counter = 0\n",
        "            for (xis, xjs) in valid_loader:\n",
        "                xis = xis.to(self.device)\n",
        "                xjs = xjs.to(self.device)\n",
        "\n",
        "                loss = self._step(model, xis, xjs, counter)\n",
        "                valid_loss += loss.item()\n",
        "                counter += 1\n",
        "            valid_loss /= counter\n",
        "\n",
        "        model.train()\n",
        "        return valid_loss\n",
        "\n"
      ],
      "metadata": {
        "id": "mBoxDWXdvCtV"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main(config):\n",
        "    if config[\"aug\"] == \"node\":\n",
        "        from dataset.dataset import MoleculeDatasetWrapper\n",
        "    elif config[\"aug\"] == \"subgraph\":\n",
        "        from dataset.dataset_subgraph import MoleculeDatasetWrapper\n",
        "    elif config[\"aug\"] == \"mix\":\n",
        "        from dataset.dataset_mix import MoleculeDatasetWrapper\n",
        "    else:\n",
        "        raise ValueError(\"Not defined molecule augmentation!\")\n",
        "\n",
        "    dataset = MoleculeDatasetWrapper(config[\"batch_size\"], **config[\"dataset\"])\n",
        "    agile_pretrain = PreTrain(dataset, config)\n",
        "    agile_pretrain.train()\n",
        "    print(f\"Training finished. Checkpoints saved in {agile_pretrain.writer.log_dir}.\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    import argparse\n",
        "\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument(\"config\", type=str, help=\"Path to the config file.\")\n",
        "    args = parser.parse_args()\n",
        "    config = yaml.load(open(args.config, \"r\"), Loader=yaml.FullLoader)\n",
        "    print(config)\n",
        "    main(config)"
      ],
      "metadata": {
        "id": "cGIOSxyGvlUX",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 214
        },
        "outputId": "442aa967-821c-4a67-c5f5-06ea08eafafd"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "usage: colab_kernel_launcher.py [-h] config\n",
            "colab_kernel_launcher.py: error: unrecognized arguments: -f\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "SystemExit",
          "evalue": "ignored",
          "traceback": [
            "An exception has occurred, use %tb to see the full traceback.\n",
            "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py:3561: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
            "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
          ]
        }
      ]
    }
  ]
}