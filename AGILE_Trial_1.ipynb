{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled",
      "provenance": [],
      "mount_file_id": "https://github.com/rsaran-BioAI/AGILE/blob/main/AGILE_Trial_1.ipynb",
      "authorship_tag": "ABX9TyPwPox5DwYn19NjtoUd2B9g"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Installation of requirements"
      ],
      "metadata": {
        "id": "azsK8X73UuoI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# install requirements\n",
        "\n",
        "!pip install torch==1.12.1+cu113 torchvision==0.13.1+cu113  --extra-index-url https://download.pytorch.org/whl/cu113\n"
      ],
      "metadata": {
        "id": "drb8TyfFFr_Q",
        "outputId": "c5b93b75-74cf-47f4-a11d-b63c3a0ef8ac",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 618
        }
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://download.pytorch.org/whl/cu113\n",
            "Collecting torch==1.12.1+cu113\n",
            "  Downloading https://download.pytorch.org/whl/cu113/torch-1.12.1%2Bcu113-cp310-cp310-linux_x86_64.whl (1837.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 GB\u001b[0m \u001b[31m657.1 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torchvision==0.13.1+cu113\n",
            "  Downloading https://download.pytorch.org/whl/cu113/torchvision-0.13.1%2Bcu113-cp310-cp310-linux_x86_64.whl (23.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.4/23.4 MB\u001b[0m \u001b[31m93.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch==1.12.1+cu113) (4.5.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision==0.13.1+cu113) (1.23.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchvision==0.13.1+cu113) (2.31.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision==0.13.1+cu113) (9.4.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision==0.13.1+cu113) (3.3.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision==0.13.1+cu113) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision==0.13.1+cu113) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision==0.13.1+cu113) (2023.7.22)\n",
            "Installing collected packages: torch, torchvision\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.1.0+cu118\n",
            "    Uninstalling torch-2.1.0+cu118:\n",
            "      Successfully uninstalled torch-2.1.0+cu118\n",
            "  Attempting uninstall: torchvision\n",
            "    Found existing installation: torchvision 0.16.0+cu118\n",
            "    Uninstalling torchvision-0.16.0+cu118:\n",
            "      Successfully uninstalled torchvision-0.16.0+cu118\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchaudio 2.1.0+cu118 requires torch==2.1.0, but you have torch 1.12.1+cu113 which is incompatible.\n",
            "torchdata 0.7.0 requires torch==2.1.0, but you have torch 1.12.1+cu113 which is incompatible.\n",
            "torchtext 0.16.0 requires torch==2.1.0, but you have torch 1.12.1+cu113 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed torch-1.12.1+cu113 torchvision-0.13.1+cu113\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "torch",
                  "torchgen",
                  "torchvision"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# install requirements\n",
        "\n",
        "!pip install torch-geometric==2.2.0 torch-sparse==0.6.16 torch-scatter==2.1.0 -f https://data.pyg.org/whl/torch-1.12.0+cu113.html"
      ],
      "metadata": {
        "id": "rnZo7oadGgby",
        "outputId": "8a2e194a-c220-4c68-dc52-255b85765831",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in links: https://data.pyg.org/whl/torch-1.12.0+cu113.html\n",
            "Collecting torch-geometric==2.2.0\n",
            "  Downloading torch_geometric-2.2.0.tar.gz (564 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m565.0/565.0 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting torch-sparse==0.6.16\n",
            "  Downloading https://data.pyg.org/whl/torch-1.12.0%2Bcu113/torch_sparse-0.6.16%2Bpt112cu113-cp310-cp310-linux_x86_64.whl (3.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.5/3.5 MB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torch-scatter==2.1.0\n",
            "  Downloading https://data.pyg.org/whl/torch-1.12.0%2Bcu113/torch_scatter-2.1.0%2Bpt112cu113-cp310-cp310-linux_x86_64.whl (8.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.9/8.9 MB\u001b[0m \u001b[31m16.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torch-geometric==2.2.0) (4.66.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torch-geometric==2.2.0) (1.21.6)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from torch-geometric==2.2.0) (1.7.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch-geometric==2.2.0) (3.1.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torch-geometric==2.2.0) (2.31.0)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.10/dist-packages (from torch-geometric==2.2.0) (3.1.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from torch-geometric==2.2.0) (1.0.2)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.10/dist-packages (from torch-geometric==2.2.0) (5.9.5)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch-geometric==2.2.0) (2.1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric==2.2.0) (3.3.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric==2.2.0) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric==2.2.0) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric==2.2.0) (2023.7.22)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->torch-geometric==2.2.0) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->torch-geometric==2.2.0) (3.2.0)\n",
            "Building wheels for collected packages: torch-geometric\n",
            "  Building wheel for torch-geometric (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for torch-geometric: filename=torch_geometric-2.2.0-py3-none-any.whl size=773276 sha256=445ae2325673e0f15241074b92020434c8926631b3327b02f60ffb1f39d3d01f\n",
            "  Stored in directory: /root/.cache/pip/wheels/c8/e4/83/5e964867e23f8a61cb8c5d5b9477617b710e96e6ebf1844562\n",
            "Successfully built torch-geometric\n",
            "Installing collected packages: torch-scatter, torch-sparse, torch-geometric\n",
            "Successfully installed torch-geometric-2.2.0 torch-scatter-2.1.0+pt112cu113 torch-sparse-0.6.16+pt112cu113\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Connecting the drive with Colab\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "a9wAv5mAGskJ",
        "outputId": "2dcbe3aa-16b1-417d-defe-92bedd1cb201",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Making sure the working directory is the one\n",
        "\n",
        "%cd /content/drive/MyDrive/AGILE"
      ],
      "metadata": {
        "id": "hDnkmT7IG2dU",
        "outputId": "d8e1aa54-f12d-489a-8290-00388a12a2f5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/AGILE\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# install requirements\n",
        "\n",
        "!pip install -r requirements.txt"
      ],
      "metadata": {
        "id": "u0SLdbBeIEKo",
        "outputId": "40d9d895-c49f-43d7-a320-752ccfa7441f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting exmol==3.0.2 (from -r requirements.txt (line 1))\n",
            "  Downloading exmol-3.0.2-py3-none-any.whl (4.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.7/4.7 MB\u001b[0m \u001b[31m38.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting matplotlib==3.5.3 (from -r requirements.txt (line 2))\n",
            "  Downloading matplotlib-3.5.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.9/11.9 MB\u001b[0m \u001b[31m74.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting networkx==2.6.3 (from -r requirements.txt (line 3))\n",
            "  Downloading networkx-2.6.3-py3-none-any.whl (1.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m59.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting numpy==1.21.6 (from -r requirements.txt (line 4))\n",
            "  Downloading numpy-1.21.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (15.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.9/15.9 MB\u001b[0m \u001b[31m73.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pandas==1.3.5 (from -r requirements.txt (line 5))\n",
            "  Downloading pandas-1.3.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.5/11.5 MB\u001b[0m \u001b[31m48.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting PyYAML==6.0 (from -r requirements.txt (line 6))\n",
            "  Downloading PyYAML-6.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (682 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m682.2/682.2 kB\u001b[0m \u001b[31m45.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting rdkit==2023.3.1 (from -r requirements.txt (line 7))\n",
            "  Downloading rdkit-2023.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (29.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m29.7/29.7 MB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting scikit-learn==1.0.2 (from -r requirements.txt (line 8))\n",
            "  Downloading scikit_learn-1.0.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (26.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m26.5/26.5 MB\u001b[0m \u001b[31m59.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting scipy==1.7.3 (from -r requirements.txt (line 9))\n",
            "  Downloading scipy-1.7.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (39.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m39.9/39.9 MB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: seaborn==0.12.2 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 10)) (0.12.2)\n",
            "Collecting skunk==1.2.0 (from -r requirements.txt (line 11))\n",
            "  Downloading skunk-1.2.0-py3-none-any.whl (6.4 kB)\n",
            "Collecting typing_extensions==4.4.0 (from -r requirements.txt (line 12))\n",
            "  Downloading typing_extensions-4.4.0-py3-none-any.whl (26 kB)\n",
            "Collecting umap-learn==0.5.3 (from -r requirements.txt (line 13))\n",
            "  Downloading umap-learn-0.5.3.tar.gz (88 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m88.2/88.2 kB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting tensorboard==2.13.0 (from -r requirements.txt (line 14))\n",
            "  Downloading tensorboard-2.13.0-py3-none-any.whl (5.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m104.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting selfies>=2.0.0 (from exmol==3.0.2->-r requirements.txt (line 1))\n",
            "  Downloading selfies-2.1.1-py3-none-any.whl (35 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from exmol==3.0.2->-r requirements.txt (line 1)) (2.31.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from exmol==3.0.2->-r requirements.txt (line 1)) (4.66.1)\n",
            "Collecting ratelimit (from exmol==3.0.2->-r requirements.txt (line 1))\n",
            "  Downloading ratelimit-2.2.1.tar.gz (5.3 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.10/dist-packages (from exmol==3.0.2->-r requirements.txt (line 1)) (6.1.0)\n",
            "Collecting synspace (from exmol==3.0.2->-r requirements.txt (line 1))\n",
            "  Downloading synspace-0.3.0-py3-none-any.whl (2.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.6/2.6 MB\u001b[0m \u001b[31m92.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting langchain (from exmol==3.0.2->-r requirements.txt (line 1))\n",
            "  Downloading langchain-0.0.325-py3-none-any.whl (1.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m82.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib==3.5.3->-r requirements.txt (line 2)) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib==3.5.3->-r requirements.txt (line 2)) (4.43.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib==3.5.3->-r requirements.txt (line 2)) (1.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib==3.5.3->-r requirements.txt (line 2)) (23.2)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib==3.5.3->-r requirements.txt (line 2)) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.2.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib==3.5.3->-r requirements.txt (line 2)) (3.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib==3.5.3->-r requirements.txt (line 2)) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.10/dist-packages (from pandas==1.3.5->-r requirements.txt (line 5)) (2023.3.post1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.10/dist-packages (from scikit-learn==1.0.2->-r requirements.txt (line 8)) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn==1.0.2->-r requirements.txt (line 8)) (3.2.0)\n",
            "Requirement already satisfied: numba>=0.49 in /usr/local/lib/python3.10/dist-packages (from umap-learn==0.5.3->-r requirements.txt (line 13)) (0.56.4)\n",
            "Collecting pynndescent>=0.5 (from umap-learn==0.5.3->-r requirements.txt (line 13))\n",
            "  Downloading pynndescent-0.5.10.tar.gz (1.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m72.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.10/dist-packages (from tensorboard==2.13.0->-r requirements.txt (line 14)) (1.4.0)\n",
            "Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.10/dist-packages (from tensorboard==2.13.0->-r requirements.txt (line 14)) (1.59.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard==2.13.0->-r requirements.txt (line 14)) (2.17.3)\n",
            "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard==2.13.0->-r requirements.txt (line 14)) (1.0.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard==2.13.0->-r requirements.txt (line 14)) (3.5)\n",
            "Requirement already satisfied: protobuf>=3.19.6 in /usr/local/lib/python3.10/dist-packages (from tensorboard==2.13.0->-r requirements.txt (line 14)) (3.20.3)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard==2.13.0->-r requirements.txt (line 14)) (67.7.2)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard==2.13.0->-r requirements.txt (line 14)) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard==2.13.0->-r requirements.txt (line 14)) (3.0.1)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.10/dist-packages (from tensorboard==2.13.0->-r requirements.txt (line 14)) (0.41.2)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard==2.13.0->-r requirements.txt (line 14)) (5.3.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard==2.13.0->-r requirements.txt (line 14)) (0.3.0)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard==2.13.0->-r requirements.txt (line 14)) (1.16.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard==2.13.0->-r requirements.txt (line 14)) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard==2.13.0->-r requirements.txt (line 14)) (1.3.1)\n",
            "Requirement already satisfied: llvmlite<0.40,>=0.39.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba>=0.49->umap-learn==0.5.3->-r requirements.txt (line 13)) (0.39.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->exmol==3.0.2->-r requirements.txt (line 1)) (3.3.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->exmol==3.0.2->-r requirements.txt (line 1)) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->exmol==3.0.2->-r requirements.txt (line 1)) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->exmol==3.0.2->-r requirements.txt (line 1)) (2023.7.22)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard==2.13.0->-r requirements.txt (line 14)) (2.1.3)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain->exmol==3.0.2->-r requirements.txt (line 1)) (2.0.22)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain->exmol==3.0.2->-r requirements.txt (line 1)) (3.8.6)\n",
            "Requirement already satisfied: anyio<4.0 in /usr/local/lib/python3.10/dist-packages (from langchain->exmol==3.0.2->-r requirements.txt (line 1)) (3.7.1)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain->exmol==3.0.2->-r requirements.txt (line 1)) (4.0.3)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain->exmol==3.0.2->-r requirements.txt (line 1))\n",
            "  Downloading dataclasses_json-0.6.1-py3-none-any.whl (27 kB)\n",
            "Collecting jsonpatch<2.0,>=1.33 (from langchain->exmol==3.0.2->-r requirements.txt (line 1))\n",
            "  Downloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
            "Collecting langsmith<0.1.0,>=0.0.52 (from langchain->exmol==3.0.2->-r requirements.txt (line 1))\n",
            "  Downloading langsmith-0.0.53-py3-none-any.whl (43 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.3/43.3 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain->exmol==3.0.2->-r requirements.txt (line 1)) (1.10.13)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain->exmol==3.0.2->-r requirements.txt (line 1)) (8.2.3)\n",
            "Collecting pystow (from synspace->exmol==3.0.2->-r requirements.txt (line 1))\n",
            "  Downloading pystow-0.5.0-py3-none-any.whl (37 kB)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain->exmol==3.0.2->-r requirements.txt (line 1)) (23.1.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain->exmol==3.0.2->-r requirements.txt (line 1)) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain->exmol==3.0.2->-r requirements.txt (line 1)) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain->exmol==3.0.2->-r requirements.txt (line 1)) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain->exmol==3.0.2->-r requirements.txt (line 1)) (1.3.1)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<4.0->langchain->exmol==3.0.2->-r requirements.txt (line 1)) (1.3.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<4.0->langchain->exmol==3.0.2->-r requirements.txt (line 1)) (1.1.3)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain->exmol==3.0.2->-r requirements.txt (line 1))\n",
            "  Downloading marshmallow-3.20.1-py3-none-any.whl (49 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain->exmol==3.0.2->-r requirements.txt (line 1))\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Collecting jsonpointer>=1.9 (from jsonpatch<2.0,>=1.33->langchain->exmol==3.0.2->-r requirements.txt (line 1))\n",
            "  Downloading jsonpointer-2.4-py2.py3-none-any.whl (7.8 kB)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard==2.13.0->-r requirements.txt (line 14)) (0.5.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard==2.13.0->-r requirements.txt (line 14)) (3.2.2)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain->exmol==3.0.2->-r requirements.txt (line 1)) (3.0.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from pystow->synspace->exmol==3.0.2->-r requirements.txt (line 1)) (8.1.7)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain->exmol==3.0.2->-r requirements.txt (line 1))\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Building wheels for collected packages: umap-learn, pynndescent, ratelimit\n",
            "  Building wheel for umap-learn (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for umap-learn: filename=umap_learn-0.5.3-py3-none-any.whl size=82807 sha256=4a6be216b9908039ed9abe781afefde15a3441a750240b18334eccbfb130b290\n",
            "  Stored in directory: /root/.cache/pip/wheels/a0/e8/c6/a37ea663620bd5200ea1ba0907ab3c217042c1d035ef606acc\n",
            "  Building wheel for pynndescent (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pynndescent: filename=pynndescent-0.5.10-py3-none-any.whl size=55615 sha256=c9392aa87ddbfe3b39e96521681e6d5333f8461fbe40f8f5229f1dd67948c97d\n",
            "  Stored in directory: /root/.cache/pip/wheels/4a/38/5d/f60a40a66a9512b7e5e83517ebc2d1b42d857be97d135f1096\n",
            "  Building wheel for ratelimit (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ratelimit: filename=ratelimit-2.2.1-py3-none-any.whl size=5894 sha256=37d2b11370edba7fc42b3f5694466b01fa7eb59580c834e51e39e9b95ee5b74a\n",
            "  Stored in directory: /root/.cache/pip/wheels/27/5f/ba/e972a56dcbf5de9f2b7d2b2a710113970bd173c4dcd3d2c902\n",
            "Successfully built umap-learn pynndescent ratelimit\n",
            "Installing collected packages: ratelimit, typing_extensions, selfies, PyYAML, numpy, networkx, mypy-extensions, marshmallow, jsonpointer, typing-inspect, scipy, rdkit, pystow, pandas, matplotlib, jsonpatch, skunk, scikit-learn, langsmith, dataclasses-json, tensorboard, synspace, pynndescent, langchain, umap-learn, exmol\n",
            "  Attempting uninstall: typing_extensions\n",
            "    Found existing installation: typing_extensions 4.5.0\n",
            "    Uninstalling typing_extensions-4.5.0:\n",
            "      Successfully uninstalled typing_extensions-4.5.0\n",
            "  Attempting uninstall: PyYAML\n",
            "    Found existing installation: PyYAML 6.0.1\n",
            "    Uninstalling PyYAML-6.0.1:\n",
            "      Successfully uninstalled PyYAML-6.0.1\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.23.5\n",
            "    Uninstalling numpy-1.23.5:\n",
            "      Successfully uninstalled numpy-1.23.5\n",
            "  Attempting uninstall: networkx\n",
            "    Found existing installation: networkx 3.2\n",
            "    Uninstalling networkx-3.2:\n",
            "      Successfully uninstalled networkx-3.2\n",
            "  Attempting uninstall: scipy\n",
            "    Found existing installation: scipy 1.11.3\n",
            "    Uninstalling scipy-1.11.3:\n",
            "      Successfully uninstalled scipy-1.11.3\n",
            "  Attempting uninstall: pandas\n",
            "    Found existing installation: pandas 1.5.3\n",
            "    Uninstalling pandas-1.5.3:\n",
            "      Successfully uninstalled pandas-1.5.3\n",
            "  Attempting uninstall: matplotlib\n",
            "    Found existing installation: matplotlib 3.7.1\n",
            "    Uninstalling matplotlib-3.7.1:\n",
            "      Successfully uninstalled matplotlib-3.7.1\n",
            "  Attempting uninstall: scikit-learn\n",
            "    Found existing installation: scikit-learn 1.2.2\n",
            "    Uninstalling scikit-learn-1.2.2:\n",
            "      Successfully uninstalled scikit-learn-1.2.2\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.14.1\n",
            "    Uninstalling tensorboard-2.14.1:\n",
            "      Successfully uninstalled tensorboard-2.14.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "lida 0.0.10 requires fastapi, which is not installed.\n",
            "lida 0.0.10 requires kaleido, which is not installed.\n",
            "lida 0.0.10 requires python-multipart, which is not installed.\n",
            "lida 0.0.10 requires uvicorn, which is not installed.\n",
            "llmx 0.0.15a0 requires cohere, which is not installed.\n",
            "llmx 0.0.15a0 requires openai, which is not installed.\n",
            "llmx 0.0.15a0 requires tiktoken, which is not installed.\n",
            "arviz 0.15.1 requires scipy>=1.8.0, but you have scipy 1.7.3 which is incompatible.\n",
            "bigframes 0.10.0 requires pandas>=1.5.0, but you have pandas 1.3.5 which is incompatible.\n",
            "bigframes 0.10.0 requires scikit-learn>=1.2.2, but you have scikit-learn 1.0.2 which is incompatible.\n",
            "google-colab 1.0.0 requires pandas==1.5.3, but you have pandas 1.3.5 which is incompatible.\n",
            "jax 0.4.16 requires numpy>=1.22, but you have numpy 1.21.6 which is incompatible.\n",
            "jaxlib 0.4.16+cuda11.cudnn86 requires numpy>=1.22, but you have numpy 1.21.6 which is incompatible.\n",
            "plotnine 0.12.3 requires matplotlib>=3.6.0, but you have matplotlib 3.5.3 which is incompatible.\n",
            "plotnine 0.12.3 requires numpy>=1.23.0, but you have numpy 1.21.6 which is incompatible.\n",
            "plotnine 0.12.3 requires pandas>=1.5.0, but you have pandas 1.3.5 which is incompatible.\n",
            "tensorflow 2.14.0 requires numpy>=1.23.5, but you have numpy 1.21.6 which is incompatible.\n",
            "tensorflow 2.14.0 requires tensorboard<2.15,>=2.14, but you have tensorboard 2.13.0 which is incompatible.\n",
            "torchaudio 2.1.0+cu118 requires torch==2.1.0, but you have torch 1.12.1+cu113 which is incompatible.\n",
            "torchdata 0.7.0 requires torch==2.1.0, but you have torch 1.12.1+cu113 which is incompatible.\n",
            "torchtext 0.16.0 requires torch==2.1.0, but you have torch 1.12.1+cu113 which is incompatible.\n",
            "xarray 2023.7.0 requires pandas>=1.4, but you have pandas 1.3.5 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed PyYAML-6.0 dataclasses-json-0.6.1 exmol-3.0.2 jsonpatch-1.33 jsonpointer-2.4 langchain-0.0.325 langsmith-0.0.53 marshmallow-3.20.1 matplotlib-3.5.3 mypy-extensions-1.0.0 networkx-2.6.3 numpy-1.21.6 pandas-1.3.5 pynndescent-0.5.10 pystow-0.5.0 ratelimit-2.2.1 rdkit-2023.3.1 scikit-learn-1.0.2 scipy-1.7.3 selfies-2.1.1 skunk-1.2.0 synspace-0.3.0 tensorboard-2.13.0 typing-inspect-0.9.0 typing_extensions-4.4.0 umap-learn-0.5.3\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "matplotlib",
                  "mpl_toolkits",
                  "networkx",
                  "numpy",
                  "pandas",
                  "scipy",
                  "tensorboard",
                  "yaml"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# The following is the content of the file 'config_pretrain.yaml'\n",
        "\n",
        "batch_size: 512 # batch size\n",
        "warm_up: 10 # warm-up epochs\n",
        "epochs: 2 # total number of epochs\n",
        "\n",
        "load_model: pretrained_gin # resume training\n",
        "eval_every_n_epochs: 1 # validation frequency\n",
        "save_every_n_epochs: 5 # automatic model saving frequecy\n",
        "log_every_n_steps: 50 # print training log frequency\n",
        "\n",
        "fp16_precision: False # float precision 16 (i.e. True/False)\n",
        "init_lr: 0.0005 # initial learning rate for Adam\n",
        "weight_decay: 1e-5 # weight decay for Adam\n",
        "gpu: cuda:0 # training GPU\n",
        "\n",
        "model:\n",
        "  num_layer: 5 # number of graph conv layers\n",
        "  emb_dim: 300 # embedding dimension in graph conv layers\n",
        "  feat_dim: 512 # output feature dimention\n",
        "  drop_ratio: 0 # dropout ratio\n",
        "  pool: mean # readout pooling (i.e., mean/max/add)\n",
        "\n",
        "aug: node # molecule graph augmentation strategy (i.e., node/subgraph/mix)\n",
        "dataset:\n",
        "  num_workers: 12 # dataloader number of workers\n",
        "  valid_size: 0.05 # ratio of validation data\n",
        "  data_path: data/{yourowndata}.csv # path of pre-training data\n",
        "\n",
        "loss:\n",
        "  temperature: 0.1 # temperature of NT-Xent loss\n",
        "  use_cosine_similarity: True # whether to use cosine similarity in NT-Xent loss (i.e. True/False)\n"
      ],
      "metadata": {
        "id": "6w6EeesTQimc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Further ahead from here, I will try to pre-train the MolCLR model with the file 'pretrain.py'"
      ],
      "metadata": {
        "id": "0QU4PQmcTdB1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "import sys\n",
        "import torch\n",
        "import yaml\n",
        "import numpy as np\n",
        "from datetime import datetime"
      ],
      "metadata": {
        "id": "E0MpRX0uNXq5"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# imports the torch.nn.functional module\n",
        "# The torch.nn.functional module contains various functions that are commonly used in neural network operations, such as activation functions, loss functions, and other operations commonly applied to tensors.\n",
        "\n",
        "import torch.nn.functional as F"
      ],
      "metadata": {
        "id": "7Xvqiq7jXgV6"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This line imports the SummaryWriter class from the torch.utils.tensorboard module.\n",
        "# SummaryWriter is a PyTorch utility that enables you to write TensorBoard-compatible logs.\n",
        "# TensorBoard is a visualization tool provided with TensorFlow, but PyTorch provides integration to use it with PyTorch models as well.\n",
        "\n",
        "from torch.utils.tensorboard import SummaryWriter"
      ],
      "metadata": {
        "id": "4bVUpCKKX77L"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# imports the CosineAnnealingLR class from the torch.optim.lr_scheduler module in PyTorch.\n",
        "# The learning rate is a hyperparameter that controls how much we are adjusting the weights of our network during training.\n",
        "# CosineAnnealingLR is a learning rate scheduler in PyTorch that reduces the learning rate following the cosine annealing schedule.\n",
        "# The learning rate starts at the initial value and is decreased following a cosine function until it reaches a minimum value and then starts increasing again.\n",
        "# This kind of schedule is often used to improve the convergence and generalization of neural networks during training.\n",
        "\n",
        "from torch.optim.lr_scheduler import CosineAnnealingLR"
      ],
      "metadata": {
        "id": "mRXCXaxpoxZ6"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Changing the working directory to 'utils'\n",
        "\n",
        "%cd /content/drive/MyDrive/AGILE"
      ],
      "metadata": {
        "id": "sfuEgK1r7rqa",
        "outputId": "6de8871e-b945-45dd-cd22-228fc4cbe8f3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/AGILE\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#  imports the NTXentLoss class from a custom module named nt_xent inside a package or directory called utils.\n",
        "# This class likely contains the implementation of the NT-Xent loss function, which is commonly used in contrastive learning tasks.\n",
        "\n",
        "from utils.nt_xent import NTXentLoss"
      ],
      "metadata": {
        "id": "kDmsoDdg1muZ"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This code snippet checks if the Apex library is installed and imports it for mixed-precision training if it is available.\n",
        "# If Apex is not installed, it prints a message indicating that Apex needs to be installed from a specific GitHub repository.\n",
        "\n",
        "apex_support = False # Initializes the apex_support variable to False.\n",
        "try: #The code inside the try block attempts to import the amp module from the Apex library.\n",
        "    sys.path.append(\"./apex\") #  Adds the \"./apex\" directory to the Python system path, allowing Python to find the Apex module in that directory.\n",
        "    from apex import amp # Tries to import the amp module from the Apex library.\n",
        "\n",
        "    apex_support = True # If the import is successful, sets apex_support to True indicating that Apex is available.\n",
        "except: # If there is an ImportError (i.e., Apex is not installed), the code inside the except block is executed.\n",
        "    print(\n",
        "        \"Please install apex for mixed precision training from: https://github.com/NVIDIA/apex\" # Prints a message indicating that Apex needs to be installed and provides the GitHub repository URL for installation.\n",
        "    )\n",
        "    apex_support = False"
      ],
      "metadata": {
        "id": "YxwY5g4G8PMs"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Installing Nvidia apex\n",
        "\n",
        "import os, sys, shutil\n",
        "import time\n",
        "import gc\n",
        "from contextlib import contextmanager\n",
        "from pathlib import Path\n",
        "import random\n",
        "import numpy as np, pandas as pd\n",
        "from tqdm import tqdm, tqdm_notebook\n",
        "\n",
        "@contextmanager\n",
        "def timer(name):\n",
        "    t0 = time.time()\n",
        "    yield\n",
        "    print(f'[{name}] done in {time.time() - t0:.0f} s')\n",
        "\n",
        "USE_APEX = True\n",
        "\n",
        "if USE_APEX:\n",
        "            with timer('install Nvidia apex'):\n",
        "                # Installing Nvidia Apex\n",
        "                os.system('git clone https://github.com/NVIDIA/apex; cd apex; pip install -v --no-cache-dir' +\n",
        "                          ' --global-option=\"--cpp_ext\" --global-option=\"--cuda_ext\" ./')\n",
        "                os.system('rm -rf apex/.git') # too many files, Kaggle fails\n"
      ],
      "metadata": {
        "id": "0RBiIXQF_gUX",
        "outputId": "132c9f26-67cc-4be3-9a27-5d0d0aa52801",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[install Nvidia apex] done in 1 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from apex import amp\n"
      ],
      "metadata": {
        "id": "hMtgroCiCiFh"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This function appears to be responsible for saving a configuration file (config.yaml)...\n",
        "# ...to a specified directory (model_checkpoints_folder).\n",
        "def _save_config_file(model_checkpoints_folder): # checks if the model_checkpoints_folder directory exists\n",
        "    if not os.path.exists(model_checkpoints_folder): # If the directory does not exist, it creates it\n",
        "        os.makedirs(model_checkpoints_folder) # copies a configuration file specified by args.config to this directory\n",
        "        shutil.copy(args.config, os.path.join(model_checkpoints_folder, \"config.yaml\")) #  copied file is named \"config.yaml\"\n",
        "\n",
        "\n",
        "class PreTrain(object): # initializer for a training process\n",
        "    def __init__(self, dataset, config): # constructor for the PreTrain class. It takes two arguments: dataset and config.\n",
        "        self.config = config # assigns the config argument to an instance variable self.config.\n",
        "        self.device = self._get_device() # It calls the _get_device() method to determine and store the...\n",
        "                                         # ...computing device (CPU or GPU) to be used for training\n",
        "\n",
        "        dir_name = datetime.now().strftime(\"%b%d_%H-%M-%S\") # generates a timestamp string in format \"MonthDay_Hour-Minute-Second\"\n",
        "        log_dir = os.path.join(\"ckpt\", dir_name)\n",
        "        # It creates a directory path by joining \"ckpt\" (presumably a checkpoint directory)...\n",
        "        # ...and the timestamp generated in the previous step. This path is where logs related to the training process will be saved.\n",
        "        self.writer = SummaryWriter(log_dir=log_dir) # It initializes a SummaryWriter object...\n",
        "        # ...typically used for logging and visualization during training. The logs will be saved in the directory specified by log_dir.\n",
        "\n",
        "        self.dataset = dataset # It assigns the dataset argument to an instance variable 'self.dataset'...\n",
        "        # ...This presumably represents the dataset used for training.\n",
        "        self.nt_xent_criterion = NTXentLoss(\n",
        "            self.device, config[\"batch_size\"], **config[\"loss\"]\n",
        "        ) #  It initializes an instance of NTXentLoss and assigns it to the self.nt_xent_criterion instance variable.\n",
        "        # This is likely a loss function used for training,\n",
        "        # it is configured based on the config dictionary's \"batch_size\" and \"loss\" settings.\n",
        "\n",
        "\n",
        "## this method is responsible for dynamically selecting the appropriate computing device for training based on system capabilities and the configuration settings.\n",
        "    # It sets the device to GPU if available and allowed, or to CPU if not.\n",
        "    def _get_device(self):\n",
        "        if torch.cuda.is_available() and self.config[\"gpu\"] != \"cpu\": # checks if CUDA (GPU support for PyTorch) is available on the system.\n",
        "            # checks if the \"gpu\" setting in the configuration (self.config) is not explicitly set to \"cpu\".\n",
        "            device = self.config[\"gpu\"] # assigns the GPU device specified in the configuration to the device variable.\n",
        "            torch.cuda.set_device(device) # sets the current CUDA device to the one specified in device.\n",
        "                                          # This ensures that the specified GPU is used for training\n",
        "        else:\n",
        "            device = \"cpu\" # assigns the string \"cpu\" to the device variable, indicating CPU usage.\n",
        "        print(\"Running on:\", device) # prints a message indicating whether the code is running on the CPU or GPU\n",
        "\n",
        "        return device # method returns the selected device (device) as a string (\"cpu\" or the GPU identifier).\n",
        "\n",
        "\n",
        "## The _step method appears to define a single training step within a training loop for a contrastive learning task.\n",
        "    def _step(self, model, xis, xjs, n_iter):\n",
        "        # get the representations and the projections\n",
        "        ris, zis = model(xis)  # [N,C]\n",
        "        # This line passes the input data 'xis' through the neural network model. It computes two sets of values:\n",
        "        # ris: These are the intermediate representations (features) obtained from the model for the input data xis.\n",
        "        # zis: These are the projection vectors or embeddings corresponding to the representations ris.\n",
        "\n",
        "        # get the representations and the projections\n",
        "        rjs, zjs = model(xjs)  # [N,C]\n",
        "        # this line passes the input data xjs through the same model to obtain representations and projections for xjs.\n",
        "        #These are stored in rjs and zjs, respectively.\n",
        "\n",
        "        # normalize projection feature vectors\n",
        "        zis = F.normalize(zis, dim=1)\n",
        "        zjs = F.normalize(zjs, dim=1)\n",
        "        # These lines normalize the projection feature vectors zis and zjs along dimension 1 (usually the channel dimension for image data).\n",
        "        # Normalization is typically done to ensure that the embeddings have a consistent scale, which can be important for contrastive learning.\n",
        "\n",
        "        loss = self.nt_xent_criterion(zis, zjs)\n",
        "        # This line computes the loss between the normalized projection vectors zis and zjs.\n",
        "        # It uses the nt_xent_criterion, which is likely a custom loss function specifically designed for contrastive learning tasks.\n",
        "        # Contrastive loss functions aim to minimize the similarity (e.g., cosine similarity) between positive pairs (pairs of similar items)...\n",
        "        # ...and maximize the similarity between negative pairs (pairs of dissimilar items).\n",
        "        return loss\n",
        "        # Finally, the computed loss is returned as the result of this _step method.\n",
        "\n",
        "\n",
        " ## This portion of the train method sets up the training process for the neural network model.\n",
        "    def train(self):\n",
        "        train_loader, valid_loader = self.dataset.get_data_loaders()\n",
        "        # This line obtains training and validation data loaders from the dataset object.\n",
        "        # It suggests that the dataset object has a method named 'get_data_loaders' that returns data loaders for training and validation data.\n",
        "\n",
        "        from models.agile_pretrain import AGILE # It imports the AGILE model from a module named 'agile_pretrain' within the 'models' package.\n",
        "        model = AGILE(**self.config[\"model\"]).to(self.device) # It instantiates the AGILE model using the model configuration specified in self.config[\"model\"].\n",
        "        # AGILE is likely a class that defines a neural network\n",
        "        # The ** syntax is used to unpack the dictionary and pass its contents as keyword arguments to the AGILE constructor.\n",
        "        # **self.config[\"model\"] suggests that the initialization parameters for the AGILE class are being unpacked from a dictionary.\n",
        "        # This dictionary is part of a larger configuration attribute or dictionary named config attached to the class instance self.\n",
        "        # The resulting model is then moved to the specified device (self.device), which can be either CPU or GPU.\n",
        "        model = self._load_pre_trained_weights(model) #  It loads pre-trained weights for the model if they exist.\n",
        "        # method _load_pre_trained_weights takes the model instance as an argument and presumably loads it with pre-trained weights.\n",
        "        # The method is prefixed with an underscore _, which by convention implies that it's meant to be a private method\n",
        "        #If pre-trained weights are not found, it initializes the model from scratch.\n",
        "        print(model)\n",
        "\n",
        "       ## Optimizer Initialization:\n",
        "        optimizer = torch.optim.Adam( #Here, an Adam optimizer is being created. This optimizer is a popular choice for training neural networks due to its adaptive learning rate properties.\n",
        "            model.parameters(), #This retrieves all the parameters of the model that are to be optimized.\n",
        "            self.config[\"init_lr\"], #This is the initial learning rate for the optimizer, likely specified in the config dictionary of the class instance.\n",
        "            weight_decay=eval(self.config[\"weight_decay\"]), # weight_decay is a regularization term that helps prevent overfitting by penalizing large weights.\n",
        "        )\n",
        "\n",
        "        ## Learning Rate Scheduler:\n",
        "        scheduler = CosineAnnealingLR( # A learning rate scheduler adjusts the learning rate during training, and\n",
        "                                       # CosineAnnealingLR is a type of scheduler that reduces the learning rate following a cosine decay schedule.\n",
        "            optimizer,\n",
        "            T_max=self.config[\"epochs\"] - self.config[\"warm_up\"], # This parameter defines the number of epochs after which the learning rate cycle should reset.\n",
        "                                                                  # It's taking the total number of epochs and subtracting a warm-up phase where the learning rate is not yet reduced.\n",
        "            eta_min=0, # This is the minimum learning rate the scheduler will reduce to.\n",
        "            last_epoch=-1, # This typically signifies that the scheduler should start from the beginning.\n",
        "        )\n",
        "\n",
        "        ## Mixed-Precision Training:\n",
        "        if apex_support and self.config[\"fp16_precision\"]: # The if statement checks if Apex (a PyTorch extension for mixed-precision and distributed training) is supported...\n",
        "                                                           # ...and if half-precision (fp16) is enabled in the configuration.\n",
        "            model, optimizer = amp.initialize( # If conditions are met, the model and optimizer are wrapped by Apex's Automatic Mixed Precision (AMP) tool to enable mixed-precision training...\n",
        "                                              # ...which can speed up computation and reduce memory usage on capable GPUs.\n",
        "                model, optimizer, opt_level=\"O2\", keep_batchnorm_fp32=True\n",
        "            )\n",
        "\n",
        "        ## Checkpoint Folder:\n",
        "        model_checkpoints_folder = os.path.join(self.writer.log_dir, \"checkpoints\") # This line creates a path to a directory where model checkpoints (saved states of the model) will be stored.\n",
        "        # The log_dir attribute of self.writer is used as the base directory, which suggests that self.writer is an instance of some logging or visualization tool, like TensorBoard for PyTorch.\n",
        "\n",
        "        ## save config file\n",
        "        _save_config_file(model_checkpoints_folder) # Before entering the training loop, it seems there is a method call to save the configuration file to the model checkpoints folder.\n",
        "                                                    # This is a common practice for reproducibility, allowing one to know the exact settings used during training.\n",
        "\n",
        "       ## Training Variables Initialization:\n",
        "        n_iter = 0 # keeps track of the number of iterations (or steps) during the training process.\n",
        "        valid_n_iter = 0\n",
        "        best_valid_loss = np.inf # is initialized to infinity and is intended to keep track of the best validation loss obtained during training for model checkpointing.\n",
        "\n",
        "        ## Training Loop:\n",
        "        for epoch_counter in range(self.config[\"epochs\"]): # The outer loop iterates over the number of epochs specified in the configuration.\n",
        "            for bn, (xis, xjs) in enumerate(train_loader): # The inner loop enumerates over train_loader, which yields batches of data (xis and xjs).\n",
        "                                                           # These are typically the input and target pairs or could be two views of the same input in contrastive learning scenarios\n",
        "                optimizer.zero_grad() # clears old gradients from the previous step before the loss calculation.\n",
        "\n",
        "                xis = xis.to(self.device)\n",
        "                xjs = xjs.to(self.device) # These lines move the current batch of data onto the device (GPU or CPU) for computation.\n",
        "\n",
        "                # Loss Computation and Logging:\n",
        "                loss = self._step(model, xis, xjs, n_iter) # a call to a method _step which likely performs a forward pass, computes the loss, and possibly some other operations.\n",
        "\n",
        "                if n_iter % self.config[\"log_every_n_steps\"] == 0: # If the iteration number is a multiple of a logging step (log_every_n_steps)...\n",
        "                                                                   # ...it logs the training loss and the current learning rate.\n",
        "                    self.writer.add_scalar(\"train_loss\", loss, global_step=n_iter)\n",
        "                    self.writer.add_scalar(\n",
        "                        \"cosine_lr_decay\",\n",
        "                        scheduler.get_last_lr()[0],\n",
        "                        global_step=n_iter,\n",
        "                    )\n",
        "                    print(\"Epoch:\", epoch_counter, \"Iteration:\", bn, \"Train loss:\",loss.item())\n",
        "\n",
        "                # Gradient Calculation:\n",
        "                # If apex_support and fp16_precision are enabled, the gradient computation is scaled for mixed-precision training to prevent underflow.\n",
        "                # Otherwise, loss.backward() computes the gradients of the loss with respect to the model parameters.\n",
        "                if apex_support and self.config[\"fp16_precision\"]:\n",
        "                    with amp.scale_loss(loss, optimizer) as scaled_loss:\n",
        "                        scaled_loss.backward()\n",
        "                else:\n",
        "                    loss.backward()\n",
        "\n",
        "              # Optimizer Step:\n",
        "                optimizer.step() # Updates the model parameters based on the gradients computed.\n",
        "                n_iter += 1 # The iteration counter is incremented after each batch is processed.\n",
        "\n",
        "            # Validation check:\n",
        "            if epoch_counter % self.config[\"eval_every_n_epochs\"] == 0: # checks if the current epoch is one at which the model should be evaluated on the validation set.\n",
        "                valid_loss = self._validate(model, valid_loader) # The method _validate is called with the current model and the validation data loader.\n",
        "                                                                 # This method likely performs a forward pass of the validation data through the model and computes the validation loss.\n",
        "                print(\"Epoch:\", epoch_counter, \"Iteration:\", bn, \"Valid loss:\", valid_loss) # A print statement outputs the current epoch...\n",
        "                                                                                            # ...iteration number (from the last batch of the training loop), and the validation loss.\n",
        "\n",
        "                # Model Checkpointing:\n",
        "                # If the validation loss is lower than the best_valid_loss recorded so far, indicating an improvement, the model's state is saved to disk.\n",
        "                # The best_valid_loss is updated to the current valid_loss.\n",
        "                if valid_loss < best_valid_loss:\n",
        "                    best_valid_loss = valid_loss\n",
        "                    torch.save(\n",
        "                        model.state_dict(),\n",
        "                        os.path.join(model_checkpoints_folder, \"model.pth\"), # Saves the model's weights to a file named model.pth within the model_checkpoints_folder.\n",
        "                    )\n",
        "\n",
        "                # Validation Loss Logging\n",
        "                self.writer.add_scalar(\n",
        "                    \"validation_loss\", valid_loss, global_step=valid_n_iter # This logs the validation loss to some visualization tool (like TensorBoard)...\n",
        "                                                                            # ...for tracking the model's performance over time.\n",
        "                )\n",
        "                valid_n_iter += 1\n",
        "\n",
        "            # Periodic Model Saving:\n",
        "            if (epoch_counter + 1) % self.config[\"save_every_n_epochs\"] == 0: # Checks whether the model should be saved at this epoch according to a regular interval defined in the configuration.\n",
        "                                                      # If true, the model's state is saved with a filename that includes the epoch number, ensuring that each saved model is uniquely identifiable.\n",
        "                torch.save(\n",
        "                    model.state_dict(),\n",
        "                    os.path.join(\n",
        "                        model_checkpoints_folder,\n",
        "                        \"model_{}.pth\".format(str(epoch_counter)),\n",
        "                    ),\n",
        "                )\n",
        "\n",
        "            # Warmup for the first few epochs:\n",
        "            # Learning Rate Scheduler Update:\n",
        "            if epoch_counter >= self.config[\"warm_up\"]: # checks whether the model has completed the warm-up phase.\n",
        "                scheduler.step() # If the model is past the warm-up phase, the learning rate scheduler updates the learning rate according to its policy (CosineAnnealingLR in this case).\n",
        "\n",
        "## Loading of pre-training weights\n",
        "    def _load_pre_trained_weights(self, model):\n",
        "        try:\n",
        "            checkpoints_folder = os.path.join(\n",
        "                \"./ckpt\", self.config[\"load_model\"], \"checkpoints\" # The method constructs a path to the checkpoints folder by joining a base directory ./ckpt,\n",
        "                                                                   # ...a subdirectory named after the configuration entry self.config[\"load_model\"], and a further subdirectory checkpoints.\n",
        "                                                                   # This establishes where the model's pre-trained weights should reside on the file system.\n",
        "            )\n",
        "            state_dict = torch.load( # It attempts to load the state dictionary (a Python dictionary object that maps each layer to its parameters)...\n",
        "                                     # ...from a file named model.pth within the constructed checkpoints_folder path.\n",
        "                os.path.join(checkpoints_folder, \"model.pth\"),\n",
        "                map_location=self.device, # This argument tells torch.load to map the model's parameters to the specified device (CPU or GPU).\n",
        "            )\n",
        "            model.load_state_dict(state_dict) # If the state dictionary is successfully loaded, this line applies the pre-trained weights to the model.\n",
        "            print(\"Loaded pre-trained model with success.\")\n",
        "        except FileNotFoundError:\n",
        "            print(\"Pre-trained weights not found. Training from scratch.\") # catches the case where model.pth does not exist at the expected path.\n",
        "\n",
        "        return model # Regardless of whether the pre-trained weights are loaded successfully, the model is returned at the end of the method.\n",
        "\n",
        "## Evaluation of the model on a validation dataset and compute the average loss.\n",
        "    def _validate(self, model, valid_loader):\n",
        "        with torch.no_grad(): # This context manager is used to deactivate gradient calculations, saving memory, and computation since gradients are not needed during validation.\n",
        "            model.eval() # This switches the model to evaluation mode, which can affect the behavior of certain layers like dropout layers and batch normalization,\n",
        "                         # ensuring they behave consistently during both training and evaluation.\n",
        "\n",
        "            valid_loss = 0.0 # initializes a variable to accumulate the total validation loss.\n",
        "            counter = 0 # keeps track of the number of batches processed.\n",
        "            for (xis, xjs) in valid_loader: # The loop iterates over the validation data loader, valid_loader, which provides batches of data pairs (xis, xjs).\n",
        "                xis = xis.to(self.device) # move the data to the device (CPU or GPU) that the model is on.\n",
        "                xjs = xjs.to(self.device) # move the data to the device (CPU or GPU) that the model is on.\n",
        "\n",
        "                loss = self._step(model, xis, xjs, counter)# This line calls the _step method, which likely performs a forward pass of the model on the validation data and computes the loss.\n",
        "                valid_loss += loss.item() # adds the computed loss (converted to a Python float with .item()) to the total validation loss.\n",
        "                counter += 1 #  increments the batch counter.\n",
        "            valid_loss /= counter # After processing all batches, the total validation loss is divided by the number of batches to get the average loss over the validation set.\n",
        "\n",
        "        model.train() # The model is switched back to training mode after validation is done to ensure it is ready for further training if needed.\n",
        "        return valid_loss # The method returns the average validation loss, which can be used to monitor the model's performance...\n",
        "                          # ...and possibly for early stopping or model checkpointing if it's part of a larger training pipeline.\n",
        "\n"
      ],
      "metadata": {
        "id": "mBoxDWXdvCtV"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## The main function is the entry point for a Python script that sets up and starts the training process for a machine learning model, based on the provided configuration.\n",
        "def main(config):\n",
        "    if config[\"aug\"] == \"node\": # The function first checks the config dictionary for an \"aug\" key, which specifies the type of data augmentation to use.\n",
        "                                # Depending on the augmentation type (\"node\", \"subgraph\", or \"mix\"), it imports the 'MoleculeDatasetWrapper' class from different modules.\n",
        "                                # This implies that the dataset handling differs based on the augmentation strategy.\n",
        "        from dataset.dataset import MoleculeDatasetWrapper\n",
        "    elif config[\"aug\"] == \"subgraph\":\n",
        "        from dataset.dataset_subgraph import MoleculeDatasetWrapper\n",
        "    elif config[\"aug\"] == \"mix\":\n",
        "        from dataset.dataset_mix import MoleculeDatasetWrapper\n",
        "    else:\n",
        "        raise ValueError(\"Not defined molecule augmentation!\") # If an augmentation type is not recognized, it raises a ValueError, indicating that the configuration is not set up correctly.\n",
        "\n",
        "    dataset = MoleculeDatasetWrapper(config[\"batch_size\"], **config[\"dataset\"]) # This line creates an instance of 'MoleculeDatasetWrapper' with the batch size...\n",
        "                                                                                # ...and additional dataset parameters from the configuration.\n",
        "    agile_pretrain = PreTrain(dataset, config) # An instance of the PreTrain class is created, which likely encapsulates the training process.\n",
        "                                               # It is initialized with the dataset and the entire configuration dictionary.\n",
        "    agile_pretrain.train() # Calls the train method of the agile_pretrain instance to start the training process.\n",
        "    print(f\"Training finished. Checkpoints saved in {agile_pretrain.writer.log_dir}.\") #"
      ],
      "metadata": {
        "id": "cGIOSxyGvlUX"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pwd"
      ],
      "metadata": {
        "id": "lqV2CxYHF0si",
        "outputId": "90c4fbf5-8963-49a3-d9c8-9f3ce50ee382",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/AGILE\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## This code snippet is typically found at the end of a Python script and is used to execute some code...\n",
        "## ...only if the script is run as the main program rather than being imported as a module in another script.\n",
        "if __name__ == \"__main__\": # checks whether the script is being run directly by the Python interpreter (which sets __name__ to \"__main__\" in this context) as opposed to being imported.\n",
        "    import argparse # The argparse module is imported, which is a standard Python library for parsing command-line arguments.\n",
        "    parser = argparse.ArgumentParser() # An ArgumentParser object is created to handle the command-line arguments.\n",
        "    parser.add_argument(\"config\", type=str, help=\"config_pretrain.yaml\") # The script is configured to accept a single positional argument config,\n",
        "                                                                         # which should be a string representing the path to a YAML configuration file (config_pretrain.yaml).\n",
        "    args = parser.parse_args([\"config_pretrain.yaml\"]) # Instead of parsing arguments from the command line, it's hardcoded to parse a list with a single string \"config_pretrain.yaml\".\n",
        "                                                       # This is unusual because typically parse_args() is called without arguments,\n",
        "                                                       # so it automatically uses the arguments passed to the script from the command line.\n",
        "    config = yaml.load(open(args.config, \"r\"), Loader=yaml.FullLoader) # This line loads the YAML configuration file into a Python dictionary.\n",
        "                                                                       # The open(args.config, \"r\") opens the file specified by the config argument, and yaml.load parses the YAML content.\n",
        "    print(config) # The loaded configuration is printed to the console, which can be useful for debugging or logging purposes.\n",
        "    main(config) # The main function is called with the configuration dictionary, which starts the training process as defined earlier."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 246
        },
        "id": "mwLajHHPBcUU",
        "outputId": "bd2cdd32-b65c-48bf-947b-0b3a807e9737"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-beaa64c78062>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_argument\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"config\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhelp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"config_pretrain.yaml\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"config_pretrain.yaml\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0myaml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLoader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0myaml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFullLoader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'yaml' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "qOLDeTitc19A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Further ahead from here, I will try to finetune the pre-trained MolCLR model with the file 'finetune.py'"
      ],
      "metadata": {
        "id": "P-NWi-i_c3U0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "import sys\n",
        "import torch\n",
        "import yaml\n",
        "import numpy as np\n",
        "from datetime import datetime"
      ],
      "metadata": {
        "id": "RS5bDNTnc9pH"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from torch.optim.lr_scheduler import CosineAnnealingLR"
      ],
      "metadata": {
        "id": "VMoGsaAEdKkQ"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from utils.nt_xent import NTXentLoss"
      ],
      "metadata": {
        "id": "Cw81GmAOdNfA"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "apex_support = False\n",
        "try:\n",
        "    sys.path.append(\"./apex\")\n",
        "    from apex import amp\n",
        "\n",
        "    apex_support = True\n",
        "except:\n",
        "    print(\n",
        "        \"Please install apex for mixed precision training from: https://github.com/NVIDIA/apex\"\n",
        "    )\n",
        "    apex_support = False"
      ],
      "metadata": {
        "id": "fo_sX9CidQor"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def _save_config_file(model_checkpoints_folder):\n",
        "    if not os.path.exists(model_checkpoints_folder): # checks if the directory where the model checkpoints are supposed to be saved does not exist.\n",
        "        os.makedirs(model_checkpoints_folder) # If the directory does not exist, os.makedirs can create intermediate directories and is equivalent to mkdir -p in Unix/Linux.\n",
        "        shutil.copy(args.config, os.path.join(model_checkpoints_folder, \"config.yaml\")) # This line copies the configuration file specified by args.config...\n",
        "                                                                                        # ...to the newly created directory under the name config.yaml."
      ],
      "metadata": {
        "id": "bERThEj6dUTa"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PreTrain(object):\n",
        "    def __init__(self, dataset, config):\n",
        "        self.config = config\n",
        "        self.device = self._get_device()\n",
        "\n",
        "        dir_name = datetime.now().strftime(\"%b%d_%H-%M-%S\")\n",
        "        log_dir = os.path.join(\"ckpt\", dir_name)\n",
        "        self.writer = SummaryWriter(log_dir=log_dir)\n",
        "\n",
        "        self.dataset = dataset\n",
        "        self.nt_xent_criterion = NTXentLoss(\n",
        "            self.device, config[\"batch_size\"], **config[\"loss\"]\n",
        "        )\n",
        "\n",
        "    def _get_device(self):\n",
        "        if torch.cuda.is_available() and self.config[\"gpu\"] != \"cpu\":\n",
        "            device = self.config[\"gpu\"]\n",
        "            torch.cuda.set_device(device)\n",
        "        else:\n",
        "            device = \"cpu\"\n",
        "        print(\"Running on:\", device)\n",
        "\n",
        "        return device\n",
        "\n",
        "    def _step(self, model, xis, xjs, n_iter):\n",
        "        # get the representations and the projections\n",
        "        ris, zis = model(xis)  # [N,C]\n",
        "\n",
        "        # get the representations and the projections\n",
        "        rjs, zjs = model(xjs)  # [N,C]\n",
        "\n",
        "        # normalize projection feature vectors\n",
        "        zis = F.normalize(zis, dim=1)\n",
        "        zjs = F.normalize(zjs, dim=1)\n",
        "\n",
        "        loss = self.nt_xent_criterion(zis, zjs)\n",
        "        return loss\n",
        "\n",
        "    def train(self):\n",
        "        train_loader, valid_loader = self.dataset.get_data_loaders()\n",
        "\n",
        "        from models.agile_pretrain import AGILE\n",
        "        model = AGILE(**self.config[\"model\"]).to(self.device)\n",
        "        model = self._load_pre_trained_weights(model)\n",
        "        print(model)\n",
        "\n",
        "        optimizer = torch.optim.Adam(\n",
        "            model.parameters(),\n",
        "            self.config[\"init_lr\"],\n",
        "            weight_decay=eval(self.config[\"weight_decay\"]),\n",
        "        )\n",
        "        scheduler = CosineAnnealingLR(\n",
        "            optimizer,\n",
        "            T_max=self.config[\"epochs\"] - self.config[\"warm_up\"],\n",
        "            eta_min=0,\n",
        "            last_epoch=-1,\n",
        "        )\n",
        "\n",
        "        if apex_support and self.config[\"fp16_precision\"]:\n",
        "            model, optimizer = amp.initialize(\n",
        "                model, optimizer, opt_level=\"O2\", keep_batchnorm_fp32=True\n",
        "            )\n",
        "\n",
        "        model_checkpoints_folder = os.path.join(self.writer.log_dir, \"checkpoints\")\n",
        "\n",
        "        # save config file\n",
        "        _save_config_file(model_checkpoints_folder)\n",
        "\n",
        "        n_iter = 0\n",
        "        valid_n_iter = 0\n",
        "        best_valid_loss = np.inf\n",
        "\n",
        "        for epoch_counter in range(self.config[\"epochs\"]):\n",
        "            for bn, (xis, xjs) in enumerate(train_loader):\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                xis = xis.to(self.device)\n",
        "                xjs = xjs.to(self.device)\n",
        "\n",
        "                loss = self._step(model, xis, xjs, n_iter)\n",
        "\n",
        "                if n_iter % self.config[\"log_every_n_steps\"] == 0:\n",
        "                    self.writer.add_scalar(\"train_loss\", loss, global_step=n_iter)\n",
        "                    self.writer.add_scalar(\n",
        "                        \"cosine_lr_decay\",\n",
        "                        scheduler.get_last_lr()[0],\n",
        "                        global_step=n_iter,\n",
        "                    )\n",
        "                    print(\"Epoch:\", epoch_counter, \"Iteration:\", bn, \"Train loss:\",loss.item())\n",
        "\n",
        "                if apex_support and self.config[\"fp16_precision\"]:\n",
        "                    with amp.scale_loss(loss, optimizer) as scaled_loss:\n",
        "                        scaled_loss.backward()\n",
        "                else:\n",
        "                    loss.backward()\n",
        "\n",
        "                optimizer.step()\n",
        "                n_iter += 1\n",
        "\n",
        "            # validate the model if requested\n",
        "            if epoch_counter % self.config[\"eval_every_n_epochs\"] == 0:\n",
        "                valid_loss = self._validate(model, valid_loader)\n",
        "                print(\"Epoch:\", epoch_counter, \"Iteration:\", bn, \"Valid loss:\", valid_loss)\n",
        "                if valid_loss < best_valid_loss:\n",
        "                    # save the model weights\n",
        "                    best_valid_loss = valid_loss\n",
        "                    torch.save(\n",
        "                        model.state_dict(),\n",
        "                        os.path.join(model_checkpoints_folder, \"model.pth\"),\n",
        "                    )\n",
        "\n",
        "                self.writer.add_scalar(\n",
        "                    \"validation_loss\", valid_loss, global_step=valid_n_iter\n",
        "                )\n",
        "                valid_n_iter += 1\n",
        "\n",
        "            if (epoch_counter + 1) % self.config[\"save_every_n_epochs\"] == 0:\n",
        "                torch.save(\n",
        "                    model.state_dict(),\n",
        "                    os.path.join(\n",
        "                        model_checkpoints_folder,\n",
        "                        \"model_{}.pth\".format(str(epoch_counter)),\n",
        "                    ),\n",
        "                )\n",
        "\n",
        "            # warmup for the first few epochs\n",
        "            if epoch_counter >= self.config[\"warm_up\"]:\n",
        "                scheduler.step()\n",
        "\n",
        "    def _load_pre_trained_weights(self, model):\n",
        "        try:\n",
        "            checkpoints_folder = os.path.join(\n",
        "                \"./ckpt\", self.config[\"load_model\"], \"checkpoints\"\n",
        "            )\n",
        "            state_dict = torch.load(\n",
        "                os.path.join(checkpoints_folder, \"model.pth\"),\n",
        "                map_location=self.device,\n",
        "            )\n",
        "            model.load_state_dict(state_dict)\n",
        "            print(\"Loaded pre-trained model with success.\")\n",
        "        except FileNotFoundError:\n",
        "            print(\"Pre-trained weights not found. Training from scratch.\")\n",
        "\n",
        "        return model\n",
        "\n",
        "    def _validate(self, model, valid_loader):\n",
        "        # validation steps\n",
        "        with torch.no_grad():\n",
        "            model.eval()\n",
        "\n",
        "            valid_loss = 0.0\n",
        "            counter = 0\n",
        "            for (xis, xjs) in valid_loader:\n",
        "                xis = xis.to(self.device)\n",
        "                xjs = xjs.to(self.device)\n",
        "\n",
        "                loss = self._step(model, xis, xjs, counter)\n",
        "                valid_loss += loss.item()\n",
        "                counter += 1\n",
        "            valid_loss /= counter\n",
        "\n",
        "        model.train()\n",
        "        return valid_loss"
      ],
      "metadata": {
        "id": "wGpGPrtjdgz4"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main(config):\n",
        "    if config[\"aug\"] == \"node\":\n",
        "        from dataset.dataset import MoleculeDatasetWrapper\n",
        "    elif config[\"aug\"] == \"subgraph\":\n",
        "        from dataset.dataset_subgraph import MoleculeDatasetWrapper\n",
        "    elif config[\"aug\"] == \"mix\":\n",
        "        from dataset.dataset_mix import MoleculeDatasetWrapper\n",
        "    else:\n",
        "        raise ValueError(\"Not defined molecule augmentation!\")\n",
        "\n",
        "    dataset = MoleculeDatasetWrapper(config[\"batch_size\"], **config[\"dataset\"])\n",
        "    agile_pretrain = PreTrain(dataset, config)\n",
        "    agile_pretrain.train()\n",
        "    print(f\"Training finished. Checkpoints saved in {agile_pretrain.writer.log_dir}.\")"
      ],
      "metadata": {
        "id": "u_bT7Xjzdjvq"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    import argparse\n",
        "\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument(\"config\", type=str, help=\"config_finetune.yaml\")\n",
        "    args = parser.parse_args([\"config_finetune.yaml\"])\n",
        "    config = yaml.load(open(args.config, \"r\"), Loader=yaml.FullLoader)\n",
        "    print(config)\n",
        "    main(config)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 370
        },
        "id": "hKTQZls-do3I",
        "outputId": "d54c32dd-b2a6-4507-df53-12c4110bb562"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'batch_size': 128, 'epochs': 30, 'eval_every_n_epochs': 1, 'fine_tune_from': 'pretrained_agile_60k', 'log_every_n_steps': 5, 'fp16_precision': False, 'init_lr': 0.0005, 'init_base_lr': 0.0001, 'weight_decay': '1e-6', 'gpu': 'cuda:0', 'task_name': 'lnp_hela_with_feat', 'model': {'num_layer': 5, 'emb_dim': 300, 'feat_dim': 512, 'drop_ratio': 0.3, 'pool': 'mean'}, 'dataset': {'num_workers': 4, 'valid_size': 0.1, 'test_size': 0.1, 'splitting': 'scaffold'}}\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-24-36051dfc749f>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0myaml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLoader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0myaml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFullLoader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-23-663f34ed6a76>\u001b[0m in \u001b[0;36mmain\u001b[0;34m(config)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"aug\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"node\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mMoleculeDatasetWrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"aug\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"subgraph\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset_subgraph\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mMoleculeDatasetWrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'aug'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Overall explanation of the finetune part:\n",
        "\n",
        "The PreTrain class encapsulates the functionality necessary for pre-training a machine learning model with a focus on a MoleculeDataset. The class is structured to handle the initialization of the training environment, the training process itself, including validation and checkpointing, and utility methods for device setup and weight loading. Let's summarize the key components and functionalities of this class:\n",
        "\n",
        "\n",
        "Initialization Method '__init__':\n",
        "Initializes the training configuration from the config parameter.\n",
        "Sets up the device (CPU or GPU) for training.\n",
        "Creates a log directory with a timestamped folder name to store training logs.\n",
        "Initializes the SummaryWriter for logging, which is often used with TensorBoard.\n",
        "Sets up the loss criterion, which is a custom loss function, NTXentLoss.\n",
        "\n",
        "\n",
        "Device Setup Method '_get_device':\n",
        "Determines whether to use a GPU or CPU for training based on the configuration and system capabilities.\n",
        "\n",
        "\n",
        "Step Method '_step':\n",
        "Performs a forward pass of the model with a pair of inputs (xis and xjs).\n",
        "Computes the loss using the NTXentLoss criterion.\n",
        "\n",
        "\n",
        "Training Method 'train':\n",
        "Retrieves data loaders for training and validation sets.\n",
        "Initializes the model and loads pre-trained weights if available.\n",
        "Sets up the optimizer (Adam) and learning rate scheduler (CosineAnnealingLR).\n",
        "Incorporates support for mixed-precision training using NVIDIA's Apex library if configured.\n",
        "Manages the training loop with logging, validation, and checkpointing logic.\n",
        "\n",
        "\n",
        "Pre-trained Weights Loading Method '_load_pre_trained_weights':\n",
        "Attempts to load pre-trained model weights from a specified checkpoint directory.\n",
        "Handles the case where pre-trained weights are not found and proceeds without them.\n",
        "\n",
        "\n",
        "Validation Method '_validate':\n",
        "Evaluates the model on the validation dataset without updating model weights.\n",
        "Computes and returns the average validation loss.\n",
        "\n",
        "\n",
        "Main Aspects of the Training Loop in 'train':\n",
        "Iterates over the specified number of epochs.\n",
        "Within each epoch, iterates over batches from the training loader.\n",
        "Resets gradients, performs a training step, and updates model weights.\n",
        "Logs training loss and learning rate.\n",
        "Performs validation at specified intervals, logs validation loss, and saves the best model.\n",
        "Saves periodic checkpoints of the model.\n",
        "Updates the learning rate after a warm-up period.\n",
        "\n",
        "\n",
        "This class is a comprehensive template for setting up and conducting pre-training for models, particularly in the context of molecule data. It demonstrates best practices like device-agnostic code, logging, checkpointing, and validation. The use of mixed-precision training indicates an optimization for computational efficiency, which can be especially beneficial when training on large datasets or using models with high computational demands."
      ],
      "metadata": {
        "id": "B8IDSoNTdtdf"
      }
    }
  ]
}